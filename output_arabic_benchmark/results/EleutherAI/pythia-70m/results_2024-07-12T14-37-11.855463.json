{
  "config_general": {
    "lighteval_sha": "4d8c620733802aba964d87aafb12739c8f72bc7d",
    "num_fewshot_seeds": 1,
    "override_batch_size": 32,
    "max_samples": null,
    "job_id": "",
    "start_time": 10819.39465512,
    "end_time": 15252.663638362,
    "total_evaluation_time_secondes": "4433.268983242",
    "model_name": "EleutherAI/pythia-70m",
    "model_sha": "a39f36b100fe8a5377810d56c3f4789b9c53ac42",
    "model_dtype": "torch.float16",
    "model_size": "138.83 MB",
    "config": null
  },
  "results": {
    "community|acva:Algeria|5": {
      "acc_norm": 0.517948717948718,
      "acc_norm_stderr": 0.03587477098773825
    },
    "community|acva:Ancient_Egypt|5": {
      "acc_norm": 0.9492063492063492,
      "acc_norm_stderr": 0.01239139518482262
    },
    "community|acva:Arab_Empire|5": {
      "acc_norm": 0.30943396226415093,
      "acc_norm_stderr": 0.028450154794118627
    },
    "community|acva:Arabic_Architecture|5": {
      "acc_norm": 0.5435897435897435,
      "acc_norm_stderr": 0.035761230969912135
    },
    "community|acva:Arabic_Art|5": {
      "acc_norm": 0.6358974358974359,
      "acc_norm_stderr": 0.03454653867786389
    },
    "community|acva:Arabic_Astronomy|5": {
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.03581804596782233
    },
    "community|acva:Arabic_Calligraphy|5": {
      "acc_norm": 0.5294117647058824,
      "acc_norm_stderr": 0.03131846503821582
    },
    "community|acva:Arabic_Ceremony|5": {
      "acc_norm": 0.4702702702702703,
      "acc_norm_stderr": 0.03679527255567926
    },
    "community|acva:Arabic_Clothing|5": {
      "acc_norm": 0.5128205128205128,
      "acc_norm_stderr": 0.03588610523192215
    },
    "community|acva:Arabic_Culture|5": {
      "acc_norm": 0.2205128205128205,
      "acc_norm_stderr": 0.02976600466164411
    },
    "community|acva:Arabic_Food|5": {
      "acc_norm": 0.441025641025641,
      "acc_norm_stderr": 0.0356473293185358
    },
    "community|acva:Arabic_Funeral|5": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.050529115263991134
    },
    "community|acva:Arabic_Geography|5": {
      "acc_norm": 0.3931034482758621,
      "acc_norm_stderr": 0.040703290137070705
    },
    "community|acva:Arabic_History|5": {
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.03298070870085619
    },
    "community|acva:Arabic_Language_Origin|5": {
      "acc_norm": 0.5473684210526316,
      "acc_norm_stderr": 0.051339113773544845
    },
    "community|acva:Arabic_Literature|5": {
      "acc_norm": 0.41379310344827586,
      "acc_norm_stderr": 0.04104269211806232
    },
    "community|acva:Arabic_Math|5": {
      "acc_norm": 0.3435897435897436,
      "acc_norm_stderr": 0.03409627301409855
    },
    "community|acva:Arabic_Medicine|5": {
      "acc_norm": 0.503448275862069,
      "acc_norm_stderr": 0.04166567577101579
    },
    "community|acva:Arabic_Music|5": {
      "acc_norm": 0.302158273381295,
      "acc_norm_stderr": 0.039089144792915614
    },
    "community|acva:Arabic_Ornament|5": {
      "acc_norm": 0.5282051282051282,
      "acc_norm_stderr": 0.035840746749208334
    },
    "community|acva:Arabic_Philosophy|5": {
      "acc_norm": 0.41379310344827586,
      "acc_norm_stderr": 0.04104269211806232
    },
    "community|acva:Arabic_Physics_and_Chemistry|5": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03581804596782232
    },
    "community|acva:Arabic_Wedding|5": {
      "acc_norm": 0.41025641025641024,
      "acc_norm_stderr": 0.03531493712326671
    },
    "community|acva:Bahrain|5": {
      "acc_norm": 0.6888888888888889,
      "acc_norm_stderr": 0.06979205927323111
    },
    "community|acva:Comoros|5": {
      "acc_norm": 0.6222222222222222,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Egypt_modern|5": {
      "acc_norm": 0.3263157894736842,
      "acc_norm_stderr": 0.04835966701461423
    },
    "community|acva:InfluenceFromAncientEgypt|5": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.035172622905632896
    },
    "community|acva:InfluenceFromByzantium|5": {
      "acc_norm": 0.7172413793103448,
      "acc_norm_stderr": 0.03752833958003337
    },
    "community|acva:InfluenceFromChina|5": {
      "acc_norm": 0.7230769230769231,
      "acc_norm_stderr": 0.032127058190759304
    },
    "community|acva:InfluenceFromGreece|5": {
      "acc_norm": 0.6307692307692307,
      "acc_norm_stderr": 0.034648411418637566
    },
    "community|acva:InfluenceFromIslam|5": {
      "acc_norm": 0.7034482758620689,
      "acc_norm_stderr": 0.03806142687309992
    },
    "community|acva:InfluenceFromPersia|5": {
      "acc_norm": 0.6971428571428572,
      "acc_norm_stderr": 0.03483414676585986
    },
    "community|acva:InfluenceFromRome|5": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.03549871080367708
    },
    "community|acva:Iraq|5": {
      "acc_norm": 0.5058823529411764,
      "acc_norm_stderr": 0.05455069703232772
    },
    "community|acva:Islam_Education|5": {
      "acc_norm": 0.5487179487179488,
      "acc_norm_stderr": 0.03572709860318392
    },
    "community|acva:Islam_branches_and_schools|5": {
      "acc_norm": 0.4342857142857143,
      "acc_norm_stderr": 0.037576101528126626
    },
    "community|acva:Islamic_law_system|5": {
      "acc_norm": 0.4205128205128205,
      "acc_norm_stderr": 0.03544138389303484
    },
    "community|acva:Jordan|5": {
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.0752101433090355
    },
    "community|acva:Kuwait|5": {
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.07535922203472523
    },
    "community|acva:Lebanon|5": {
      "acc_norm": 0.8222222222222222,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Libya|5": {
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.07491109582924914
    },
    "community|acva:Mauritania|5": {
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Mesopotamia_civilization|5": {
      "acc_norm": 0.5225806451612903,
      "acc_norm_stderr": 0.0402500394824441
    },
    "community|acva:Morocco|5": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.07385489458759965
    },
    "community|acva:Oman|5": {
      "acc_norm": 0.8222222222222222,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Palestine|5": {
      "acc_norm": 0.7529411764705882,
      "acc_norm_stderr": 0.047058823529411785
    },
    "community|acva:Qatar|5": {
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.07446027270295806
    },
    "community|acva:Saudi_Arabia|5": {
      "acc_norm": 0.3282051282051282,
      "acc_norm_stderr": 0.03371243782413707
    },
    "community|acva:Somalia|5": {
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Sudan|5": {
      "acc_norm": 0.6444444444444445,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Syria|5": {
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Tunisia|5": {
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.0697920592732311
    },
    "community|acva:United_Arab_Emirates|5": {
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.04628210543937907
    },
    "community|acva:Yemen|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.13333333333333333
    },
    "community|acva:communication|5": {
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.025974025974025955
    },
    "community|acva:computer_and_phone|5": {
      "acc_norm": 0.535593220338983,
      "acc_norm_stderr": 0.029086612547284608
    },
    "community|acva:daily_life|5": {
      "acc_norm": 0.16023738872403562,
      "acc_norm_stderr": 0.0200120029442851
    },
    "community|acva:entertainment|5": {
      "acc_norm": 0.7661016949152543,
      "acc_norm_stderr": 0.024687839412166384
    },
    "community|alghafa:mcq_exams_test_ar|5": {
      "acc_norm": 0.2621184919210054,
      "acc_norm_stderr": 0.018651112765714392
    },
    "community|alghafa:meta_ar_dialects|5": {
      "acc_norm": 0.2457831325301205,
      "acc_norm_stderr": 0.005862308739300398
    },
    "community|alghafa:meta_ar_msa|5": {
      "acc_norm": 0.2536312849162011,
      "acc_norm_stderr": 0.01455155365936992
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": {
      "acc_norm": 0.4266666666666667,
      "acc_norm_stderr": 0.057495266811327245
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": {
      "acc_norm": 0.3933333333333333,
      "acc_norm_stderr": 0.04001863846147464
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": {
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.03498801328777481
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": {
      "acc_norm": 0.499812382739212,
      "acc_norm_stderr": 0.005592267043694276
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|5": {
      "acc_norm": 0.3302752293577982,
      "acc_norm_stderr": 0.006074741656339506
    },
    "community|alghafa:multiple_choice_sentiment_task|5": {
      "acc_norm": 0.3325581395348837,
      "acc_norm_stderr": 0.011363250303250376
    },
    "community|arabic_exams|5": {
      "acc_norm": 0.23649906890130354,
      "acc_norm_stderr": 0.018354269670319875
    },
    "community|arabic_mmlu:abstract_algebra|5": {
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932268
    },
    "community|arabic_mmlu:anatomy|5": {
      "acc_norm": 0.1925925925925926,
      "acc_norm_stderr": 0.03406542058502655
    },
    "community|arabic_mmlu:astronomy|5": {
      "acc_norm": 0.17763157894736842,
      "acc_norm_stderr": 0.031103182383123398
    },
    "community|arabic_mmlu:business_ethics|5": {
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "community|arabic_mmlu:clinical_knowledge|5": {
      "acc_norm": 0.2188679245283019,
      "acc_norm_stderr": 0.025447863825108625
    },
    "community|arabic_mmlu:college_biology|5": {
      "acc_norm": 0.20833333333333334,
      "acc_norm_stderr": 0.033961162058453336
    },
    "community|arabic_mmlu:college_chemistry|5": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "community|arabic_mmlu:college_computer_science|5": {
      "acc_norm": 0.17,
      "acc_norm_stderr": 0.0377525168068637
    },
    "community|arabic_mmlu:college_mathematics|5": {
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "community|arabic_mmlu:college_medicine|5": {
      "acc_norm": 0.20809248554913296,
      "acc_norm_stderr": 0.030952890217749874
    },
    "community|arabic_mmlu:college_physics|5": {
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237654
    },
    "community|arabic_mmlu:computer_security|5": {
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909283
    },
    "community|arabic_mmlu:conceptual_physics|5": {
      "acc_norm": 0.26382978723404255,
      "acc_norm_stderr": 0.028809989854102973
    },
    "community|arabic_mmlu:econometrics|5": {
      "acc_norm": 0.2543859649122807,
      "acc_norm_stderr": 0.04096985139843671
    },
    "community|arabic_mmlu:electrical_engineering|5": {
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.03565998174135302
    },
    "community|arabic_mmlu:elementary_mathematics|5": {
      "acc_norm": 0.2566137566137566,
      "acc_norm_stderr": 0.022494510767503154
    },
    "community|arabic_mmlu:formal_logic|5": {
      "acc_norm": 0.19047619047619047,
      "acc_norm_stderr": 0.03512207412302054
    },
    "community|arabic_mmlu:global_facts|5": {
      "acc_norm": 0.18,
      "acc_norm_stderr": 0.038612291966536934
    },
    "community|arabic_mmlu:high_school_biology|5": {
      "acc_norm": 0.18064516129032257,
      "acc_norm_stderr": 0.021886178567172548
    },
    "community|arabic_mmlu:high_school_chemistry|5": {
      "acc_norm": 0.3054187192118227,
      "acc_norm_stderr": 0.03240661565868408
    },
    "community|arabic_mmlu:high_school_computer_science|5": {
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "community|arabic_mmlu:high_school_european_history|5": {
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03225078108306289
    },
    "community|arabic_mmlu:high_school_geography|5": {
      "acc_norm": 0.18686868686868688,
      "acc_norm_stderr": 0.02777253333421898
    },
    "community|arabic_mmlu:high_school_government_and_politics|5": {
      "acc_norm": 0.22797927461139897,
      "acc_norm_stderr": 0.030276909945178267
    },
    "community|arabic_mmlu:high_school_macroeconomics|5": {
      "acc_norm": 0.21025641025641026,
      "acc_norm_stderr": 0.02066059748502693
    },
    "community|arabic_mmlu:high_school_mathematics|5": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.024388430433987657
    },
    "community|arabic_mmlu:high_school_microeconomics|5": {
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.02626502460827589
    },
    "community|arabic_mmlu:high_school_physics|5": {
      "acc_norm": 0.1986754966887417,
      "acc_norm_stderr": 0.03257847384436775
    },
    "community|arabic_mmlu:high_school_psychology|5": {
      "acc_norm": 0.1944954128440367,
      "acc_norm_stderr": 0.016970289090458054
    },
    "community|arabic_mmlu:high_school_statistics|5": {
      "acc_norm": 0.46296296296296297,
      "acc_norm_stderr": 0.03400603625538272
    },
    "community|arabic_mmlu:high_school_us_history|5": {
      "acc_norm": 0.23039215686274508,
      "acc_norm_stderr": 0.02955429260569507
    },
    "community|arabic_mmlu:high_school_world_history|5": {
      "acc_norm": 0.25316455696202533,
      "acc_norm_stderr": 0.028304657943035313
    },
    "community|arabic_mmlu:human_aging|5": {
      "acc_norm": 0.31390134529147984,
      "acc_norm_stderr": 0.031146796482972465
    },
    "community|arabic_mmlu:human_sexuality|5": {
      "acc_norm": 0.2595419847328244,
      "acc_norm_stderr": 0.03844876139785271
    },
    "community|arabic_mmlu:international_law|5": {
      "acc_norm": 0.2396694214876033,
      "acc_norm_stderr": 0.03896878985070417
    },
    "community|arabic_mmlu:jurisprudence|5": {
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.042365112580946336
    },
    "community|arabic_mmlu:logical_fallacies|5": {
      "acc_norm": 0.18404907975460122,
      "acc_norm_stderr": 0.030446777687971733
    },
    "community|arabic_mmlu:machine_learning|5": {
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.043994650575715215
    },
    "community|arabic_mmlu:management|5": {
      "acc_norm": 0.17475728155339806,
      "acc_norm_stderr": 0.037601780060266224
    },
    "community|arabic_mmlu:marketing|5": {
      "acc_norm": 0.19658119658119658,
      "acc_norm_stderr": 0.02603538609895129
    },
    "community|arabic_mmlu:medical_genetics|5": {
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "community|arabic_mmlu:miscellaneous|5": {
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.015302380123542089
    },
    "community|arabic_mmlu:moral_disputes|5": {
      "acc_norm": 0.24855491329479767,
      "acc_norm_stderr": 0.023267528432100174
    },
    "community|arabic_mmlu:moral_scenarios|5": {
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "community|arabic_mmlu:nutrition|5": {
      "acc_norm": 0.24836601307189543,
      "acc_norm_stderr": 0.02473998135511359
    },
    "community|arabic_mmlu:philosophy|5": {
      "acc_norm": 0.1832797427652733,
      "acc_norm_stderr": 0.021974198848265805
    },
    "community|arabic_mmlu:prehistory|5": {
      "acc_norm": 0.21604938271604937,
      "acc_norm_stderr": 0.022899162918445806
    },
    "community|arabic_mmlu:professional_accounting|5": {
      "acc_norm": 0.2695035460992908,
      "acc_norm_stderr": 0.02646903681859063
    },
    "community|arabic_mmlu:professional_law|5": {
      "acc_norm": 0.22816166883963493,
      "acc_norm_stderr": 0.010717992192047866
    },
    "community|arabic_mmlu:professional_medicine|5": {
      "acc_norm": 0.31985294117647056,
      "acc_norm_stderr": 0.028332959514031218
    },
    "community|arabic_mmlu:professional_psychology|5": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.01751781884501444
    },
    "community|arabic_mmlu:public_relations|5": {
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.04013964554072775
    },
    "community|arabic_mmlu:security_studies|5": {
      "acc_norm": 0.17959183673469387,
      "acc_norm_stderr": 0.024573293589585637
    },
    "community|arabic_mmlu:sociology|5": {
      "acc_norm": 0.24378109452736318,
      "acc_norm_stderr": 0.03036049015401465
    },
    "community|arabic_mmlu:us_foreign_policy|5": {
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "community|arabic_mmlu:virology|5": {
      "acc_norm": 0.28313253012048195,
      "acc_norm_stderr": 0.03507295431370518
    },
    "community|arabic_mmlu:world_religions|5": {
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0312678171466318
    },
    "community|arc_challenge_okapi_ar|5": {
      "acc_norm": 0.2706896551724138,
      "acc_norm_stderr": 0.01305119559397962
    },
    "community|arc_easy_ar|5": {
      "acc_norm": 0.24957698815566837,
      "acc_norm_stderr": 0.008902735701949107
    },
    "community|boolq_ar|5": {
      "acc_norm": 0.6205521472392638,
      "acc_norm_stderr": 0.00850007995551102
    },
    "community|copa_ext_ar|5": {
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.05267171812666418
    },
    "community|hellaswag_okapi_ar|5": {
      "acc_norm": 0.24130411078399303,
      "acc_norm_stderr": 0.004468189234853998
    },
    "community|mmlu_okapi_ar|5": {
      "acc_norm": 0.2604658361061673,
      "acc_norm_stderr": 0.0038609096271105246
    },
    "community|openbook_qa_ext_ar|5": {
      "acc_norm": 0.2828282828282828,
      "acc_norm_stderr": 0.02026326951952281
    },
    "community|piqa_ar|5": {
      "acc_norm": 0.4866339334424441,
      "acc_norm_stderr": 0.011677558168271054
    },
    "community|race_ar|5": {
      "acc_norm": 0.27551227429498887,
      "acc_norm_stderr": 0.006364299987263261
    },
    "community|sciq_ar|5": {
      "acc_norm": 0.3597989949748744,
      "acc_norm_stderr": 0.01522281454547954
    },
    "community|toxigen_ar|5": {
      "acc_norm": 0.4310160427807487,
      "acc_norm_stderr": 0.016204039390071808
    },
    "lighteval|xstory_cloze:ar|0": {
      "acc": 0.46657842488418266,
      "acc_stderr": 0.012838347934731672
    },
    "community|acva:_average|5": {
      "acc_norm": 0.5139790587227919,
      "acc_norm_stderr": 0.04622320128538295
    },
    "community|alghafa:_average|5": {
      "acc_norm": 0.3315754067776912,
      "acc_norm_stderr": 0.021621905858693953
    },
    "community|arabic_mmlu:_average|5": {
      "acc_norm": 0.23807916889390823,
      "acc_norm_stderr": 0.031662530096253
    },
    "all": {
      "acc_norm": 0.37150587112498895,
      "acc_norm_stderr": 0.0357341773697638,
      "acc": 0.46657842488418266,
      "acc_stderr": 0.012838347934731672
    }
  },
  "versions": {
    "community|acva:Algeria|5": 0,
    "community|acva:Ancient_Egypt|5": 0,
    "community|acva:Arab_Empire|5": 0,
    "community|acva:Arabic_Architecture|5": 0,
    "community|acva:Arabic_Art|5": 0,
    "community|acva:Arabic_Astronomy|5": 0,
    "community|acva:Arabic_Calligraphy|5": 0,
    "community|acva:Arabic_Ceremony|5": 0,
    "community|acva:Arabic_Clothing|5": 0,
    "community|acva:Arabic_Culture|5": 0,
    "community|acva:Arabic_Food|5": 0,
    "community|acva:Arabic_Funeral|5": 0,
    "community|acva:Arabic_Geography|5": 0,
    "community|acva:Arabic_History|5": 0,
    "community|acva:Arabic_Language_Origin|5": 0,
    "community|acva:Arabic_Literature|5": 0,
    "community|acva:Arabic_Math|5": 0,
    "community|acva:Arabic_Medicine|5": 0,
    "community|acva:Arabic_Music|5": 0,
    "community|acva:Arabic_Ornament|5": 0,
    "community|acva:Arabic_Philosophy|5": 0,
    "community|acva:Arabic_Physics_and_Chemistry|5": 0,
    "community|acva:Arabic_Wedding|5": 0,
    "community|acva:Bahrain|5": 0,
    "community|acva:Comoros|5": 0,
    "community|acva:Egypt_modern|5": 0,
    "community|acva:InfluenceFromAncientEgypt|5": 0,
    "community|acva:InfluenceFromByzantium|5": 0,
    "community|acva:InfluenceFromChina|5": 0,
    "community|acva:InfluenceFromGreece|5": 0,
    "community|acva:InfluenceFromIslam|5": 0,
    "community|acva:InfluenceFromPersia|5": 0,
    "community|acva:InfluenceFromRome|5": 0,
    "community|acva:Iraq|5": 0,
    "community|acva:Islam_Education|5": 0,
    "community|acva:Islam_branches_and_schools|5": 0,
    "community|acva:Islamic_law_system|5": 0,
    "community|acva:Jordan|5": 0,
    "community|acva:Kuwait|5": 0,
    "community|acva:Lebanon|5": 0,
    "community|acva:Libya|5": 0,
    "community|acva:Mauritania|5": 0,
    "community|acva:Mesopotamia_civilization|5": 0,
    "community|acva:Morocco|5": 0,
    "community|acva:Oman|5": 0,
    "community|acva:Palestine|5": 0,
    "community|acva:Qatar|5": 0,
    "community|acva:Saudi_Arabia|5": 0,
    "community|acva:Somalia|5": 0,
    "community|acva:Sudan|5": 0,
    "community|acva:Syria|5": 0,
    "community|acva:Tunisia|5": 0,
    "community|acva:United_Arab_Emirates|5": 0,
    "community|acva:Yemen|5": 0,
    "community|acva:communication|5": 0,
    "community|acva:computer_and_phone|5": 0,
    "community|acva:daily_life|5": 0,
    "community|acva:entertainment|5": 0,
    "community|alghafa:mcq_exams_test_ar|5": 0,
    "community|alghafa:meta_ar_dialects|5": 0,
    "community|alghafa:meta_ar_msa|5": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|5": 0,
    "community|alghafa:multiple_choice_sentiment_task|5": 0,
    "community|arabic_exams|5": 0,
    "community|arabic_mmlu:abstract_algebra|5": 0,
    "community|arabic_mmlu:anatomy|5": 0,
    "community|arabic_mmlu:astronomy|5": 0,
    "community|arabic_mmlu:business_ethics|5": 0,
    "community|arabic_mmlu:clinical_knowledge|5": 0,
    "community|arabic_mmlu:college_biology|5": 0,
    "community|arabic_mmlu:college_chemistry|5": 0,
    "community|arabic_mmlu:college_computer_science|5": 0,
    "community|arabic_mmlu:college_mathematics|5": 0,
    "community|arabic_mmlu:college_medicine|5": 0,
    "community|arabic_mmlu:college_physics|5": 0,
    "community|arabic_mmlu:computer_security|5": 0,
    "community|arabic_mmlu:conceptual_physics|5": 0,
    "community|arabic_mmlu:econometrics|5": 0,
    "community|arabic_mmlu:electrical_engineering|5": 0,
    "community|arabic_mmlu:elementary_mathematics|5": 0,
    "community|arabic_mmlu:formal_logic|5": 0,
    "community|arabic_mmlu:global_facts|5": 0,
    "community|arabic_mmlu:high_school_biology|5": 0,
    "community|arabic_mmlu:high_school_chemistry|5": 0,
    "community|arabic_mmlu:high_school_computer_science|5": 0,
    "community|arabic_mmlu:high_school_european_history|5": 0,
    "community|arabic_mmlu:high_school_geography|5": 0,
    "community|arabic_mmlu:high_school_government_and_politics|5": 0,
    "community|arabic_mmlu:high_school_macroeconomics|5": 0,
    "community|arabic_mmlu:high_school_mathematics|5": 0,
    "community|arabic_mmlu:high_school_microeconomics|5": 0,
    "community|arabic_mmlu:high_school_physics|5": 0,
    "community|arabic_mmlu:high_school_psychology|5": 0,
    "community|arabic_mmlu:high_school_statistics|5": 0,
    "community|arabic_mmlu:high_school_us_history|5": 0,
    "community|arabic_mmlu:high_school_world_history|5": 0,
    "community|arabic_mmlu:human_aging|5": 0,
    "community|arabic_mmlu:human_sexuality|5": 0,
    "community|arabic_mmlu:international_law|5": 0,
    "community|arabic_mmlu:jurisprudence|5": 0,
    "community|arabic_mmlu:logical_fallacies|5": 0,
    "community|arabic_mmlu:machine_learning|5": 0,
    "community|arabic_mmlu:management|5": 0,
    "community|arabic_mmlu:marketing|5": 0,
    "community|arabic_mmlu:medical_genetics|5": 0,
    "community|arabic_mmlu:miscellaneous|5": 0,
    "community|arabic_mmlu:moral_disputes|5": 0,
    "community|arabic_mmlu:moral_scenarios|5": 0,
    "community|arabic_mmlu:nutrition|5": 0,
    "community|arabic_mmlu:philosophy|5": 0,
    "community|arabic_mmlu:prehistory|5": 0,
    "community|arabic_mmlu:professional_accounting|5": 0,
    "community|arabic_mmlu:professional_law|5": 0,
    "community|arabic_mmlu:professional_medicine|5": 0,
    "community|arabic_mmlu:professional_psychology|5": 0,
    "community|arabic_mmlu:public_relations|5": 0,
    "community|arabic_mmlu:security_studies|5": 0,
    "community|arabic_mmlu:sociology|5": 0,
    "community|arabic_mmlu:us_foreign_policy|5": 0,
    "community|arabic_mmlu:virology|5": 0,
    "community|arabic_mmlu:world_religions|5": 0,
    "community|arc_challenge_okapi_ar|5": 0,
    "community|arc_easy_ar|5": 0,
    "community|boolq_ar|5": 0,
    "community|copa_ext_ar|5": 0,
    "community|hellaswag_okapi_ar|5": 0,
    "community|mmlu_okapi_ar|5": 0,
    "community|openbook_qa_ext_ar|5": 0,
    "community|piqa_ar|5": 0,
    "community|race_ar|5": 0,
    "community|sciq_ar|5": 0,
    "community|toxigen_ar|5": 0,
    "lighteval|xstory_cloze:ar|0": 0
  },
  "config_tasks": {
    "community|acva:Algeria": {
      "name": "acva:Algeria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Algeria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Ancient_Egypt": {
      "name": "acva:Ancient_Egypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Ancient_Egypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 315,
      "effective_num_docs": 315,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arab_Empire": {
      "name": "acva:Arab_Empire",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arab_Empire",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Architecture": {
      "name": "acva:Arabic_Architecture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Architecture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Art": {
      "name": "acva:Arabic_Art",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Art",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Astronomy": {
      "name": "acva:Arabic_Astronomy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Calligraphy": {
      "name": "acva:Arabic_Calligraphy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Calligraphy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 255,
      "effective_num_docs": 255,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ceremony": {
      "name": "acva:Arabic_Ceremony",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ceremony",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 185,
      "effective_num_docs": 185,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Clothing": {
      "name": "acva:Arabic_Clothing",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Clothing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Culture": {
      "name": "acva:Arabic_Culture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Culture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Food": {
      "name": "acva:Arabic_Food",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Food",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Funeral": {
      "name": "acva:Arabic_Funeral",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Funeral",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Geography": {
      "name": "acva:Arabic_Geography",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_History": {
      "name": "acva:Arabic_History",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_History",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Language_Origin": {
      "name": "acva:Arabic_Language_Origin",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Language_Origin",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Literature": {
      "name": "acva:Arabic_Literature",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Literature",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Math": {
      "name": "acva:Arabic_Math",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Math",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Medicine": {
      "name": "acva:Arabic_Medicine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Music": {
      "name": "acva:Arabic_Music",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Music",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 139,
      "effective_num_docs": 139,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ornament": {
      "name": "acva:Arabic_Ornament",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ornament",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Philosophy": {
      "name": "acva:Arabic_Philosophy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry": {
      "name": "acva:Arabic_Physics_and_Chemistry",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Physics_and_Chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Wedding": {
      "name": "acva:Arabic_Wedding",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Wedding",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Bahrain": {
      "name": "acva:Bahrain",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Bahrain",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Comoros": {
      "name": "acva:Comoros",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Comoros",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Egypt_modern": {
      "name": "acva:Egypt_modern",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Egypt_modern",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromAncientEgypt": {
      "name": "acva:InfluenceFromAncientEgypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromAncientEgypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromByzantium": {
      "name": "acva:InfluenceFromByzantium",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromByzantium",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromChina": {
      "name": "acva:InfluenceFromChina",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromChina",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromGreece": {
      "name": "acva:InfluenceFromGreece",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromGreece",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromIslam": {
      "name": "acva:InfluenceFromIslam",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromIslam",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromPersia": {
      "name": "acva:InfluenceFromPersia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromPersia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromRome": {
      "name": "acva:InfluenceFromRome",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromRome",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Iraq": {
      "name": "acva:Iraq",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Iraq",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_Education": {
      "name": "acva:Islam_Education",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_Education",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_branches_and_schools": {
      "name": "acva:Islam_branches_and_schools",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_branches_and_schools",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islamic_law_system": {
      "name": "acva:Islamic_law_system",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islamic_law_system",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Jordan": {
      "name": "acva:Jordan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Jordan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Kuwait": {
      "name": "acva:Kuwait",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Kuwait",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Lebanon": {
      "name": "acva:Lebanon",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Lebanon",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Libya": {
      "name": "acva:Libya",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Libya",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mauritania": {
      "name": "acva:Mauritania",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mauritania",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mesopotamia_civilization": {
      "name": "acva:Mesopotamia_civilization",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mesopotamia_civilization",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 155,
      "effective_num_docs": 155,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Morocco": {
      "name": "acva:Morocco",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Morocco",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Oman": {
      "name": "acva:Oman",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Oman",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Palestine": {
      "name": "acva:Palestine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Palestine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Qatar": {
      "name": "acva:Qatar",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Qatar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Saudi_Arabia": {
      "name": "acva:Saudi_Arabia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Saudi_Arabia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Somalia": {
      "name": "acva:Somalia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Somalia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Sudan": {
      "name": "acva:Sudan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Sudan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Syria": {
      "name": "acva:Syria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Syria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Tunisia": {
      "name": "acva:Tunisia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Tunisia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:United_Arab_Emirates": {
      "name": "acva:United_Arab_Emirates",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "United_Arab_Emirates",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Yemen": {
      "name": "acva:Yemen",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Yemen",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 10,
      "effective_num_docs": 10,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:communication": {
      "name": "acva:communication",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "communication",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 364,
      "effective_num_docs": 364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:computer_and_phone": {
      "name": "acva:computer_and_phone",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "computer_and_phone",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:daily_life": {
      "name": "acva:daily_life",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "daily_life",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 337,
      "effective_num_docs": 337,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:entertainment": {
      "name": "acva:entertainment",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "entertainment",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_exams": {
      "name": "arabic_exams",
      "prompt_function": "arabic_exams",
      "hf_repo": "OALL/Arabic_EXAMS",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 537,
      "effective_num_docs": 537,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:abstract_algebra": {
      "name": "arabic_mmlu:abstract_algebra",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:anatomy": {
      "name": "arabic_mmlu:anatomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:astronomy": {
      "name": "arabic_mmlu:astronomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:business_ethics": {
      "name": "arabic_mmlu:business_ethics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:clinical_knowledge": {
      "name": "arabic_mmlu:clinical_knowledge",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_biology": {
      "name": "arabic_mmlu:college_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_chemistry": {
      "name": "arabic_mmlu:college_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_computer_science": {
      "name": "arabic_mmlu:college_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_mathematics": {
      "name": "arabic_mmlu:college_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_medicine": {
      "name": "arabic_mmlu:college_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_physics": {
      "name": "arabic_mmlu:college_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:computer_security": {
      "name": "arabic_mmlu:computer_security",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:conceptual_physics": {
      "name": "arabic_mmlu:conceptual_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:econometrics": {
      "name": "arabic_mmlu:econometrics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:electrical_engineering": {
      "name": "arabic_mmlu:electrical_engineering",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:elementary_mathematics": {
      "name": "arabic_mmlu:elementary_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:formal_logic": {
      "name": "arabic_mmlu:formal_logic",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:global_facts": {
      "name": "arabic_mmlu:global_facts",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_biology": {
      "name": "arabic_mmlu:high_school_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_chemistry": {
      "name": "arabic_mmlu:high_school_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_computer_science": {
      "name": "arabic_mmlu:high_school_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_european_history": {
      "name": "arabic_mmlu:high_school_european_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_geography": {
      "name": "arabic_mmlu:high_school_geography",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics": {
      "name": "arabic_mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics": {
      "name": "arabic_mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_mathematics": {
      "name": "arabic_mmlu:high_school_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_microeconomics": {
      "name": "arabic_mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_physics": {
      "name": "arabic_mmlu:high_school_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_psychology": {
      "name": "arabic_mmlu:high_school_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_statistics": {
      "name": "arabic_mmlu:high_school_statistics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_us_history": {
      "name": "arabic_mmlu:high_school_us_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_world_history": {
      "name": "arabic_mmlu:high_school_world_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_aging": {
      "name": "arabic_mmlu:human_aging",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_sexuality": {
      "name": "arabic_mmlu:human_sexuality",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:international_law": {
      "name": "arabic_mmlu:international_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:jurisprudence": {
      "name": "arabic_mmlu:jurisprudence",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:logical_fallacies": {
      "name": "arabic_mmlu:logical_fallacies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:machine_learning": {
      "name": "arabic_mmlu:machine_learning",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:management": {
      "name": "arabic_mmlu:management",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:marketing": {
      "name": "arabic_mmlu:marketing",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:medical_genetics": {
      "name": "arabic_mmlu:medical_genetics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:miscellaneous": {
      "name": "arabic_mmlu:miscellaneous",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_disputes": {
      "name": "arabic_mmlu:moral_disputes",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_scenarios": {
      "name": "arabic_mmlu:moral_scenarios",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:nutrition": {
      "name": "arabic_mmlu:nutrition",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:philosophy": {
      "name": "arabic_mmlu:philosophy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:prehistory": {
      "name": "arabic_mmlu:prehistory",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_accounting": {
      "name": "arabic_mmlu:professional_accounting",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_law": {
      "name": "arabic_mmlu:professional_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_medicine": {
      "name": "arabic_mmlu:professional_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_psychology": {
      "name": "arabic_mmlu:professional_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:public_relations": {
      "name": "arabic_mmlu:public_relations",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:security_studies": {
      "name": "arabic_mmlu:security_studies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:sociology": {
      "name": "arabic_mmlu:sociology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:us_foreign_policy": {
      "name": "arabic_mmlu:us_foreign_policy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:virology": {
      "name": "arabic_mmlu:virology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:world_religions": {
      "name": "arabic_mmlu:world_religions",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_challenge_okapi_ar": {
      "name": "arc_challenge_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_challenge_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1160,
      "effective_num_docs": 1160,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_easy_ar": {
      "name": "arc_easy_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_easy_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 2364,
      "effective_num_docs": 2364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|copa_ext_ar": {
      "name": "copa_ext_ar",
      "prompt_function": "copa_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "copa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 90,
      "effective_num_docs": 90,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|mmlu_okapi_ar": {
      "name": "mmlu_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "mmlu_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 12923,
      "effective_num_docs": 12923,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|openbook_qa_ext_ar": {
      "name": "openbook_qa_ext_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "openbook_qa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 495,
      "effective_num_docs": 495,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|piqa_ar": {
      "name": "piqa_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "piqa_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1833,
      "effective_num_docs": 1833,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|sciq_ar": {
      "name": "sciq_ar",
      "prompt_function": "sciq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "sciq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 995,
      "effective_num_docs": 995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|toxigen_ar": {
      "name": "toxigen_ar",
      "prompt_function": "toxigen_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "toxigen_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 935,
      "effective_num_docs": 935,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "lighteval|xstory_cloze:ar": {
      "name": "xstory_cloze:ar",
      "prompt_function": "storycloze",
      "hf_repo": "juletxara/xstory_cloze",
      "hf_subset": "ar",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "training",
        "eval"
      ],
      "evaluation_splits": [
        "eval"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "lighteval"
      ],
      "original_num_docs": 1511,
      "effective_num_docs": 1511,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|acva:Algeria|5": {
      "hashes": {
        "hash_examples": "da5a3003cd46f6f9",
        "hash_full_prompts": "c29c94a4778b91af",
        "hash_input_tokens": "6d4e13abd9fae4d2",
        "hash_cont_tokens": "e9b450f19080f1a5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Ancient_Egypt|5": {
      "hashes": {
        "hash_examples": "52d6f767fede195b",
        "hash_full_prompts": "180fa0424d9930c1",
        "hash_input_tokens": "68e3b258f9181228",
        "hash_cont_tokens": "e40d9355630f8476"
      },
      "truncated": 0,
      "non_truncated": 315,
      "padded": 630,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arab_Empire|5": {
      "hashes": {
        "hash_examples": "8dacff6a79804a75",
        "hash_full_prompts": "887aee46e82b83be",
        "hash_input_tokens": "6cde31d72f9f5654",
        "hash_cont_tokens": "e6ea69d458457ebd"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 530,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Architecture|5": {
      "hashes": {
        "hash_examples": "df286cd862d9f6bb",
        "hash_full_prompts": "81711e01be90b5e6",
        "hash_input_tokens": "2bc97ecf3f887b72",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Art|5": {
      "hashes": {
        "hash_examples": "112883d764118a49",
        "hash_full_prompts": "5edc75896c16741a",
        "hash_input_tokens": "f1b174a760518f51",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Astronomy|5": {
      "hashes": {
        "hash_examples": "20dcdf2454bf8671",
        "hash_full_prompts": "a256f8ebf1979ebe",
        "hash_input_tokens": "2860c17aac5d682d",
        "hash_cont_tokens": "938a0948e5f9277c"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Calligraphy|5": {
      "hashes": {
        "hash_examples": "3a9f9d1ebe868a15",
        "hash_full_prompts": "6fa5839f835a8d91",
        "hash_input_tokens": "a3a92cd0ceecac69",
        "hash_cont_tokens": "43733169225ac13f"
      },
      "truncated": 0,
      "non_truncated": 255,
      "padded": 510,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ceremony|5": {
      "hashes": {
        "hash_examples": "c927630f8d2f44da",
        "hash_full_prompts": "5c855368b76ce9a2",
        "hash_input_tokens": "cce5b82a534351b2",
        "hash_cont_tokens": "a66d4f3f9224ff06"
      },
      "truncated": 0,
      "non_truncated": 185,
      "padded": 370,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Clothing|5": {
      "hashes": {
        "hash_examples": "6ad0740c2ac6ac92",
        "hash_full_prompts": "07a8647e8f9c8d1b",
        "hash_input_tokens": "14adfe850a79e7b7",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Culture|5": {
      "hashes": {
        "hash_examples": "2177bd857ad872ae",
        "hash_full_prompts": "c1cf28608e1fb425",
        "hash_input_tokens": "8c1d6bbd8ee71ce8",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Food|5": {
      "hashes": {
        "hash_examples": "a6ada65b71d7c9c5",
        "hash_full_prompts": "86ba76526b296be2",
        "hash_input_tokens": "f7961deb46327200",
        "hash_cont_tokens": "1ec1d28270cd48f4"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Funeral|5": {
      "hashes": {
        "hash_examples": "fcee39dc29eaae91",
        "hash_full_prompts": "cb5c8f47c7040564",
        "hash_input_tokens": "f4318c1e884e09fe",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Geography|5": {
      "hashes": {
        "hash_examples": "d36eda7c89231c02",
        "hash_full_prompts": "28de532ce8af96b4",
        "hash_input_tokens": "f141f4ee44b7c89a",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_History|5": {
      "hashes": {
        "hash_examples": "6354ac0d6db6a5fc",
        "hash_full_prompts": "6d6bed6651718cbd",
        "hash_input_tokens": "73006b22f1fb9d82",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Language_Origin|5": {
      "hashes": {
        "hash_examples": "ddc967c8aca34402",
        "hash_full_prompts": "43331d29ba5ee173",
        "hash_input_tokens": "4aeeb4a4961b0a65",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Literature|5": {
      "hashes": {
        "hash_examples": "4305379fd46be5d8",
        "hash_full_prompts": "bfa6c0c0b2cc3bf3",
        "hash_input_tokens": "915e36b33062ab77",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Math|5": {
      "hashes": {
        "hash_examples": "dec621144f4d28be",
        "hash_full_prompts": "fc31377795d0278a",
        "hash_input_tokens": "f10c9eadf8fd9dc3",
        "hash_cont_tokens": "2764b336e4a32a55"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Medicine|5": {
      "hashes": {
        "hash_examples": "2b344cdae9495ff2",
        "hash_full_prompts": "dd28c6072634e8b4",
        "hash_input_tokens": "5c47ea350836ba64",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Music|5": {
      "hashes": {
        "hash_examples": "0c54624d881944ce",
        "hash_full_prompts": "15efa221a080fba6",
        "hash_input_tokens": "59db582214b1963e",
        "hash_cont_tokens": "5aa58462a622ebdd"
      },
      "truncated": 0,
      "non_truncated": 139,
      "padded": 278,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ornament|5": {
      "hashes": {
        "hash_examples": "251a4a84289d8bc1",
        "hash_full_prompts": "0726b58b307c920e",
        "hash_input_tokens": "a1214ebb037b9d12",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Philosophy|5": {
      "hashes": {
        "hash_examples": "3f86fb9c94c13d22",
        "hash_full_prompts": "4f314ee6b028a296",
        "hash_input_tokens": "c5848c68792685a1",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry|5": {
      "hashes": {
        "hash_examples": "8fec65af3695b62a",
        "hash_full_prompts": "4946c67f4133bcd2",
        "hash_input_tokens": "db9d066e274d23e8",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Wedding|5": {
      "hashes": {
        "hash_examples": "9cc3477184d7a4b8",
        "hash_full_prompts": "7d55bb3d704ed684",
        "hash_input_tokens": "817be7d0248de49c",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Bahrain|5": {
      "hashes": {
        "hash_examples": "c92e803a0fa8b9e2",
        "hash_full_prompts": "be1b4f346995ec0a",
        "hash_input_tokens": "8e8900be5f37f72e",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Comoros|5": {
      "hashes": {
        "hash_examples": "06e5d4bba8e54cae",
        "hash_full_prompts": "5002502694357eaf",
        "hash_input_tokens": "33c1e6088dfe06a6",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Egypt_modern|5": {
      "hashes": {
        "hash_examples": "c6ec369164f93446",
        "hash_full_prompts": "d36abc64c365b67a",
        "hash_input_tokens": "77655003084bd1b7",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromAncientEgypt|5": {
      "hashes": {
        "hash_examples": "b9d56d74818b9bd4",
        "hash_full_prompts": "291e1066a5795d9a",
        "hash_input_tokens": "626bb50757c53d4a",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromByzantium|5": {
      "hashes": {
        "hash_examples": "5316c9624e7e59b8",
        "hash_full_prompts": "3787d57f746e98d0",
        "hash_input_tokens": "53e75b26a1bf3da7",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromChina|5": {
      "hashes": {
        "hash_examples": "87894bce95a56411",
        "hash_full_prompts": "503a8884b533989e",
        "hash_input_tokens": "c7256f1e51cfbb1f",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromGreece|5": {
      "hashes": {
        "hash_examples": "0baa78a27e469312",
        "hash_full_prompts": "81e9040116fe5fa2",
        "hash_input_tokens": "64f9aa29dc5a3afb",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromIslam|5": {
      "hashes": {
        "hash_examples": "0c2532cde6541ff2",
        "hash_full_prompts": "26910f4b59987e00",
        "hash_input_tokens": "3d279598fa4bfddf",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromPersia|5": {
      "hashes": {
        "hash_examples": "efcd8112dc53c6e5",
        "hash_full_prompts": "14e956e7740010ad",
        "hash_input_tokens": "200e509ddb8222bf",
        "hash_cont_tokens": "62d62b3e9e6183e3"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromRome|5": {
      "hashes": {
        "hash_examples": "9db61480e2e85fd3",
        "hash_full_prompts": "031043cb3d0601e7",
        "hash_input_tokens": "20b2c6d15138693c",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Iraq|5": {
      "hashes": {
        "hash_examples": "96dac3dfa8d2f41f",
        "hash_full_prompts": "314e7d813cc0ebb0",
        "hash_input_tokens": "5159c7ff47e1c0fe",
        "hash_cont_tokens": "9ec87f6f54aa68d0"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_Education|5": {
      "hashes": {
        "hash_examples": "0d80355f6a4cb51b",
        "hash_full_prompts": "5c72236d1ab83ca8",
        "hash_input_tokens": "593934adc35c1113",
        "hash_cont_tokens": "70f34847e8d384f0"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_branches_and_schools|5": {
      "hashes": {
        "hash_examples": "5cedce1be2c3ad50",
        "hash_full_prompts": "6073825f4be45d10",
        "hash_input_tokens": "187bc29cb4bc5fd3",
        "hash_cont_tokens": "62d62b3e9e6183e3"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islamic_law_system|5": {
      "hashes": {
        "hash_examples": "c0e6db8bc84e105e",
        "hash_full_prompts": "d4a26fdef76078d0",
        "hash_input_tokens": "b3283458ed622f09",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Jordan|5": {
      "hashes": {
        "hash_examples": "33deb5b4e5ddd6a1",
        "hash_full_prompts": "3f74d1fb89882c0c",
        "hash_input_tokens": "82da6c9c565eeb74",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Kuwait|5": {
      "hashes": {
        "hash_examples": "eb41773346d7c46c",
        "hash_full_prompts": "75a56fd0b085c6b2",
        "hash_input_tokens": "46fd1cbd48b3c621",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Lebanon|5": {
      "hashes": {
        "hash_examples": "25932dbf4c13d34f",
        "hash_full_prompts": "2bcf2e3b8ff223fe",
        "hash_input_tokens": "6ca9db7015462677",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Libya|5": {
      "hashes": {
        "hash_examples": "f2c4db63cd402926",
        "hash_full_prompts": "e7d725abcb4313af",
        "hash_input_tokens": "826ff6f2a23cd211",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mauritania|5": {
      "hashes": {
        "hash_examples": "8723ab5fdf286b54",
        "hash_full_prompts": "f0e7d75900f14fcf",
        "hash_input_tokens": "847ab2136972f3e3",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mesopotamia_civilization|5": {
      "hashes": {
        "hash_examples": "c33f5502a6130ca9",
        "hash_full_prompts": "8de23dd7e0dc6435",
        "hash_input_tokens": "af1a4f8b22cba95d",
        "hash_cont_tokens": "2384a0ce35e7c12c"
      },
      "truncated": 0,
      "non_truncated": 155,
      "padded": 310,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Morocco|5": {
      "hashes": {
        "hash_examples": "588a5ed27904b1ae",
        "hash_full_prompts": "24847db696527ca5",
        "hash_input_tokens": "d82743f8ab4cc427",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Oman|5": {
      "hashes": {
        "hash_examples": "d447c52b94248b69",
        "hash_full_prompts": "a5b8786aba97e97f",
        "hash_input_tokens": "6e96bb9987ce4214",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Palestine|5": {
      "hashes": {
        "hash_examples": "19197e076ad14ff5",
        "hash_full_prompts": "c4070490cd3bda3e",
        "hash_input_tokens": "894d8ad40784ce67",
        "hash_cont_tokens": "9ec87f6f54aa68d0"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Qatar|5": {
      "hashes": {
        "hash_examples": "cf0736fa185b28f6",
        "hash_full_prompts": "a37061703a491998",
        "hash_input_tokens": "98cb1f9184460a72",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Saudi_Arabia|5": {
      "hashes": {
        "hash_examples": "69beda6e1b85a08d",
        "hash_full_prompts": "33655b54c7bcc6db",
        "hash_input_tokens": "f04afc1d1ce7b4a1",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Somalia|5": {
      "hashes": {
        "hash_examples": "b387940c65784fbf",
        "hash_full_prompts": "9f380e51f126e8e3",
        "hash_input_tokens": "69992f3b56696e2e",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Sudan|5": {
      "hashes": {
        "hash_examples": "e02c32b9d2dd0c3f",
        "hash_full_prompts": "ed1ff80cdafbb906",
        "hash_input_tokens": "438cde504c154131",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Syria|5": {
      "hashes": {
        "hash_examples": "60a6f8fe73bda4bb",
        "hash_full_prompts": "24478f579bfa86a2",
        "hash_input_tokens": "a6286d99b5d92da2",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Tunisia|5": {
      "hashes": {
        "hash_examples": "34bb15d3830c5649",
        "hash_full_prompts": "33ee55e0c12fcd78",
        "hash_input_tokens": "ee656cb431dd5623",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:United_Arab_Emirates|5": {
      "hashes": {
        "hash_examples": "98a0ba78172718ce",
        "hash_full_prompts": "513edd41b5b39486",
        "hash_input_tokens": "c2e2ebfbb3323cfe",
        "hash_cont_tokens": "9ec87f6f54aa68d0"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Yemen|5": {
      "hashes": {
        "hash_examples": "18e9bcccbb4ced7a",
        "hash_full_prompts": "5bb795ad5c240147",
        "hash_input_tokens": "7c74b46821019e4d",
        "hash_cont_tokens": "bf7b0ddbcd40af39"
      },
      "truncated": 0,
      "non_truncated": 10,
      "padded": 20,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:communication|5": {
      "hashes": {
        "hash_examples": "9ff28ab5eab5c97b",
        "hash_full_prompts": "e6adba04b96e808d",
        "hash_input_tokens": "ea2f405591984b77",
        "hash_cont_tokens": "5284be22b16be911"
      },
      "truncated": 0,
      "non_truncated": 364,
      "padded": 728,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:computer_and_phone|5": {
      "hashes": {
        "hash_examples": "37bac2f086aaf6c2",
        "hash_full_prompts": "20ca19372ec6bb12",
        "hash_input_tokens": "08aec0416a1cdacf",
        "hash_cont_tokens": "0bd8068f53e1b6bc"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:daily_life|5": {
      "hashes": {
        "hash_examples": "bf07363c1c252e2f",
        "hash_full_prompts": "971ea143f1975413",
        "hash_input_tokens": "60bf1769ec8aeb55",
        "hash_cont_tokens": "9a40e5bcd8f77aac"
      },
      "truncated": 0,
      "non_truncated": 337,
      "padded": 674,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:entertainment|5": {
      "hashes": {
        "hash_examples": "37077bc00f0ac56a",
        "hash_full_prompts": "00278d9786c51665",
        "hash_input_tokens": "23d7cb02e2568d42",
        "hash_cont_tokens": "41a388c5221031f8"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:mcq_exams_test_ar|5": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "afb81428d520b7b9",
        "hash_input_tokens": "b951bbfb7a5544d2",
        "hash_cont_tokens": "207a824cc5fa5a81"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2228,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|5": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "8bde4662d93ca3e6",
        "hash_input_tokens": "f0608821a18f99c0",
        "hash_cont_tokens": "2722d2a261e02d22"
      },
      "truncated": 421,
      "non_truncated": 4974,
      "padded": 21125,
      "non_padded": 455,
      "effective_few_shots": 3.0535681186283594,
      "num_truncated_few_shots": 5395
    },
    "community|alghafa:meta_ar_msa|5": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "2673433e32c4fca7",
        "hash_input_tokens": "0eb3fcc4c2ad70d7",
        "hash_cont_tokens": "74f4bc3c2b473929"
      },
      "truncated": 79,
      "non_truncated": 816,
      "padded": 3499,
      "non_padded": 81,
      "effective_few_shots": 2.946368715083799,
      "num_truncated_few_shots": 895
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "39535bc07d629946",
        "hash_input_tokens": "dc45c54fd6f4c5d9",
        "hash_cont_tokens": "bf07c4e3b0af5d81"
      },
      "truncated": 2,
      "non_truncated": 73,
      "padded": 148,
      "non_padded": 2,
      "effective_few_shots": 1.6933333333333334,
      "num_truncated_few_shots": 75
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "0b2608bdd96b254d",
        "hash_input_tokens": "482d9de6fdcb1ef7",
        "hash_cont_tokens": "e2f824fe1985d97f"
      },
      "truncated": 22,
      "non_truncated": 128,
      "padded": 728,
      "non_padded": 22,
      "effective_few_shots": 4.02,
      "num_truncated_few_shots": 144
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "01e9ef715b0e7b40",
        "hash_input_tokens": "b5dd7069edbd38f3",
        "hash_cont_tokens": "4fb3a4182a513f38"
      },
      "truncated": 28,
      "non_truncated": 122,
      "padded": 721,
      "non_padded": 29,
      "effective_few_shots": 4.3,
      "num_truncated_few_shots": 105
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "f2c03fad137267e7",
        "hash_input_tokens": "026ffa310a6f9635",
        "hash_cont_tokens": "189017d806da1f86"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|5": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "fc2bbf3f82ee99d5",
        "hash_input_tokens": "e73982172a07121a",
        "hash_cont_tokens": "2a2e353bedb59923"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17985,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|5": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "1d942001b16272af",
        "hash_input_tokens": "1b652d9d125aa4df",
        "hash_cont_tokens": "2c166925cb36b6a6"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5160,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_exams|5": {
      "hashes": {
        "hash_examples": "6d721df351722656",
        "hash_full_prompts": "9241f1fcb58c868b",
        "hash_input_tokens": "759927185195f263",
        "hash_cont_tokens": "49da8b934ed112b6"
      },
      "truncated": 0,
      "non_truncated": 537,
      "padded": 2148,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "f2ddca8f45c0a511",
        "hash_full_prompts": "7dae76e9c966c8ee",
        "hash_input_tokens": "fafb6a28d60b17f7",
        "hash_cont_tokens": "b263f87c43f50bf2"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "dfdbc1b83107668d",
        "hash_full_prompts": "96b301d0d23a23b2",
        "hash_input_tokens": "9e43b1a1d42af705",
        "hash_cont_tokens": "b7cc7bcb7ec8a7aa"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "9736a606002a848e",
        "hash_full_prompts": "18aab550cbf246fb",
        "hash_input_tokens": "79e16bc1396e5ecf",
        "hash_cont_tokens": "9e1e328e730f3680"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "735e452fbb6dc63d",
        "hash_full_prompts": "0b6d539453c08221",
        "hash_input_tokens": "2e2c6f56b11dad9c",
        "hash_cont_tokens": "72382ac035bcefc5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "6ab0ca4da98aedcf",
        "hash_full_prompts": "35e95a711827936f",
        "hash_input_tokens": "5345572e93081008",
        "hash_cont_tokens": "38615ca940967377"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "17e4e390848018a4",
        "hash_full_prompts": "27484c8bd9570e0e",
        "hash_input_tokens": "9d9cfc940b79d8d3",
        "hash_cont_tokens": "7419aea2cef5901a"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "4abb169f6dfd234b",
        "hash_full_prompts": "bb78f6101049e7f6",
        "hash_input_tokens": "e928ace5287ba03b",
        "hash_cont_tokens": "3ac02ef84524e7dd"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "a369e2e941358a1e",
        "hash_full_prompts": "154a49a7f108784b",
        "hash_input_tokens": "f2f249aa5f0f1284",
        "hash_cont_tokens": "8d4a4677405eb0e0"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "d7be03b8b6020bff",
        "hash_full_prompts": "e6a4d0be1daf1046",
        "hash_input_tokens": "cbed9639db593d0f",
        "hash_cont_tokens": "4e174c8ef49e482e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "0518a00f097346bf",
        "hash_full_prompts": "77eb0c4b4114f286",
        "hash_input_tokens": "b917ae550d617461",
        "hash_cont_tokens": "d48072f7c5d89cde"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "5d842cd49bc70e12",
        "hash_full_prompts": "790ba8d9706901f1",
        "hash_input_tokens": "ce36849c71167f74",
        "hash_cont_tokens": "bd819cf775a173d0"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "8e85d9f85be9b32f",
        "hash_full_prompts": "82e46497a611b83c",
        "hash_input_tokens": "c07864199cfdeb1d",
        "hash_cont_tokens": "f5c2c728cce18702"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "7964b55a0a49502b",
        "hash_full_prompts": "7ba8d0ef2ee4c1ad",
        "hash_input_tokens": "0f37b980c9d7306a",
        "hash_cont_tokens": "1a6386192520b9bb"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "1e192eae38347257",
        "hash_full_prompts": "aaf20c2b20aa94cb",
        "hash_input_tokens": "a4f6d0c7ef03a85b",
        "hash_cont_tokens": "90e20c39427af1d5"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "cf97671d5c441da1",
        "hash_full_prompts": "0be6a375839169cf",
        "hash_input_tokens": "1f702e50a4a6e7d7",
        "hash_cont_tokens": "34b31b2592e08cdf"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "6f49107ed43c40c5",
        "hash_full_prompts": "40389b2e90ff570d",
        "hash_input_tokens": "3159441d2ad0835f",
        "hash_cont_tokens": "bdefb60938861d38"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "7922c376008ba77b",
        "hash_full_prompts": "fdb00d752feca5dc",
        "hash_input_tokens": "c037865384ab7f70",
        "hash_cont_tokens": "fc181b23960b36f4"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "11f9813185047d5b",
        "hash_full_prompts": "c77b6431a7de1a69",
        "hash_input_tokens": "ac1ac8c42ddbab76",
        "hash_cont_tokens": "f95d62089c8ed179"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "2a804b1d90cbe66e",
        "hash_full_prompts": "73da779f407e8b11",
        "hash_input_tokens": "a5e55c116caad5b1",
        "hash_cont_tokens": "c9b2639d5a61e72a"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "0032168adabc53b4",
        "hash_full_prompts": "f32312ef923f9535",
        "hash_input_tokens": "b2f65fb0a888cb98",
        "hash_cont_tokens": "1e8dffebb056a02e"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "f2fb8740f9df980f",
        "hash_full_prompts": "503becb3ca3fa1fd",
        "hash_input_tokens": "8a3a6e8c40a8ecad",
        "hash_cont_tokens": "2e9575ed05ebd94e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 4.69,
      "num_truncated_few_shots": 31
    },
    "community|arabic_mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "73509021e7e66435",
        "hash_full_prompts": "87a2b1b5bd581c69",
        "hash_input_tokens": "9341c7f7f151feeb",
        "hash_cont_tokens": "6ff5301d291baa97"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 4.945454545454545,
      "num_truncated_few_shots": 5
    },
    "community|arabic_mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "9e08d1894940ff42",
        "hash_full_prompts": "f4585f814c7f4de2",
        "hash_input_tokens": "cb269c58f56cd683",
        "hash_cont_tokens": "a1da15981daf76ec"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "64b7e97817ca6c76",
        "hash_full_prompts": "add1904e5a48de31",
        "hash_input_tokens": "6b60a4c163b6a114",
        "hash_cont_tokens": "ae133a62e22c1234"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "9f582da8534bd2ef",
        "hash_full_prompts": "6ec9972b6419a57d",
        "hash_input_tokens": "9ee0ef36b58726de",
        "hash_cont_tokens": "3b7d111a44ffaebf"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd54f1c10d423c51",
        "hash_full_prompts": "7dbb722d48b01d1a",
        "hash_input_tokens": "7b851b5c8713b0d6",
        "hash_cont_tokens": "c4a03934c1a5085e"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "7037896925aaf42f",
        "hash_full_prompts": "cf06d07948536fde",
        "hash_input_tokens": "80dd0a36279cf0f2",
        "hash_cont_tokens": "b059b758067163da"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "60c3776215167dae",
        "hash_full_prompts": "e6d5f073ef80880e",
        "hash_input_tokens": "959661df19f3fc31",
        "hash_cont_tokens": "48d81aa34ec8f1eb"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "61176bfd5da1298f",
        "hash_full_prompts": "ee7f4f2064eae0a4",
        "hash_input_tokens": "c580c0a179524a3e",
        "hash_cont_tokens": "dc07d77ebfce7137"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "40dfeebd1ea10f76",
        "hash_full_prompts": "4375b93018e97c76",
        "hash_input_tokens": "49c63ad8c2ea2b4a",
        "hash_cont_tokens": "a8b4891266e2ffde"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 4.99537037037037,
      "num_truncated_few_shots": 1
    },
    "community|arabic_mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "03daa510ba917f4d",
        "hash_full_prompts": "44063e13b88958e9",
        "hash_input_tokens": "878678878c1676b6",
        "hash_cont_tokens": "ca81f2e2d3f49e38"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "be075ffd579f43c2",
        "hash_full_prompts": "746184e8b08a1f37",
        "hash_input_tokens": "cd9c4f3df8fe2465",
        "hash_cont_tokens": "e542499935dc4f8f"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 4.962025316455696,
      "num_truncated_few_shots": 3
    },
    "community|arabic_mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "caa5b69f640bd1ef",
        "hash_full_prompts": "ef1e39e2ed4b7062",
        "hash_input_tokens": "94758f7f75a79122",
        "hash_cont_tokens": "b92df3f50264dc64"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "5ed2e38fb25a3767",
        "hash_full_prompts": "97ed1d98bc4fe65a",
        "hash_input_tokens": "74eb9053ac576273",
        "hash_cont_tokens": "e0f3b2fb7618d161"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 524,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "4e3e9e28d1b96484",
        "hash_full_prompts": "10b514961edfc393",
        "hash_input_tokens": "0613b6a3fc8ad746",
        "hash_cont_tokens": "b8d8db34a082cd0a"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e264b755366310b3",
        "hash_full_prompts": "2e06fff9ab533915",
        "hash_input_tokens": "61825e39ff31bcbf",
        "hash_cont_tokens": "33e0904b0189cdcf"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 432,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "a4ab6965a3e38071",
        "hash_full_prompts": "23ecda0d323885aa",
        "hash_input_tokens": "1f95015484164efe",
        "hash_cont_tokens": "ffe3f685d1fb6270"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "b92320efa6636b40",
        "hash_full_prompts": "d176e1708e5d03dd",
        "hash_input_tokens": "e786ccfc52590e85",
        "hash_cont_tokens": "94eaa18241e707d9"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:management|5": {
      "hashes": {
        "hash_examples": "c9ee4872a850fe20",
        "hash_full_prompts": "2925872de18555d3",
        "hash_input_tokens": "e927643a5dd96ded",
        "hash_cont_tokens": "f3b6130d54aa6193"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "0c151b70f6a047e3",
        "hash_full_prompts": "14844c0e262b85c6",
        "hash_input_tokens": "670d1a7b01a1d69d",
        "hash_cont_tokens": "b37906819f17e5ef"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 936,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "513f6cb8fca3a24e",
        "hash_full_prompts": "7651deeb7bf367df",
        "hash_input_tokens": "c4face7425d529ef",
        "hash_cont_tokens": "bb9ac446bfb4bca7"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "259a190d635331db",
        "hash_full_prompts": "bc4aa9321c563043",
        "hash_input_tokens": "7f630081451b7053",
        "hash_cont_tokens": "912a6721be994972"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3132,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "b85052c48a0b7bc3",
        "hash_full_prompts": "3bfcc2ad730416a4",
        "hash_input_tokens": "faf1e80082274cec",
        "hash_cont_tokens": "7a09b4c9b71d9a6e"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "28d0b069ef00dd00",
        "hash_full_prompts": "21227b832f92977e",
        "hash_input_tokens": "0b758d7f260b33e2",
        "hash_cont_tokens": "efb4c129573a26ce"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "00c9bc5f1d305b2f",
        "hash_full_prompts": "5f7a9b4dabc0305e",
        "hash_input_tokens": "4429c2da3e8d11dd",
        "hash_cont_tokens": "381a06af519d868b"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1221,
      "non_padded": 3,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a458c08454a3fd5f",
        "hash_full_prompts": "02e6601b66c6ffac",
        "hash_input_tokens": "39fcc23434a1a6eb",
        "hash_cont_tokens": "26e50ab9392f13f8"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1244,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "d6a0ecbdbb670e9c",
        "hash_full_prompts": "5093e3964d99c472",
        "hash_input_tokens": "d9ab69cd047e82ad",
        "hash_cont_tokens": "7e31f5d8ef9aae96"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1296,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "b4a95fe480b6540e",
        "hash_full_prompts": "ca667d2f4db1c8ee",
        "hash_input_tokens": "98ac78ac76fe3d24",
        "hash_cont_tokens": "385f748165e1931c"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1128,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "c2be9651cdbdde3b",
        "hash_full_prompts": "4f56cdb1ed351fd3",
        "hash_input_tokens": "075be48b967a1342",
        "hash_cont_tokens": "fcdc5c9e34d816fd"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6120,
      "non_padded": 16,
      "effective_few_shots": 3.4237288135593222,
      "num_truncated_few_shots": 1534
    },
    "community|arabic_mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "26ce92416288f273",
        "hash_full_prompts": "97ab989d8507b2e3",
        "hash_input_tokens": "3f197f78e97ae712",
        "hash_cont_tokens": "7ecd066ca9be0fe1"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 4.988970588235294,
      "num_truncated_few_shots": 3
    },
    "community|arabic_mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "71ea5f182ea9a641",
        "hash_full_prompts": "571222520787adc2",
        "hash_input_tokens": "80eb92f33a493b0b",
        "hash_cont_tokens": "c3fa927173be9a62"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2448,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "125adc21f91f8d77",
        "hash_full_prompts": "917adaee95dc1ca4",
        "hash_input_tokens": "877dd955214a4d50",
        "hash_cont_tokens": "e317b162c82a0099"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 440,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "3c18b216c099fb26",
        "hash_full_prompts": "ceffe04eccc4da84",
        "hash_input_tokens": "e750527a636bee63",
        "hash_cont_tokens": "8a01c855d929d8bc"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 2.816326530612245,
      "num_truncated_few_shots": 245
    },
    "community|arabic_mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "3f2a9634cef7417d",
        "hash_full_prompts": "b3fccb5fe12ce4f8",
        "hash_input_tokens": "5a68e01c8cefb58b",
        "hash_cont_tokens": "4e7a17f35aad2165"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 804,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "22249da54056475e",
        "hash_full_prompts": "d1ed3520cfcb7c44",
        "hash_input_tokens": "e7ca90f89ce7140a",
        "hash_cont_tokens": "dd735fb6b291ed42"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:virology|5": {
      "hashes": {
        "hash_examples": "9d194b9471dc624e",
        "hash_full_prompts": "780f7b0e04e47c82",
        "hash_input_tokens": "982567943c517739",
        "hash_cont_tokens": "9a45358b27f7a080"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 663,
      "non_padded": 1,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "229e5fe50082b064",
        "hash_full_prompts": "0ed2b106f1fca3f3",
        "hash_input_tokens": "f5dcf9847ba18797",
        "hash_cont_tokens": "240ade13d52495aa"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_challenge_okapi_ar|5": {
      "hashes": {
        "hash_examples": "ab893807673bc355",
        "hash_full_prompts": "cd42d148b1c3e15f",
        "hash_input_tokens": "06be1fa81be0b275",
        "hash_cont_tokens": "b9d9b21585abad12"
      },
      "truncated": 0,
      "non_truncated": 1160,
      "padded": 4640,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_easy_ar|5": {
      "hashes": {
        "hash_examples": "acb688624acc3d04",
        "hash_full_prompts": "d99289bd63ebdf59",
        "hash_input_tokens": "37651584b1b77c8a",
        "hash_cont_tokens": "a738d5c58da6782e"
      },
      "truncated": 0,
      "non_truncated": 2364,
      "padded": 9456,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|boolq_ar|5": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "d17c3d0ae8b2259e",
        "hash_input_tokens": "02bdcee3f233f1a3",
        "hash_cont_tokens": "7c47138413104ca6"
      },
      "truncated": 20,
      "non_truncated": 3240,
      "padded": 6492,
      "non_padded": 28,
      "effective_few_shots": 4.739877300613497,
      "num_truncated_few_shots": 709
    },
    "community|copa_ext_ar|5": {
      "hashes": {
        "hash_examples": "9bb83301bb72eecf",
        "hash_full_prompts": "7ef156dc64bf3171",
        "hash_input_tokens": "3f63b9260097b510",
        "hash_cont_tokens": "b7e63bdb85202f13"
      },
      "truncated": 0,
      "non_truncated": 90,
      "padded": 180,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|5": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "0dddbaabf55d5d79",
        "hash_input_tokens": "2fc4444780bee7a9",
        "hash_cont_tokens": "0c094a7b60a8803b"
      },
      "truncated": 4779,
      "non_truncated": 4392,
      "padded": 31868,
      "non_padded": 4816,
      "effective_few_shots": 2.475193544869698,
      "num_truncated_few_shots": 9171
    },
    "community|mmlu_okapi_ar|5": {
      "hashes": {
        "hash_examples": "723b5371ad6fdc31",
        "hash_full_prompts": "aa753e06f1f70e0a",
        "hash_input_tokens": "03745e6da7239aa7",
        "hash_cont_tokens": "080f3f6bf9f026e6"
      },
      "truncated": 2244,
      "non_truncated": 10679,
      "padded": 49393,
      "non_padded": 2299,
      "effective_few_shots": 4.760814052464598,
      "num_truncated_few_shots": 1915
    },
    "community|openbook_qa_ext_ar|5": {
      "hashes": {
        "hash_examples": "923d41eb0aca93eb",
        "hash_full_prompts": "4ec4310687d06c92",
        "hash_input_tokens": "691fa9cd56d2f93e",
        "hash_cont_tokens": "d1a32c76d6d96367"
      },
      "truncated": 0,
      "non_truncated": 495,
      "padded": 1961,
      "non_padded": 19,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|piqa_ar|5": {
      "hashes": {
        "hash_examples": "94bc205a520d3ea0",
        "hash_full_prompts": "d8aa0901d77715e8",
        "hash_input_tokens": "56b31f678f55e5b1",
        "hash_cont_tokens": "6d2c8f490a7dfad4"
      },
      "truncated": 14,
      "non_truncated": 1819,
      "padded": 3650,
      "non_padded": 16,
      "effective_few_shots": 4.998908892525914,
      "num_truncated_few_shots": 1
    },
    "community|race_ar|5": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "abb077592364e3a9",
        "hash_input_tokens": "2882f12f6653366d",
        "hash_cont_tokens": "fc6520583ed7e608"
      },
      "truncated": 639,
      "non_truncated": 4290,
      "padded": 19041,
      "non_padded": 675,
      "effective_few_shots": 0.28626496246703187,
      "num_truncated_few_shots": 4929
    },
    "community|sciq_ar|5": {
      "hashes": {
        "hash_examples": "3d68195306537db9",
        "hash_full_prompts": "2a89aa32e23737cb",
        "hash_input_tokens": "18861a37f44dc412",
        "hash_cont_tokens": "a9b7ea5df6991cc7"
      },
      "truncated": 19,
      "non_truncated": 976,
      "padded": 3945,
      "non_padded": 35,
      "effective_few_shots": 2.7336683417085426,
      "num_truncated_few_shots": 995
    },
    "community|toxigen_ar|5": {
      "hashes": {
        "hash_examples": "1e139513004a9a2e",
        "hash_full_prompts": "1073f8d7361001b3",
        "hash_input_tokens": "c59e44360f00c498",
        "hash_cont_tokens": "d98b63d9971ee8a3"
      },
      "truncated": 0,
      "non_truncated": 935,
      "padded": 1870,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "lighteval|xstory_cloze:ar|0": {
      "hashes": {
        "hash_examples": "865426a22c787481",
        "hash_full_prompts": "865426a22c787481",
        "hash_input_tokens": "69d2c9a4aa68642c",
        "hash_cont_tokens": "cd75fba3263ab888"
      },
      "truncated": 0,
      "non_truncated": 1511,
      "padded": 3022,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "a18a51bf5511ce6d",
      "hash_full_prompts": "03bedff8679d0e45",
      "hash_input_tokens": "1b98c689647bae05",
      "hash_cont_tokens": "9d3b6cfc04a6f838"
    },
    "truncated": 8267,
    "non_truncated": 77620,
    "padded": 278818,
    "non_padded": 8497,
    "num_truncated_few_shots": 26156
  }
}