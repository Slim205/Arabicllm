{
  "config_general": {
    "lighteval_sha": "a98210fd3a2d1e8bface1c32b72ebd5017173a4c",
    "num_fewshot_seeds": 1,
    "override_batch_size": 32,
    "max_samples": null,
    "job_id": "",
    "start_time": 367647.980512146,
    "end_time": 369613.797596602,
    "total_evaluation_time_secondes": "1965.817084455979",
    "model_name": "EleutherAI/pythia-70m",
    "model_sha": "a39f36b100fe8a5377810d56c3f4789b9c53ac42",
    "model_dtype": "torch.float16",
    "model_size": "138.83 MB",
    "config": null
  },
  "results": {
    "community|acva:Algeria|0": {
      "acc_norm": 0.5230769230769231,
      "acc_norm_stderr": 0.0358596530894741
    },
    "community|acva:Ancient_Egypt|0": {
      "acc_norm": 0.050793650793650794,
      "acc_norm_stderr": 0.01239139518482262
    },
    "community|acva:Arab_Empire|0": {
      "acc_norm": 0.30943396226415093,
      "acc_norm_stderr": 0.028450154794118627
    },
    "community|acva:Arabic_Architecture|0": {
      "acc_norm": 0.4564102564102564,
      "acc_norm_stderr": 0.035761230969912135
    },
    "community|acva:Arabic_Art|0": {
      "acc_norm": 0.3435897435897436,
      "acc_norm_stderr": 0.03409627301409856
    },
    "community|acva:Arabic_Astronomy|0": {
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.03581804596782233
    },
    "community|acva:Arabic_Calligraphy|0": {
      "acc_norm": 0.47843137254901963,
      "acc_norm_stderr": 0.0313435870640056
    },
    "community|acva:Arabic_Ceremony|0": {
      "acc_norm": 0.518918918918919,
      "acc_norm_stderr": 0.036834092970087065
    },
    "community|acva:Arabic_Clothing|0": {
      "acc_norm": 0.5128205128205128,
      "acc_norm_stderr": 0.03588610523192215
    },
    "community|acva:Arabic_Culture|0": {
      "acc_norm": 0.23076923076923078,
      "acc_norm_stderr": 0.0302493752938313
    },
    "community|acva:Arabic_Food|0": {
      "acc_norm": 0.441025641025641,
      "acc_norm_stderr": 0.0356473293185358
    },
    "community|acva:Arabic_Funeral|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.050529115263991134
    },
    "community|acva:Arabic_Geography|0": {
      "acc_norm": 0.6068965517241379,
      "acc_norm_stderr": 0.040703290137070705
    },
    "community|acva:Arabic_History|0": {
      "acc_norm": 0.3076923076923077,
      "acc_norm_stderr": 0.03313653039774174
    },
    "community|acva:Arabic_Language_Origin|0": {
      "acc_norm": 0.5473684210526316,
      "acc_norm_stderr": 0.051339113773544845
    },
    "community|acva:Arabic_Literature|0": {
      "acc_norm": 0.4689655172413793,
      "acc_norm_stderr": 0.04158632762097828
    },
    "community|acva:Arabic_Math|0": {
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.03298070870085618
    },
    "community|acva:Arabic_Medicine|0": {
      "acc_norm": 0.46206896551724136,
      "acc_norm_stderr": 0.041546596717075474
    },
    "community|acva:Arabic_Music|0": {
      "acc_norm": 0.23741007194244604,
      "acc_norm_stderr": 0.036220593237998276
    },
    "community|acva:Arabic_Ornament|0": {
      "acc_norm": 0.4717948717948718,
      "acc_norm_stderr": 0.035840746749208334
    },
    "community|acva:Arabic_Philosophy|0": {
      "acc_norm": 0.5793103448275863,
      "acc_norm_stderr": 0.0411391498118926
    },
    "community|acva:Arabic_Physics_and_Chemistry|0": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03581804596782232
    },
    "community|acva:Arabic_Wedding|0": {
      "acc_norm": 0.41025641025641024,
      "acc_norm_stderr": 0.03531493712326671
    },
    "community|acva:Bahrain|0": {
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.06979205927323111
    },
    "community|acva:Comoros|0": {
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Egypt_modern|0": {
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.04794350420740798
    },
    "community|acva:InfluenceFromAncientEgypt|0": {
      "acc_norm": 0.6051282051282051,
      "acc_norm_stderr": 0.03509545602262038
    },
    "community|acva:InfluenceFromByzantium|0": {
      "acc_norm": 0.7172413793103448,
      "acc_norm_stderr": 0.03752833958003337
    },
    "community|acva:InfluenceFromChina|0": {
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.0317493043641267
    },
    "community|acva:InfluenceFromGreece|0": {
      "acc_norm": 0.6307692307692307,
      "acc_norm_stderr": 0.034648411418637566
    },
    "community|acva:InfluenceFromIslam|0": {
      "acc_norm": 0.296551724137931,
      "acc_norm_stderr": 0.03806142687309993
    },
    "community|acva:InfluenceFromPersia|0": {
      "acc_norm": 0.6971428571428572,
      "acc_norm_stderr": 0.03483414676585986
    },
    "community|acva:InfluenceFromRome|0": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.03549871080367708
    },
    "community|acva:Iraq|0": {
      "acc_norm": 0.5058823529411764,
      "acc_norm_stderr": 0.05455069703232772
    },
    "community|acva:Islam_Education|0": {
      "acc_norm": 0.4512820512820513,
      "acc_norm_stderr": 0.03572709860318392
    },
    "community|acva:Islam_branches_and_schools|0": {
      "acc_norm": 0.4342857142857143,
      "acc_norm_stderr": 0.037576101528126626
    },
    "community|acva:Islamic_law_system|0": {
      "acc_norm": 0.4256410256410256,
      "acc_norm_stderr": 0.035498710803677086
    },
    "community|acva:Jordan|0": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Kuwait|0": {
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.06666666666666667
    },
    "community|acva:Lebanon|0": {
      "acc_norm": 0.17777777777777778,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Libya|0": {
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.07491109582924914
    },
    "community|acva:Mauritania|0": {
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.07446027270295805
    },
    "community|acva:Mesopotamia_civilization|0": {
      "acc_norm": 0.5225806451612903,
      "acc_norm_stderr": 0.0402500394824441
    },
    "community|acva:Morocco|0": {
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.06267511942419628
    },
    "community|acva:Oman|0": {
      "acc_norm": 0.17777777777777778,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Palestine|0": {
      "acc_norm": 0.24705882352941178,
      "acc_norm_stderr": 0.047058823529411785
    },
    "community|acva:Qatar|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.07385489458759964
    },
    "community|acva:Saudi_Arabia|0": {
      "acc_norm": 0.3282051282051282,
      "acc_norm_stderr": 0.03371243782413707
    },
    "community|acva:Somalia|0": {
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Sudan|0": {
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Syria|0": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Tunisia|0": {
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.06979205927323111
    },
    "community|acva:United_Arab_Emirates|0": {
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.04628210543937907
    },
    "community|acva:Yemen|0": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.13333333333333333
    },
    "community|acva:communication|0": {
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.025974025974025955
    },
    "community|acva:computer_and_phone|0": {
      "acc_norm": 0.45084745762711864,
      "acc_norm_stderr": 0.02901934773187137
    },
    "community|acva:daily_life|0": {
      "acc_norm": 0.18694362017804153,
      "acc_norm_stderr": 0.021268948348414647
    },
    "community|acva:entertainment|0": {
      "acc_norm": 0.23389830508474577,
      "acc_norm_stderr": 0.024687839412166384
    },
    "community|alghafa:mcq_exams_test_ar|0": {
      "acc_norm": 0.26750448833034113,
      "acc_norm_stderr": 0.018772867927944355
    },
    "community|alghafa:meta_ar_dialects|0": {
      "acc_norm": 0.24355885078776646,
      "acc_norm_stderr": 0.005844320977521911
    },
    "community|alghafa:meta_ar_msa|0": {
      "acc_norm": 0.2536312849162011,
      "acc_norm_stderr": 0.014551553659369918
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.05799451149344531
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": {
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.040665603096078466
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": {
      "acc_norm": 0.2733333333333333,
      "acc_norm_stderr": 0.03651075250486199
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": {
      "acc_norm": 0.5049405878674171,
      "acc_norm_stderr": 0.005591994422847942
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|0": {
      "acc_norm": 0.33878231859883234,
      "acc_norm_stderr": 0.006113279156829171
    },
    "community|alghafa:multiple_choice_sentiment_task|0": {
      "acc_norm": 0.3453488372093023,
      "acc_norm_stderr": 0.011468220445671152
    },
    "community|arabic_exams|0": {
      "acc_norm": 0.23649906890130354,
      "acc_norm_stderr": 0.018354269670319875
    },
    "community|arabic_mmlu:abstract_algebra|0": {
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932268
    },
    "community|arabic_mmlu:anatomy|0": {
      "acc_norm": 0.18518518518518517,
      "acc_norm_stderr": 0.03355677216313142
    },
    "community|arabic_mmlu:astronomy|0": {
      "acc_norm": 0.17763157894736842,
      "acc_norm_stderr": 0.031103182383123398
    },
    "community|arabic_mmlu:business_ethics|0": {
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "community|arabic_mmlu:clinical_knowledge|0": {
      "acc_norm": 0.21509433962264152,
      "acc_norm_stderr": 0.02528839450289137
    },
    "community|arabic_mmlu:college_biology|0": {
      "acc_norm": 0.2569444444444444,
      "acc_norm_stderr": 0.03653946969442099
    },
    "community|arabic_mmlu:college_chemistry|0": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "community|arabic_mmlu:college_computer_science|0": {
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "community|arabic_mmlu:college_mathematics|0": {
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "community|arabic_mmlu:college_medicine|0": {
      "acc_norm": 0.20809248554913296,
      "acc_norm_stderr": 0.030952890217749874
    },
    "community|arabic_mmlu:college_physics|0": {
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237654
    },
    "community|arabic_mmlu:computer_security|0": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "community|arabic_mmlu:conceptual_physics|0": {
      "acc_norm": 0.26382978723404255,
      "acc_norm_stderr": 0.028809989854102973
    },
    "community|arabic_mmlu:econometrics|0": {
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.039994238792813365
    },
    "community|arabic_mmlu:electrical_engineering|0": {
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.03565998174135302
    },
    "community|arabic_mmlu:elementary_mathematics|0": {
      "acc_norm": 0.20899470899470898,
      "acc_norm_stderr": 0.02094048156533486
    },
    "community|arabic_mmlu:formal_logic|0": {
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.04040610178208841
    },
    "community|arabic_mmlu:global_facts|0": {
      "acc_norm": 0.18,
      "acc_norm_stderr": 0.038612291966536934
    },
    "community|arabic_mmlu:high_school_biology|0": {
      "acc_norm": 0.1774193548387097,
      "acc_norm_stderr": 0.02173254068932927
    },
    "community|arabic_mmlu:high_school_chemistry|0": {
      "acc_norm": 0.15270935960591134,
      "acc_norm_stderr": 0.02530890453938063
    },
    "community|arabic_mmlu:high_school_computer_science|0": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "community|arabic_mmlu:high_school_european_history|0": {
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03225078108306289
    },
    "community|arabic_mmlu:high_school_geography|0": {
      "acc_norm": 0.17676767676767677,
      "acc_norm_stderr": 0.027178752639044915
    },
    "community|arabic_mmlu:high_school_government_and_politics|0": {
      "acc_norm": 0.19689119170984457,
      "acc_norm_stderr": 0.028697873971860664
    },
    "community|arabic_mmlu:high_school_macroeconomics|0": {
      "acc_norm": 0.20256410256410257,
      "acc_norm_stderr": 0.020377660970371372
    },
    "community|arabic_mmlu:high_school_mathematics|0": {
      "acc_norm": 0.2111111111111111,
      "acc_norm_stderr": 0.024882116857655075
    },
    "community|arabic_mmlu:high_school_microeconomics|0": {
      "acc_norm": 0.21008403361344538,
      "acc_norm_stderr": 0.026461398717471874
    },
    "community|arabic_mmlu:high_school_physics|0": {
      "acc_norm": 0.1986754966887417,
      "acc_norm_stderr": 0.03257847384436776
    },
    "community|arabic_mmlu:high_school_psychology|0": {
      "acc_norm": 0.1926605504587156,
      "acc_norm_stderr": 0.016909276884936094
    },
    "community|arabic_mmlu:high_school_statistics|0": {
      "acc_norm": 0.16203703703703703,
      "acc_norm_stderr": 0.02513045365226846
    },
    "community|arabic_mmlu:high_school_us_history|0": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03039153369274154
    },
    "community|arabic_mmlu:high_school_world_history|0": {
      "acc_norm": 0.270042194092827,
      "acc_norm_stderr": 0.028900721906293426
    },
    "community|arabic_mmlu:human_aging|0": {
      "acc_norm": 0.31390134529147984,
      "acc_norm_stderr": 0.031146796482972465
    },
    "community|arabic_mmlu:human_sexuality|0": {
      "acc_norm": 0.2595419847328244,
      "acc_norm_stderr": 0.03844876139785271
    },
    "community|arabic_mmlu:international_law|0": {
      "acc_norm": 0.2396694214876033,
      "acc_norm_stderr": 0.03896878985070417
    },
    "community|arabic_mmlu:jurisprudence|0": {
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.042365112580946336
    },
    "community|arabic_mmlu:logical_fallacies|0": {
      "acc_norm": 0.22085889570552147,
      "acc_norm_stderr": 0.032591773927421776
    },
    "community|arabic_mmlu:machine_learning|0": {
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.043994650575715215
    },
    "community|arabic_mmlu:management|0": {
      "acc_norm": 0.17475728155339806,
      "acc_norm_stderr": 0.037601780060266224
    },
    "community|arabic_mmlu:marketing|0": {
      "acc_norm": 0.2905982905982906,
      "acc_norm_stderr": 0.02974504857267404
    },
    "community|arabic_mmlu:medical_genetics|0": {
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "community|arabic_mmlu:miscellaneous|0": {
      "acc_norm": 0.23754789272030652,
      "acc_norm_stderr": 0.015218733046150193
    },
    "community|arabic_mmlu:moral_disputes|0": {
      "acc_norm": 0.24855491329479767,
      "acc_norm_stderr": 0.023267528432100174
    },
    "community|arabic_mmlu:moral_scenarios|0": {
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "community|arabic_mmlu:nutrition|0": {
      "acc_norm": 0.22549019607843138,
      "acc_norm_stderr": 0.023929155517351284
    },
    "community|arabic_mmlu:philosophy|0": {
      "acc_norm": 0.1864951768488746,
      "acc_norm_stderr": 0.02212243977248077
    },
    "community|arabic_mmlu:prehistory|0": {
      "acc_norm": 0.21604938271604937,
      "acc_norm_stderr": 0.022899162918445806
    },
    "community|arabic_mmlu:professional_accounting|0": {
      "acc_norm": 0.23404255319148937,
      "acc_norm_stderr": 0.025257861359432417
    },
    "community|arabic_mmlu:professional_law|0": {
      "acc_norm": 0.2457627118644068,
      "acc_norm_stderr": 0.010996156635142692
    },
    "community|arabic_mmlu:professional_medicine|0": {
      "acc_norm": 0.18382352941176472,
      "acc_norm_stderr": 0.023529242185193106
    },
    "community|arabic_mmlu:professional_psychology|0": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.01751781884501444
    },
    "community|arabic_mmlu:public_relations|0": {
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03955932861795833
    },
    "community|arabic_mmlu:security_studies|0": {
      "acc_norm": 0.18775510204081633,
      "acc_norm_stderr": 0.02500025603954621
    },
    "community|arabic_mmlu:sociology|0": {
      "acc_norm": 0.24378109452736318,
      "acc_norm_stderr": 0.03036049015401465
    },
    "community|arabic_mmlu:us_foreign_policy|0": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "community|arabic_mmlu:virology|0": {
      "acc_norm": 0.28313253012048195,
      "acc_norm_stderr": 0.03507295431370518
    },
    "community|arabic_mmlu:world_religions|0": {
      "acc_norm": 0.3216374269005848,
      "acc_norm_stderr": 0.03582529442573122
    },
    "community|arc_challenge_okapi_ar|0": {
      "acc_norm": 0.27155172413793105,
      "acc_norm_stderr": 0.01306423320277828
    },
    "community|arc_easy_ar|0": {
      "acc_norm": 0.2512690355329949,
      "acc_norm_stderr": 0.008922786933884949
    },
    "community|boolq_ar|0": {
      "acc_norm": 0.5834355828220859,
      "acc_norm_stderr": 0.008635656537237336
    },
    "community|copa_ext_ar|0": {
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.05298680599073449
    },
    "community|hellaswag_okapi_ar|0": {
      "acc_norm": 0.24479337040671684,
      "acc_norm_stderr": 0.004490017720328116
    },
    "community|mmlu_okapi_ar|0": {
      "acc_norm": 0.2680492145786582,
      "acc_norm_stderr": 0.0038965777088465713
    },
    "community|openbook_qa_ext_ar|0": {
      "acc_norm": 0.3090909090909091,
      "acc_norm_stderr": 0.020791704541704155
    },
    "community|piqa_ar|0": {
      "acc_norm": 0.49863611565739224,
      "acc_norm_stderr": 0.011681689377555815
    },
    "community|race_ar|0": {
      "acc_norm": 0.2807871779265571,
      "acc_norm_stderr": 0.006401503644617772
    },
    "community|sciq_ar|0": {
      "acc_norm": 0.33668341708542715,
      "acc_norm_stderr": 0.014989187710964106
    },
    "community|toxigen_ar|0": {
      "acc_norm": 0.4310160427807487,
      "acc_norm_stderr": 0.016204039390071805
    },
    "lighteval|xstory_cloze:ar|0": {
      "acc": 0.47319655857048315,
      "acc_stderr": 0.012848623899505774
    },
    "community|acva:_average|0": {
      "acc_norm": 0.39487285943228706,
      "acc_norm_stderr": 0.0457891663893322
    },
    "community|alghafa:_average|0": {
      "acc_norm": 0.35560367048628083,
      "acc_norm_stderr": 0.021945900409396688
    },
    "community|arabic_mmlu:_average|0": {
      "acc_norm": 0.23133101869860961,
      "acc_norm_stderr": 0.03150862743213405
    },
    "all": {
      "acc_norm": 0.3197766727579378,
      "acc_norm_stderr": 0.035512463164165606,
      "acc": 0.47319655857048315,
      "acc_stderr": 0.012848623899505774
    }
  },
  "versions": {
    "community|acva:Algeria|0": 0,
    "community|acva:Ancient_Egypt|0": 0,
    "community|acva:Arab_Empire|0": 0,
    "community|acva:Arabic_Architecture|0": 0,
    "community|acva:Arabic_Art|0": 0,
    "community|acva:Arabic_Astronomy|0": 0,
    "community|acva:Arabic_Calligraphy|0": 0,
    "community|acva:Arabic_Ceremony|0": 0,
    "community|acva:Arabic_Clothing|0": 0,
    "community|acva:Arabic_Culture|0": 0,
    "community|acva:Arabic_Food|0": 0,
    "community|acva:Arabic_Funeral|0": 0,
    "community|acva:Arabic_Geography|0": 0,
    "community|acva:Arabic_History|0": 0,
    "community|acva:Arabic_Language_Origin|0": 0,
    "community|acva:Arabic_Literature|0": 0,
    "community|acva:Arabic_Math|0": 0,
    "community|acva:Arabic_Medicine|0": 0,
    "community|acva:Arabic_Music|0": 0,
    "community|acva:Arabic_Ornament|0": 0,
    "community|acva:Arabic_Philosophy|0": 0,
    "community|acva:Arabic_Physics_and_Chemistry|0": 0,
    "community|acva:Arabic_Wedding|0": 0,
    "community|acva:Bahrain|0": 0,
    "community|acva:Comoros|0": 0,
    "community|acva:Egypt_modern|0": 0,
    "community|acva:InfluenceFromAncientEgypt|0": 0,
    "community|acva:InfluenceFromByzantium|0": 0,
    "community|acva:InfluenceFromChina|0": 0,
    "community|acva:InfluenceFromGreece|0": 0,
    "community|acva:InfluenceFromIslam|0": 0,
    "community|acva:InfluenceFromPersia|0": 0,
    "community|acva:InfluenceFromRome|0": 0,
    "community|acva:Iraq|0": 0,
    "community|acva:Islam_Education|0": 0,
    "community|acva:Islam_branches_and_schools|0": 0,
    "community|acva:Islamic_law_system|0": 0,
    "community|acva:Jordan|0": 0,
    "community|acva:Kuwait|0": 0,
    "community|acva:Lebanon|0": 0,
    "community|acva:Libya|0": 0,
    "community|acva:Mauritania|0": 0,
    "community|acva:Mesopotamia_civilization|0": 0,
    "community|acva:Morocco|0": 0,
    "community|acva:Oman|0": 0,
    "community|acva:Palestine|0": 0,
    "community|acva:Qatar|0": 0,
    "community|acva:Saudi_Arabia|0": 0,
    "community|acva:Somalia|0": 0,
    "community|acva:Sudan|0": 0,
    "community|acva:Syria|0": 0,
    "community|acva:Tunisia|0": 0,
    "community|acva:United_Arab_Emirates|0": 0,
    "community|acva:Yemen|0": 0,
    "community|acva:communication|0": 0,
    "community|acva:computer_and_phone|0": 0,
    "community|acva:daily_life|0": 0,
    "community|acva:entertainment|0": 0,
    "community|alghafa:mcq_exams_test_ar|0": 0,
    "community|alghafa:meta_ar_dialects|0": 0,
    "community|alghafa:meta_ar_msa|0": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|0": 0,
    "community|alghafa:multiple_choice_sentiment_task|0": 0,
    "community|arabic_exams|0": 0,
    "community|arabic_mmlu:abstract_algebra|0": 0,
    "community|arabic_mmlu:anatomy|0": 0,
    "community|arabic_mmlu:astronomy|0": 0,
    "community|arabic_mmlu:business_ethics|0": 0,
    "community|arabic_mmlu:clinical_knowledge|0": 0,
    "community|arabic_mmlu:college_biology|0": 0,
    "community|arabic_mmlu:college_chemistry|0": 0,
    "community|arabic_mmlu:college_computer_science|0": 0,
    "community|arabic_mmlu:college_mathematics|0": 0,
    "community|arabic_mmlu:college_medicine|0": 0,
    "community|arabic_mmlu:college_physics|0": 0,
    "community|arabic_mmlu:computer_security|0": 0,
    "community|arabic_mmlu:conceptual_physics|0": 0,
    "community|arabic_mmlu:econometrics|0": 0,
    "community|arabic_mmlu:electrical_engineering|0": 0,
    "community|arabic_mmlu:elementary_mathematics|0": 0,
    "community|arabic_mmlu:formal_logic|0": 0,
    "community|arabic_mmlu:global_facts|0": 0,
    "community|arabic_mmlu:high_school_biology|0": 0,
    "community|arabic_mmlu:high_school_chemistry|0": 0,
    "community|arabic_mmlu:high_school_computer_science|0": 0,
    "community|arabic_mmlu:high_school_european_history|0": 0,
    "community|arabic_mmlu:high_school_geography|0": 0,
    "community|arabic_mmlu:high_school_government_and_politics|0": 0,
    "community|arabic_mmlu:high_school_macroeconomics|0": 0,
    "community|arabic_mmlu:high_school_mathematics|0": 0,
    "community|arabic_mmlu:high_school_microeconomics|0": 0,
    "community|arabic_mmlu:high_school_physics|0": 0,
    "community|arabic_mmlu:high_school_psychology|0": 0,
    "community|arabic_mmlu:high_school_statistics|0": 0,
    "community|arabic_mmlu:high_school_us_history|0": 0,
    "community|arabic_mmlu:high_school_world_history|0": 0,
    "community|arabic_mmlu:human_aging|0": 0,
    "community|arabic_mmlu:human_sexuality|0": 0,
    "community|arabic_mmlu:international_law|0": 0,
    "community|arabic_mmlu:jurisprudence|0": 0,
    "community|arabic_mmlu:logical_fallacies|0": 0,
    "community|arabic_mmlu:machine_learning|0": 0,
    "community|arabic_mmlu:management|0": 0,
    "community|arabic_mmlu:marketing|0": 0,
    "community|arabic_mmlu:medical_genetics|0": 0,
    "community|arabic_mmlu:miscellaneous|0": 0,
    "community|arabic_mmlu:moral_disputes|0": 0,
    "community|arabic_mmlu:moral_scenarios|0": 0,
    "community|arabic_mmlu:nutrition|0": 0,
    "community|arabic_mmlu:philosophy|0": 0,
    "community|arabic_mmlu:prehistory|0": 0,
    "community|arabic_mmlu:professional_accounting|0": 0,
    "community|arabic_mmlu:professional_law|0": 0,
    "community|arabic_mmlu:professional_medicine|0": 0,
    "community|arabic_mmlu:professional_psychology|0": 0,
    "community|arabic_mmlu:public_relations|0": 0,
    "community|arabic_mmlu:security_studies|0": 0,
    "community|arabic_mmlu:sociology|0": 0,
    "community|arabic_mmlu:us_foreign_policy|0": 0,
    "community|arabic_mmlu:virology|0": 0,
    "community|arabic_mmlu:world_religions|0": 0,
    "community|arc_challenge_okapi_ar|0": 0,
    "community|arc_easy_ar|0": 0,
    "community|boolq_ar|0": 0,
    "community|copa_ext_ar|0": 0,
    "community|hellaswag_okapi_ar|0": 0,
    "community|mmlu_okapi_ar|0": 0,
    "community|openbook_qa_ext_ar|0": 0,
    "community|piqa_ar|0": 0,
    "community|race_ar|0": 0,
    "community|sciq_ar|0": 0,
    "community|toxigen_ar|0": 0,
    "lighteval|xstory_cloze:ar|0": 0
  },
  "config_tasks": {
    "community|acva:Algeria": {
      "name": "acva:Algeria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Algeria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Ancient_Egypt": {
      "name": "acva:Ancient_Egypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Ancient_Egypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 315,
      "effective_num_docs": 315,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arab_Empire": {
      "name": "acva:Arab_Empire",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arab_Empire",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Architecture": {
      "name": "acva:Arabic_Architecture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Architecture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Art": {
      "name": "acva:Arabic_Art",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Art",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Astronomy": {
      "name": "acva:Arabic_Astronomy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Calligraphy": {
      "name": "acva:Arabic_Calligraphy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Calligraphy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 255,
      "effective_num_docs": 255,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ceremony": {
      "name": "acva:Arabic_Ceremony",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ceremony",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 185,
      "effective_num_docs": 185,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Clothing": {
      "name": "acva:Arabic_Clothing",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Clothing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Culture": {
      "name": "acva:Arabic_Culture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Culture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Food": {
      "name": "acva:Arabic_Food",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Food",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Funeral": {
      "name": "acva:Arabic_Funeral",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Funeral",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Geography": {
      "name": "acva:Arabic_Geography",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_History": {
      "name": "acva:Arabic_History",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_History",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Language_Origin": {
      "name": "acva:Arabic_Language_Origin",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Language_Origin",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Literature": {
      "name": "acva:Arabic_Literature",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Literature",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Math": {
      "name": "acva:Arabic_Math",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Math",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Medicine": {
      "name": "acva:Arabic_Medicine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Music": {
      "name": "acva:Arabic_Music",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Music",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 139,
      "effective_num_docs": 139,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ornament": {
      "name": "acva:Arabic_Ornament",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ornament",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Philosophy": {
      "name": "acva:Arabic_Philosophy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry": {
      "name": "acva:Arabic_Physics_and_Chemistry",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Physics_and_Chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Wedding": {
      "name": "acva:Arabic_Wedding",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Wedding",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Bahrain": {
      "name": "acva:Bahrain",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Bahrain",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Comoros": {
      "name": "acva:Comoros",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Comoros",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Egypt_modern": {
      "name": "acva:Egypt_modern",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Egypt_modern",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromAncientEgypt": {
      "name": "acva:InfluenceFromAncientEgypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromAncientEgypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromByzantium": {
      "name": "acva:InfluenceFromByzantium",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromByzantium",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromChina": {
      "name": "acva:InfluenceFromChina",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromChina",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromGreece": {
      "name": "acva:InfluenceFromGreece",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromGreece",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromIslam": {
      "name": "acva:InfluenceFromIslam",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromIslam",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromPersia": {
      "name": "acva:InfluenceFromPersia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromPersia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromRome": {
      "name": "acva:InfluenceFromRome",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromRome",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Iraq": {
      "name": "acva:Iraq",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Iraq",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_Education": {
      "name": "acva:Islam_Education",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_Education",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_branches_and_schools": {
      "name": "acva:Islam_branches_and_schools",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_branches_and_schools",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islamic_law_system": {
      "name": "acva:Islamic_law_system",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islamic_law_system",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Jordan": {
      "name": "acva:Jordan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Jordan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Kuwait": {
      "name": "acva:Kuwait",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Kuwait",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Lebanon": {
      "name": "acva:Lebanon",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Lebanon",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Libya": {
      "name": "acva:Libya",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Libya",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mauritania": {
      "name": "acva:Mauritania",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mauritania",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mesopotamia_civilization": {
      "name": "acva:Mesopotamia_civilization",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mesopotamia_civilization",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 155,
      "effective_num_docs": 155,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Morocco": {
      "name": "acva:Morocco",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Morocco",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Oman": {
      "name": "acva:Oman",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Oman",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Palestine": {
      "name": "acva:Palestine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Palestine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Qatar": {
      "name": "acva:Qatar",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Qatar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Saudi_Arabia": {
      "name": "acva:Saudi_Arabia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Saudi_Arabia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Somalia": {
      "name": "acva:Somalia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Somalia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Sudan": {
      "name": "acva:Sudan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Sudan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Syria": {
      "name": "acva:Syria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Syria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Tunisia": {
      "name": "acva:Tunisia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Tunisia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:United_Arab_Emirates": {
      "name": "acva:United_Arab_Emirates",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "United_Arab_Emirates",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Yemen": {
      "name": "acva:Yemen",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Yemen",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 10,
      "effective_num_docs": 10,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:communication": {
      "name": "acva:communication",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "communication",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 364,
      "effective_num_docs": 364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:computer_and_phone": {
      "name": "acva:computer_and_phone",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "computer_and_phone",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:daily_life": {
      "name": "acva:daily_life",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "daily_life",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 337,
      "effective_num_docs": 337,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:entertainment": {
      "name": "acva:entertainment",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "entertainment",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_exams": {
      "name": "arabic_exams",
      "prompt_function": "arabic_exams",
      "hf_repo": "OALL/Arabic_EXAMS",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 537,
      "effective_num_docs": 537,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:abstract_algebra": {
      "name": "arabic_mmlu:abstract_algebra",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:anatomy": {
      "name": "arabic_mmlu:anatomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:astronomy": {
      "name": "arabic_mmlu:astronomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:business_ethics": {
      "name": "arabic_mmlu:business_ethics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:clinical_knowledge": {
      "name": "arabic_mmlu:clinical_knowledge",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_biology": {
      "name": "arabic_mmlu:college_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_chemistry": {
      "name": "arabic_mmlu:college_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_computer_science": {
      "name": "arabic_mmlu:college_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_mathematics": {
      "name": "arabic_mmlu:college_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_medicine": {
      "name": "arabic_mmlu:college_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_physics": {
      "name": "arabic_mmlu:college_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:computer_security": {
      "name": "arabic_mmlu:computer_security",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:conceptual_physics": {
      "name": "arabic_mmlu:conceptual_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:econometrics": {
      "name": "arabic_mmlu:econometrics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:electrical_engineering": {
      "name": "arabic_mmlu:electrical_engineering",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:elementary_mathematics": {
      "name": "arabic_mmlu:elementary_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:formal_logic": {
      "name": "arabic_mmlu:formal_logic",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:global_facts": {
      "name": "arabic_mmlu:global_facts",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_biology": {
      "name": "arabic_mmlu:high_school_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_chemistry": {
      "name": "arabic_mmlu:high_school_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_computer_science": {
      "name": "arabic_mmlu:high_school_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_european_history": {
      "name": "arabic_mmlu:high_school_european_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_geography": {
      "name": "arabic_mmlu:high_school_geography",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics": {
      "name": "arabic_mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics": {
      "name": "arabic_mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_mathematics": {
      "name": "arabic_mmlu:high_school_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_microeconomics": {
      "name": "arabic_mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_physics": {
      "name": "arabic_mmlu:high_school_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_psychology": {
      "name": "arabic_mmlu:high_school_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_statistics": {
      "name": "arabic_mmlu:high_school_statistics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_us_history": {
      "name": "arabic_mmlu:high_school_us_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_world_history": {
      "name": "arabic_mmlu:high_school_world_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_aging": {
      "name": "arabic_mmlu:human_aging",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_sexuality": {
      "name": "arabic_mmlu:human_sexuality",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:international_law": {
      "name": "arabic_mmlu:international_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:jurisprudence": {
      "name": "arabic_mmlu:jurisprudence",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:logical_fallacies": {
      "name": "arabic_mmlu:logical_fallacies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:machine_learning": {
      "name": "arabic_mmlu:machine_learning",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:management": {
      "name": "arabic_mmlu:management",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:marketing": {
      "name": "arabic_mmlu:marketing",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:medical_genetics": {
      "name": "arabic_mmlu:medical_genetics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:miscellaneous": {
      "name": "arabic_mmlu:miscellaneous",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_disputes": {
      "name": "arabic_mmlu:moral_disputes",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_scenarios": {
      "name": "arabic_mmlu:moral_scenarios",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:nutrition": {
      "name": "arabic_mmlu:nutrition",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:philosophy": {
      "name": "arabic_mmlu:philosophy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:prehistory": {
      "name": "arabic_mmlu:prehistory",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_accounting": {
      "name": "arabic_mmlu:professional_accounting",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_law": {
      "name": "arabic_mmlu:professional_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_medicine": {
      "name": "arabic_mmlu:professional_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_psychology": {
      "name": "arabic_mmlu:professional_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:public_relations": {
      "name": "arabic_mmlu:public_relations",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:security_studies": {
      "name": "arabic_mmlu:security_studies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:sociology": {
      "name": "arabic_mmlu:sociology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:us_foreign_policy": {
      "name": "arabic_mmlu:us_foreign_policy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:virology": {
      "name": "arabic_mmlu:virology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:world_religions": {
      "name": "arabic_mmlu:world_religions",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_challenge_okapi_ar": {
      "name": "arc_challenge_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_challenge_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1160,
      "effective_num_docs": 1160,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_easy_ar": {
      "name": "arc_easy_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_easy_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 2364,
      "effective_num_docs": 2364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|copa_ext_ar": {
      "name": "copa_ext_ar",
      "prompt_function": "copa_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "copa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 90,
      "effective_num_docs": 90,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|mmlu_okapi_ar": {
      "name": "mmlu_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "mmlu_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 12923,
      "effective_num_docs": 12923,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|openbook_qa_ext_ar": {
      "name": "openbook_qa_ext_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "openbook_qa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 495,
      "effective_num_docs": 495,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|piqa_ar": {
      "name": "piqa_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "piqa_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1833,
      "effective_num_docs": 1833,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|sciq_ar": {
      "name": "sciq_ar",
      "prompt_function": "sciq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "sciq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 995,
      "effective_num_docs": 995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|toxigen_ar": {
      "name": "toxigen_ar",
      "prompt_function": "toxigen_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "toxigen_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 935,
      "effective_num_docs": 935,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "lighteval|xstory_cloze:ar": {
      "name": "xstory_cloze:ar",
      "prompt_function": "storycloze",
      "hf_repo": "juletxara/xstory_cloze",
      "hf_subset": "ar",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "training",
        "eval"
      ],
      "evaluation_splits": [
        "eval"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "lighteval"
      ],
      "original_num_docs": 1511,
      "effective_num_docs": 1511,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|acva:Algeria|0": {
      "hashes": {
        "hash_examples": "da5a3003cd46f6f9",
        "hash_full_prompts": "da5a3003cd46f6f9",
        "hash_input_tokens": "d112152194516d73",
        "hash_cont_tokens": "58b0c8b2e0ea2de0"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Ancient_Egypt|0": {
      "hashes": {
        "hash_examples": "52d6f767fede195b",
        "hash_full_prompts": "52d6f767fede195b",
        "hash_input_tokens": "6a2cddce8aeb2871",
        "hash_cont_tokens": "7c7f30f06f1126c0"
      },
      "truncated": 0,
      "non_truncated": 315,
      "padded": 630,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arab_Empire|0": {
      "hashes": {
        "hash_examples": "8dacff6a79804a75",
        "hash_full_prompts": "8dacff6a79804a75",
        "hash_input_tokens": "b882ca5dc62f1300",
        "hash_cont_tokens": "7fad996560f952a0"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 530,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Architecture|0": {
      "hashes": {
        "hash_examples": "df286cd862d9f6bb",
        "hash_full_prompts": "df286cd862d9f6bb",
        "hash_input_tokens": "f3542358275e85b6",
        "hash_cont_tokens": "1c2b55d9ca2ede63"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Art|0": {
      "hashes": {
        "hash_examples": "112883d764118a49",
        "hash_full_prompts": "112883d764118a49",
        "hash_input_tokens": "495d941e1013e3f5",
        "hash_cont_tokens": "e74cf4cda5feb0ca"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Astronomy|0": {
      "hashes": {
        "hash_examples": "20dcdf2454bf8671",
        "hash_full_prompts": "20dcdf2454bf8671",
        "hash_input_tokens": "2b34cbc77265ecf2",
        "hash_cont_tokens": "e74cf4cda5feb0ca"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Calligraphy|0": {
      "hashes": {
        "hash_examples": "3a9f9d1ebe868a15",
        "hash_full_prompts": "3a9f9d1ebe868a15",
        "hash_input_tokens": "a19e7f627d7110c6",
        "hash_cont_tokens": "43733169225ac13f"
      },
      "truncated": 0,
      "non_truncated": 255,
      "padded": 510,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ceremony|0": {
      "hashes": {
        "hash_examples": "c927630f8d2f44da",
        "hash_full_prompts": "c927630f8d2f44da",
        "hash_input_tokens": "7454c9dcafa0bfa4",
        "hash_cont_tokens": "a96f887cff29fccf"
      },
      "truncated": 0,
      "non_truncated": 185,
      "padded": 370,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Clothing|0": {
      "hashes": {
        "hash_examples": "6ad0740c2ac6ac92",
        "hash_full_prompts": "6ad0740c2ac6ac92",
        "hash_input_tokens": "64845a7570a62a23",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Culture|0": {
      "hashes": {
        "hash_examples": "2177bd857ad872ae",
        "hash_full_prompts": "2177bd857ad872ae",
        "hash_input_tokens": "70cef9464e7e2914",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Food|0": {
      "hashes": {
        "hash_examples": "a6ada65b71d7c9c5",
        "hash_full_prompts": "a6ada65b71d7c9c5",
        "hash_input_tokens": "7fdc290a8eb72d21",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Funeral|0": {
      "hashes": {
        "hash_examples": "fcee39dc29eaae91",
        "hash_full_prompts": "fcee39dc29eaae91",
        "hash_input_tokens": "f9a0e220c891b4f6",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Geography|0": {
      "hashes": {
        "hash_examples": "d36eda7c89231c02",
        "hash_full_prompts": "d36eda7c89231c02",
        "hash_input_tokens": "65e58a4fcede9719",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_History|0": {
      "hashes": {
        "hash_examples": "6354ac0d6db6a5fc",
        "hash_full_prompts": "6354ac0d6db6a5fc",
        "hash_input_tokens": "3161104573bbce30",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Language_Origin|0": {
      "hashes": {
        "hash_examples": "ddc967c8aca34402",
        "hash_full_prompts": "ddc967c8aca34402",
        "hash_input_tokens": "713f765c463a63a6",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Literature|0": {
      "hashes": {
        "hash_examples": "4305379fd46be5d8",
        "hash_full_prompts": "4305379fd46be5d8",
        "hash_input_tokens": "2710dfc819cfa628",
        "hash_cont_tokens": "f6180f792c3d45d0"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Math|0": {
      "hashes": {
        "hash_examples": "dec621144f4d28be",
        "hash_full_prompts": "dec621144f4d28be",
        "hash_input_tokens": "ec583aa27ff6eac5",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Medicine|0": {
      "hashes": {
        "hash_examples": "2b344cdae9495ff2",
        "hash_full_prompts": "2b344cdae9495ff2",
        "hash_input_tokens": "42edf6c5dfd636eb",
        "hash_cont_tokens": "85b9982120d61007"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Music|0": {
      "hashes": {
        "hash_examples": "0c54624d881944ce",
        "hash_full_prompts": "0c54624d881944ce",
        "hash_input_tokens": "c48dae8aaf2cf2a8",
        "hash_cont_tokens": "5aa58462a622ebdd"
      },
      "truncated": 0,
      "non_truncated": 139,
      "padded": 278,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ornament|0": {
      "hashes": {
        "hash_examples": "251a4a84289d8bc1",
        "hash_full_prompts": "251a4a84289d8bc1",
        "hash_input_tokens": "af582aca804e1e61",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Philosophy|0": {
      "hashes": {
        "hash_examples": "3f86fb9c94c13d22",
        "hash_full_prompts": "3f86fb9c94c13d22",
        "hash_input_tokens": "34b28ef1b6b96b1f",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry|0": {
      "hashes": {
        "hash_examples": "8fec65af3695b62a",
        "hash_full_prompts": "8fec65af3695b62a",
        "hash_input_tokens": "df1d5aaa6262ffba",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Wedding|0": {
      "hashes": {
        "hash_examples": "9cc3477184d7a4b8",
        "hash_full_prompts": "9cc3477184d7a4b8",
        "hash_input_tokens": "3a6dd3b4040c49ce",
        "hash_cont_tokens": "665997b53995b738"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Bahrain|0": {
      "hashes": {
        "hash_examples": "c92e803a0fa8b9e2",
        "hash_full_prompts": "c92e803a0fa8b9e2",
        "hash_input_tokens": "d4bd2e6ec806a7c8",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Comoros|0": {
      "hashes": {
        "hash_examples": "06e5d4bba8e54cae",
        "hash_full_prompts": "06e5d4bba8e54cae",
        "hash_input_tokens": "13448194e533acc4",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Egypt_modern|0": {
      "hashes": {
        "hash_examples": "c6ec369164f93446",
        "hash_full_prompts": "c6ec369164f93446",
        "hash_input_tokens": "3be0defa9ac77348",
        "hash_cont_tokens": "cb515731e143ab85"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromAncientEgypt|0": {
      "hashes": {
        "hash_examples": "b9d56d74818b9bd4",
        "hash_full_prompts": "b9d56d74818b9bd4",
        "hash_input_tokens": "07fd3f430b097a0a",
        "hash_cont_tokens": "9fdcc0d8a3800dca"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromByzantium|0": {
      "hashes": {
        "hash_examples": "5316c9624e7e59b8",
        "hash_full_prompts": "5316c9624e7e59b8",
        "hash_input_tokens": "1dd5fc8273ffb843",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromChina|0": {
      "hashes": {
        "hash_examples": "87894bce95a56411",
        "hash_full_prompts": "87894bce95a56411",
        "hash_input_tokens": "84e9dcd1e6fb3c68",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromGreece|0": {
      "hashes": {
        "hash_examples": "0baa78a27e469312",
        "hash_full_prompts": "0baa78a27e469312",
        "hash_input_tokens": "1c24072801538a19",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromIslam|0": {
      "hashes": {
        "hash_examples": "0c2532cde6541ff2",
        "hash_full_prompts": "0c2532cde6541ff2",
        "hash_input_tokens": "697d7e120cc0ebf1",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromPersia|0": {
      "hashes": {
        "hash_examples": "efcd8112dc53c6e5",
        "hash_full_prompts": "efcd8112dc53c6e5",
        "hash_input_tokens": "0ec057198ada8047",
        "hash_cont_tokens": "62d62b3e9e6183e3"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromRome|0": {
      "hashes": {
        "hash_examples": "9db61480e2e85fd3",
        "hash_full_prompts": "9db61480e2e85fd3",
        "hash_input_tokens": "fcb18a21bbdb3f69",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Iraq|0": {
      "hashes": {
        "hash_examples": "96dac3dfa8d2f41f",
        "hash_full_prompts": "96dac3dfa8d2f41f",
        "hash_input_tokens": "e048b464a49c3c2c",
        "hash_cont_tokens": "9ec87f6f54aa68d0"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_Education|0": {
      "hashes": {
        "hash_examples": "0d80355f6a4cb51b",
        "hash_full_prompts": "0d80355f6a4cb51b",
        "hash_input_tokens": "3eb75021857c4393",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_branches_and_schools|0": {
      "hashes": {
        "hash_examples": "5cedce1be2c3ad50",
        "hash_full_prompts": "5cedce1be2c3ad50",
        "hash_input_tokens": "8278e634db4b3882",
        "hash_cont_tokens": "62d62b3e9e6183e3"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islamic_law_system|0": {
      "hashes": {
        "hash_examples": "c0e6db8bc84e105e",
        "hash_full_prompts": "c0e6db8bc84e105e",
        "hash_input_tokens": "24bd284475736dde",
        "hash_cont_tokens": "fd6e4b67c6698dd4"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Jordan|0": {
      "hashes": {
        "hash_examples": "33deb5b4e5ddd6a1",
        "hash_full_prompts": "33deb5b4e5ddd6a1",
        "hash_input_tokens": "e99bef58d82c6877",
        "hash_cont_tokens": "6017f6603db77242"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Kuwait|0": {
      "hashes": {
        "hash_examples": "eb41773346d7c46c",
        "hash_full_prompts": "eb41773346d7c46c",
        "hash_input_tokens": "8327568d8987080e",
        "hash_cont_tokens": "cb3bdd5b3afc82e3"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Lebanon|0": {
      "hashes": {
        "hash_examples": "25932dbf4c13d34f",
        "hash_full_prompts": "25932dbf4c13d34f",
        "hash_input_tokens": "42b9067379dec57d",
        "hash_cont_tokens": "5bdd0eb0ad0cb51d"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Libya|0": {
      "hashes": {
        "hash_examples": "f2c4db63cd402926",
        "hash_full_prompts": "f2c4db63cd402926",
        "hash_input_tokens": "46f4e603ca6f4d57",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mauritania|0": {
      "hashes": {
        "hash_examples": "8723ab5fdf286b54",
        "hash_full_prompts": "8723ab5fdf286b54",
        "hash_input_tokens": "05452f93de228fc3",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mesopotamia_civilization|0": {
      "hashes": {
        "hash_examples": "c33f5502a6130ca9",
        "hash_full_prompts": "c33f5502a6130ca9",
        "hash_input_tokens": "ce30b7a5516ebb59",
        "hash_cont_tokens": "7df212f489cf2849"
      },
      "truncated": 0,
      "non_truncated": 155,
      "padded": 310,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Morocco|0": {
      "hashes": {
        "hash_examples": "588a5ed27904b1ae",
        "hash_full_prompts": "588a5ed27904b1ae",
        "hash_input_tokens": "443e7090140f10bf",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Oman|0": {
      "hashes": {
        "hash_examples": "d447c52b94248b69",
        "hash_full_prompts": "d447c52b94248b69",
        "hash_input_tokens": "57ba83def6290ce3",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Palestine|0": {
      "hashes": {
        "hash_examples": "19197e076ad14ff5",
        "hash_full_prompts": "19197e076ad14ff5",
        "hash_input_tokens": "a20c3d0f4526f117",
        "hash_cont_tokens": "9ec87f6f54aa68d0"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Qatar|0": {
      "hashes": {
        "hash_examples": "cf0736fa185b28f6",
        "hash_full_prompts": "cf0736fa185b28f6",
        "hash_input_tokens": "9a7d3cf4d3623235",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Saudi_Arabia|0": {
      "hashes": {
        "hash_examples": "69beda6e1b85a08d",
        "hash_full_prompts": "69beda6e1b85a08d",
        "hash_input_tokens": "9355b6416f9952bb",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Somalia|0": {
      "hashes": {
        "hash_examples": "b387940c65784fbf",
        "hash_full_prompts": "b387940c65784fbf",
        "hash_input_tokens": "f0a64e59e488768f",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Sudan|0": {
      "hashes": {
        "hash_examples": "e02c32b9d2dd0c3f",
        "hash_full_prompts": "e02c32b9d2dd0c3f",
        "hash_input_tokens": "48335bfad8e45fc2",
        "hash_cont_tokens": "c043533857b63932"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Syria|0": {
      "hashes": {
        "hash_examples": "60a6f8fe73bda4bb",
        "hash_full_prompts": "60a6f8fe73bda4bb",
        "hash_input_tokens": "50010c1021143f7e",
        "hash_cont_tokens": "284808465f3e81d2"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Tunisia|0": {
      "hashes": {
        "hash_examples": "34bb15d3830c5649",
        "hash_full_prompts": "34bb15d3830c5649",
        "hash_input_tokens": "bdace5a454d1dea9",
        "hash_cont_tokens": "5584c5b22252fc59"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:United_Arab_Emirates|0": {
      "hashes": {
        "hash_examples": "98a0ba78172718ce",
        "hash_full_prompts": "98a0ba78172718ce",
        "hash_input_tokens": "4d9521c2b82b7444",
        "hash_cont_tokens": "d93b7a07a279509c"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Yemen|0": {
      "hashes": {
        "hash_examples": "18e9bcccbb4ced7a",
        "hash_full_prompts": "18e9bcccbb4ced7a",
        "hash_input_tokens": "f7924acd152fae5f",
        "hash_cont_tokens": "e8275ad19b292e93"
      },
      "truncated": 0,
      "non_truncated": 10,
      "padded": 20,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:communication|0": {
      "hashes": {
        "hash_examples": "9ff28ab5eab5c97b",
        "hash_full_prompts": "9ff28ab5eab5c97b",
        "hash_input_tokens": "f08ecdbfea57f32d",
        "hash_cont_tokens": "3bfb0d933faaef93"
      },
      "truncated": 0,
      "non_truncated": 364,
      "padded": 728,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:computer_and_phone|0": {
      "hashes": {
        "hash_examples": "37bac2f086aaf6c2",
        "hash_full_prompts": "37bac2f086aaf6c2",
        "hash_input_tokens": "734dab2fcda86ae8",
        "hash_cont_tokens": "55e15a27fca6e227"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:daily_life|0": {
      "hashes": {
        "hash_examples": "bf07363c1c252e2f",
        "hash_full_prompts": "bf07363c1c252e2f",
        "hash_input_tokens": "629e60c9ad85b028",
        "hash_cont_tokens": "e8ee8eb5524ae937"
      },
      "truncated": 0,
      "non_truncated": 337,
      "padded": 674,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:entertainment|0": {
      "hashes": {
        "hash_examples": "37077bc00f0ac56a",
        "hash_full_prompts": "37077bc00f0ac56a",
        "hash_input_tokens": "c18d2491ed02e6cf",
        "hash_cont_tokens": "7c1064430ec2a21e"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:mcq_exams_test_ar|0": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "c07a5e78c5c0b8fe",
        "hash_input_tokens": "4ff7a91265e58845",
        "hash_cont_tokens": "2c4daafd63761e4f"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2228,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|0": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "c0b6081f83e14064",
        "hash_input_tokens": "83db4255e5f21415",
        "hash_cont_tokens": "6b11217fb54715bd"
      },
      "truncated": 0,
      "non_truncated": 5395,
      "padded": 21580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_msa|0": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "64eb78a7c5b7484b",
        "hash_input_tokens": "a0b302ed2506b593",
        "hash_cont_tokens": "e9cab3ce1329309b"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "54fc3502c1c02c06",
        "hash_input_tokens": "4e24c0e6ee2e3ac0",
        "hash_cont_tokens": "40e9b345f9904453"
      },
      "truncated": 0,
      "non_truncated": 75,
      "padded": 150,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "46572d83696552ae",
        "hash_input_tokens": "5f43de4c43b003cc",
        "hash_cont_tokens": "5167e0df120d8920"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "f430d97ff715bc1c",
        "hash_input_tokens": "531a21f4ee3e0d73",
        "hash_cont_tokens": "dd6f8d33917ae107"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "6b70a7416584f98c",
        "hash_input_tokens": "17231140b39a5e27",
        "hash_cont_tokens": "785270a931edde1e"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|0": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "bc2005cc9d2f436e",
        "hash_input_tokens": "cb33784392734704",
        "hash_cont_tokens": "ac46dd456a37b4ff"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17985,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|0": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "6fb0e254ea5945d8",
        "hash_input_tokens": "fe40e3b0520e2cd9",
        "hash_cont_tokens": "8f5871b726e030eb"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5160,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_exams|0": {
      "hashes": {
        "hash_examples": "6d721df351722656",
        "hash_full_prompts": "6d721df351722656",
        "hash_input_tokens": "be9304f7533f46a9",
        "hash_cont_tokens": "0e70a551c79d18d1"
      },
      "truncated": 0,
      "non_truncated": 537,
      "padded": 2148,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "f2ddca8f45c0a511",
        "hash_full_prompts": "f2ddca8f45c0a511",
        "hash_input_tokens": "95804cf0891708a7",
        "hash_cont_tokens": "62af5492541b50a5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:anatomy|0": {
      "hashes": {
        "hash_examples": "dfdbc1b83107668d",
        "hash_full_prompts": "dfdbc1b83107668d",
        "hash_input_tokens": "60d3fbc951fc3491",
        "hash_cont_tokens": "b956c1e21cf00b3c"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:astronomy|0": {
      "hashes": {
        "hash_examples": "9736a606002a848e",
        "hash_full_prompts": "9736a606002a848e",
        "hash_input_tokens": "cd96f65850c1ea01",
        "hash_cont_tokens": "9ed532da5cfd8f73"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:business_ethics|0": {
      "hashes": {
        "hash_examples": "735e452fbb6dc63d",
        "hash_full_prompts": "735e452fbb6dc63d",
        "hash_input_tokens": "1f581d56321304d5",
        "hash_cont_tokens": "d012a308646a511f"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "6ab0ca4da98aedcf",
        "hash_full_prompts": "6ab0ca4da98aedcf",
        "hash_input_tokens": "83059bc773a6f3cf",
        "hash_cont_tokens": "6b3e1d1f2a0f787d"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_biology|0": {
      "hashes": {
        "hash_examples": "17e4e390848018a4",
        "hash_full_prompts": "17e4e390848018a4",
        "hash_input_tokens": "6a083e198ac29b73",
        "hash_cont_tokens": "0d272404f948283a"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_chemistry|0": {
      "hashes": {
        "hash_examples": "4abb169f6dfd234b",
        "hash_full_prompts": "4abb169f6dfd234b",
        "hash_input_tokens": "7430add9d4ab67d5",
        "hash_cont_tokens": "5e9d0818dbc8daf0"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_computer_science|0": {
      "hashes": {
        "hash_examples": "a369e2e941358a1e",
        "hash_full_prompts": "a369e2e941358a1e",
        "hash_input_tokens": "1de07f40d5cc4b59",
        "hash_cont_tokens": "2d6d32c913daa5e3"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_mathematics|0": {
      "hashes": {
        "hash_examples": "d7be03b8b6020bff",
        "hash_full_prompts": "d7be03b8b6020bff",
        "hash_input_tokens": "7ebe40d322c70d1b",
        "hash_cont_tokens": "66d8db0238a57a53"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_medicine|0": {
      "hashes": {
        "hash_examples": "0518a00f097346bf",
        "hash_full_prompts": "0518a00f097346bf",
        "hash_input_tokens": "a895290d3f469744",
        "hash_cont_tokens": "d16fdd2676b12373"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_physics|0": {
      "hashes": {
        "hash_examples": "5d842cd49bc70e12",
        "hash_full_prompts": "5d842cd49bc70e12",
        "hash_input_tokens": "45dfdb13257a0de2",
        "hash_cont_tokens": "3cef3b9724b597dc"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:computer_security|0": {
      "hashes": {
        "hash_examples": "8e85d9f85be9b32f",
        "hash_full_prompts": "8e85d9f85be9b32f",
        "hash_input_tokens": "f3a7f1a6a82e4499",
        "hash_cont_tokens": "486e1a47b9e50a84"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "7964b55a0a49502b",
        "hash_full_prompts": "7964b55a0a49502b",
        "hash_input_tokens": "3bc4971848bacfe6",
        "hash_cont_tokens": "a90c531e8f9eb43a"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:econometrics|0": {
      "hashes": {
        "hash_examples": "1e192eae38347257",
        "hash_full_prompts": "1e192eae38347257",
        "hash_input_tokens": "cea13fc5c0bf6a0b",
        "hash_cont_tokens": "2003a78561a84f02"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "cf97671d5c441da1",
        "hash_full_prompts": "cf97671d5c441da1",
        "hash_input_tokens": "caa9dde7bcc0d303",
        "hash_cont_tokens": "70ba48b2318416a5"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "6f49107ed43c40c5",
        "hash_full_prompts": "6f49107ed43c40c5",
        "hash_input_tokens": "bd8766849fc28117",
        "hash_cont_tokens": "ee4fb9f1e9ed4366"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:formal_logic|0": {
      "hashes": {
        "hash_examples": "7922c376008ba77b",
        "hash_full_prompts": "7922c376008ba77b",
        "hash_input_tokens": "d8efcb8e5ec5926d",
        "hash_cont_tokens": "699da6b476c0818f"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:global_facts|0": {
      "hashes": {
        "hash_examples": "11f9813185047d5b",
        "hash_full_prompts": "11f9813185047d5b",
        "hash_input_tokens": "fa0060962694cd43",
        "hash_cont_tokens": "93cc6720cc737a0c"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_biology|0": {
      "hashes": {
        "hash_examples": "2a804b1d90cbe66e",
        "hash_full_prompts": "2a804b1d90cbe66e",
        "hash_input_tokens": "4ddaa80742703e1b",
        "hash_cont_tokens": "4cc72a37ed86021b"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "0032168adabc53b4",
        "hash_full_prompts": "0032168adabc53b4",
        "hash_input_tokens": "61f7d5f26caa9ab5",
        "hash_cont_tokens": "290cd95d3cd7bd58"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "f2fb8740f9df980f",
        "hash_full_prompts": "f2fb8740f9df980f",
        "hash_input_tokens": "d5c72682bacd75a4",
        "hash_cont_tokens": "fd00ab2d3f3279e9"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "73509021e7e66435",
        "hash_full_prompts": "73509021e7e66435",
        "hash_input_tokens": "dceede62481e79ec",
        "hash_cont_tokens": "0aee7c62bd1a89a2"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_geography|0": {
      "hashes": {
        "hash_examples": "9e08d1894940ff42",
        "hash_full_prompts": "9e08d1894940ff42",
        "hash_input_tokens": "85f865e0b8d726ce",
        "hash_cont_tokens": "6b4e1063f5fc1bda"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "64b7e97817ca6c76",
        "hash_full_prompts": "64b7e97817ca6c76",
        "hash_input_tokens": "96d6ffac122f8480",
        "hash_cont_tokens": "45e02f5ff495ccbb"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "9f582da8534bd2ef",
        "hash_full_prompts": "9f582da8534bd2ef",
        "hash_input_tokens": "533386dcda482f63",
        "hash_cont_tokens": "66921489e2f62353"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "fd54f1c10d423c51",
        "hash_full_prompts": "fd54f1c10d423c51",
        "hash_input_tokens": "16f969082ba34384",
        "hash_cont_tokens": "1409a3ccc18bb24c"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "7037896925aaf42f",
        "hash_full_prompts": "7037896925aaf42f",
        "hash_input_tokens": "329f3ab87d5d18fa",
        "hash_cont_tokens": "c49fdb03dcc30f5d"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_physics|0": {
      "hashes": {
        "hash_examples": "60c3776215167dae",
        "hash_full_prompts": "60c3776215167dae",
        "hash_input_tokens": "10b2b1bbd00dfe94",
        "hash_cont_tokens": "536e02d819884158"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "61176bfd5da1298f",
        "hash_full_prompts": "61176bfd5da1298f",
        "hash_input_tokens": "1f79547ab5538fec",
        "hash_cont_tokens": "c0ce8ce90a8dc21d"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "40dfeebd1ea10f76",
        "hash_full_prompts": "40dfeebd1ea10f76",
        "hash_input_tokens": "926cde989ccf4fbd",
        "hash_cont_tokens": "2160464e0bd58c10"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 860,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "03daa510ba917f4d",
        "hash_full_prompts": "03daa510ba917f4d",
        "hash_input_tokens": "6b9fe8746813230a",
        "hash_cont_tokens": "a10172e2c0e4ddce"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "be075ffd579f43c2",
        "hash_full_prompts": "be075ffd579f43c2",
        "hash_input_tokens": "d32237beb3357acf",
        "hash_cont_tokens": "c34c399a394dd181"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_aging|0": {
      "hashes": {
        "hash_examples": "caa5b69f640bd1ef",
        "hash_full_prompts": "caa5b69f640bd1ef",
        "hash_input_tokens": "0cf6f2c0f22e29f3",
        "hash_cont_tokens": "6d2b8fceea53635c"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_sexuality|0": {
      "hashes": {
        "hash_examples": "5ed2e38fb25a3767",
        "hash_full_prompts": "5ed2e38fb25a3767",
        "hash_input_tokens": "4a58a3ad3ccbd581",
        "hash_cont_tokens": "5d6ff346d3f2aa5b"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 524,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:international_law|0": {
      "hashes": {
        "hash_examples": "4e3e9e28d1b96484",
        "hash_full_prompts": "4e3e9e28d1b96484",
        "hash_input_tokens": "3dc99c0677f693fb",
        "hash_cont_tokens": "f111502dbad05da0"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:jurisprudence|0": {
      "hashes": {
        "hash_examples": "e264b755366310b3",
        "hash_full_prompts": "e264b755366310b3",
        "hash_input_tokens": "c242c73439fbe476",
        "hash_cont_tokens": "df204a1ebc98af29"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 432,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "a4ab6965a3e38071",
        "hash_full_prompts": "a4ab6965a3e38071",
        "hash_input_tokens": "77d44733ca287818",
        "hash_cont_tokens": "fe49a9f883a64e50"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:machine_learning|0": {
      "hashes": {
        "hash_examples": "b92320efa6636b40",
        "hash_full_prompts": "b92320efa6636b40",
        "hash_input_tokens": "7e7ce1c737003331",
        "hash_cont_tokens": "80e4c1f5865d9fe8"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:management|0": {
      "hashes": {
        "hash_examples": "c9ee4872a850fe20",
        "hash_full_prompts": "c9ee4872a850fe20",
        "hash_input_tokens": "4b714425d84a85a8",
        "hash_cont_tokens": "d4631f1f07f87968"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:marketing|0": {
      "hashes": {
        "hash_examples": "0c151b70f6a047e3",
        "hash_full_prompts": "0c151b70f6a047e3",
        "hash_input_tokens": "3ef179f9de5709ac",
        "hash_cont_tokens": "0ca189c8111968a8"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 936,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:medical_genetics|0": {
      "hashes": {
        "hash_examples": "513f6cb8fca3a24e",
        "hash_full_prompts": "513f6cb8fca3a24e",
        "hash_input_tokens": "4d31ea42b1de8759",
        "hash_cont_tokens": "62ce32b38701d2d5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:miscellaneous|0": {
      "hashes": {
        "hash_examples": "259a190d635331db",
        "hash_full_prompts": "259a190d635331db",
        "hash_input_tokens": "6e62132ae060cb50",
        "hash_cont_tokens": "dbc38c613d2175ea"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3132,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_disputes|0": {
      "hashes": {
        "hash_examples": "b85052c48a0b7bc3",
        "hash_full_prompts": "b85052c48a0b7bc3",
        "hash_input_tokens": "1827a6716172a761",
        "hash_cont_tokens": "c6116a0e71ce0cd3"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "28d0b069ef00dd00",
        "hash_full_prompts": "28d0b069ef00dd00",
        "hash_input_tokens": "ed7aead5582292d2",
        "hash_cont_tokens": "56f5d0283f760076"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:nutrition|0": {
      "hashes": {
        "hash_examples": "00c9bc5f1d305b2f",
        "hash_full_prompts": "00c9bc5f1d305b2f",
        "hash_input_tokens": "37a4ea004981c874",
        "hash_cont_tokens": "843f7b8653219595"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1224,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:philosophy|0": {
      "hashes": {
        "hash_examples": "a458c08454a3fd5f",
        "hash_full_prompts": "a458c08454a3fd5f",
        "hash_input_tokens": "19550fb5ec023a7c",
        "hash_cont_tokens": "8079da51f8a6dd30"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1244,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:prehistory|0": {
      "hashes": {
        "hash_examples": "d6a0ecbdbb670e9c",
        "hash_full_prompts": "d6a0ecbdbb670e9c",
        "hash_input_tokens": "aea98786f14e74c6",
        "hash_cont_tokens": "e553bebb99d58554"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1296,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_accounting|0": {
      "hashes": {
        "hash_examples": "b4a95fe480b6540e",
        "hash_full_prompts": "b4a95fe480b6540e",
        "hash_input_tokens": "5f0d4ed6b592ab6b",
        "hash_cont_tokens": "763def7d3c29faa9"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1128,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_law|0": {
      "hashes": {
        "hash_examples": "c2be9651cdbdde3b",
        "hash_full_prompts": "c2be9651cdbdde3b",
        "hash_input_tokens": "c1368e0714d50499",
        "hash_cont_tokens": "f3f6484dd7ce5cf1"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6132,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_medicine|0": {
      "hashes": {
        "hash_examples": "26ce92416288f273",
        "hash_full_prompts": "26ce92416288f273",
        "hash_input_tokens": "968ca7d19099fc41",
        "hash_cont_tokens": "b0854d5f67a737bd"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_psychology|0": {
      "hashes": {
        "hash_examples": "71ea5f182ea9a641",
        "hash_full_prompts": "71ea5f182ea9a641",
        "hash_input_tokens": "7d1199afb1873982",
        "hash_cont_tokens": "54362a900a34d41a"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2448,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:public_relations|0": {
      "hashes": {
        "hash_examples": "125adc21f91f8d77",
        "hash_full_prompts": "125adc21f91f8d77",
        "hash_input_tokens": "b7fda0b946c41690",
        "hash_cont_tokens": "1599a62244940b3e"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 440,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:security_studies|0": {
      "hashes": {
        "hash_examples": "3c18b216c099fb26",
        "hash_full_prompts": "3c18b216c099fb26",
        "hash_input_tokens": "86631b774857b129",
        "hash_cont_tokens": "ecd7f1c570f550dc"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:sociology|0": {
      "hashes": {
        "hash_examples": "3f2a9634cef7417d",
        "hash_full_prompts": "3f2a9634cef7417d",
        "hash_input_tokens": "4cb0f938d71fb02f",
        "hash_cont_tokens": "0af49f489c948e57"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 804,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "22249da54056475e",
        "hash_full_prompts": "22249da54056475e",
        "hash_input_tokens": "5a29dad80049a2c7",
        "hash_cont_tokens": "86e9e0244f538a5a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:virology|0": {
      "hashes": {
        "hash_examples": "9d194b9471dc624e",
        "hash_full_prompts": "9d194b9471dc624e",
        "hash_input_tokens": "2e702ddccd6fc93c",
        "hash_cont_tokens": "c34e8615a5495f77"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 664,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:world_religions|0": {
      "hashes": {
        "hash_examples": "229e5fe50082b064",
        "hash_full_prompts": "229e5fe50082b064",
        "hash_input_tokens": "0b5e80a6b8b1b13d",
        "hash_cont_tokens": "dfd02cecc49dac4a"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_challenge_okapi_ar|0": {
      "hashes": {
        "hash_examples": "ab893807673bc355",
        "hash_full_prompts": "ab893807673bc355",
        "hash_input_tokens": "388581393ede4136",
        "hash_cont_tokens": "da3a53f65d893596"
      },
      "truncated": 0,
      "non_truncated": 1160,
      "padded": 4640,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_easy_ar|0": {
      "hashes": {
        "hash_examples": "acb688624acc3d04",
        "hash_full_prompts": "acb688624acc3d04",
        "hash_input_tokens": "1c833e26a6460919",
        "hash_cont_tokens": "feadbb907de8c148"
      },
      "truncated": 0,
      "non_truncated": 2364,
      "padded": 9455,
      "non_padded": 1,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|boolq_ar|0": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "48355a67867e0c32",
        "hash_input_tokens": "c5bea2ee077cbdcf",
        "hash_cont_tokens": "f3b1275a6983c49f"
      },
      "truncated": 2,
      "non_truncated": 3258,
      "padded": 6512,
      "non_padded": 8,
      "effective_few_shots": -0.00030674846625766873,
      "num_truncated_few_shots": 1
    },
    "community|copa_ext_ar|0": {
      "hashes": {
        "hash_examples": "9bb83301bb72eecf",
        "hash_full_prompts": "9bb83301bb72eecf",
        "hash_input_tokens": "882a2c6e08636bb8",
        "hash_cont_tokens": "60b647ef09cfeb77"
      },
      "truncated": 0,
      "non_truncated": 90,
      "padded": 180,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|0": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "6e8cf57a322dfadd",
        "hash_input_tokens": "c393ced1cec9cff9",
        "hash_cont_tokens": "2220087c0f24bf48"
      },
      "truncated": 0,
      "non_truncated": 9171,
      "padded": 36674,
      "non_padded": 10,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|mmlu_okapi_ar|0": {
      "hashes": {
        "hash_examples": "723b5371ad6fdc31",
        "hash_full_prompts": "723b5371ad6fdc31",
        "hash_input_tokens": "789fa1e60619bd92",
        "hash_cont_tokens": "e4c587e401d0e8b6"
      },
      "truncated": 24,
      "non_truncated": 12899,
      "padded": 51645,
      "non_padded": 47,
      "effective_few_shots": -0.0003869070649230055,
      "num_truncated_few_shots": 5
    },
    "community|openbook_qa_ext_ar|0": {
      "hashes": {
        "hash_examples": "923d41eb0aca93eb",
        "hash_full_prompts": "923d41eb0aca93eb",
        "hash_input_tokens": "c2f2767f1ab5a959",
        "hash_cont_tokens": "a5f533eaff3cda1d"
      },
      "truncated": 0,
      "non_truncated": 495,
      "padded": 1980,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|piqa_ar|0": {
      "hashes": {
        "hash_examples": "94bc205a520d3ea0",
        "hash_full_prompts": "94bc205a520d3ea0",
        "hash_input_tokens": "c80883b1f15da582",
        "hash_cont_tokens": "4390d98320ad111e"
      },
      "truncated": 0,
      "non_truncated": 1833,
      "padded": 3660,
      "non_padded": 6,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|race_ar|0": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "de65130bae647516",
        "hash_input_tokens": "7e63a8c88ddc9de2",
        "hash_cont_tokens": "661c26d197b76b7c"
      },
      "truncated": 360,
      "non_truncated": 4569,
      "padded": 19348,
      "non_padded": 368,
      "effective_few_shots": -0.018259281801582473,
      "num_truncated_few_shots": 90
    },
    "community|sciq_ar|0": {
      "hashes": {
        "hash_examples": "562ccea2e30e5e04",
        "hash_full_prompts": "562ccea2e30e5e04",
        "hash_input_tokens": "77bc870ffefbf389",
        "hash_cont_tokens": "865b058a48134834"
      },
      "truncated": 0,
      "non_truncated": 995,
      "padded": 3965,
      "non_padded": 15,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|toxigen_ar|0": {
      "hashes": {
        "hash_examples": "1e139513004a9a2e",
        "hash_full_prompts": "1e139513004a9a2e",
        "hash_input_tokens": "47803bc4617b63f5",
        "hash_cont_tokens": "20a3ed842c60ed22"
      },
      "truncated": 0,
      "non_truncated": 935,
      "padded": 1862,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "lighteval|xstory_cloze:ar|0": {
      "hashes": {
        "hash_examples": "865426a22c787481",
        "hash_full_prompts": "865426a22c787481",
        "hash_input_tokens": "5699fad05a2ef439",
        "hash_cont_tokens": "2b67247b9172c3d0"
      },
      "truncated": 0,
      "non_truncated": 1511,
      "padded": 2990,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "a5f1a4baabdaa7ff",
      "hash_full_prompts": "a5f1a4baabdaa7ff",
      "hash_input_tokens": "a0c3aa2be1546d29",
      "hash_cont_tokens": "b1c842186a104148"
    },
    "truncated": 386,
    "non_truncated": 85501,
    "padded": 286812,
    "non_padded": 503,
    "num_truncated_few_shots": 96
  }
}