{
  "config_general": {
    "lighteval_sha": "4d8c620733802aba964d87aafb12739c8f72bc7d",
    "num_fewshot_seeds": 1,
    "override_batch_size": 4,
    "max_samples": null,
    "job_id": "",
    "start_time": 259146.622166826,
    "end_time": 285238.89227889,
    "total_evaluation_time_secondes": "26092.270112063998",
    "model_name": "meta-llama/Meta-Llama-3-8B-Instruct",
    "model_sha": "e1945c40cd546c78e41f1151f4db032b271faeaa",
    "model_dtype": "torch.bfloat16",
    "model_size": "14.96 GB",
    "config": null
  },
  "results": {
    "community|acva:Algeria|5": {
      "acc_norm": 0.8512820512820513,
      "acc_norm_stderr": 0.02554568582285815
    },
    "community|acva:Ancient_Egypt|5": {
      "acc_norm": 0.9523809523809523,
      "acc_norm_stderr": 0.012017963455358762
    },
    "community|acva:Arab_Empire|5": {
      "acc_norm": 0.660377358490566,
      "acc_norm_stderr": 0.02914690474779832
    },
    "community|acva:Arabic_Architecture|5": {
      "acc_norm": 0.7948717948717948,
      "acc_norm_stderr": 0.028990811252138905
    },
    "community|acva:Arabic_Art|5": {
      "acc_norm": 0.7076923076923077,
      "acc_norm_stderr": 0.032654383937495104
    },
    "community|acva:Arabic_Astronomy|5": {
      "acc_norm": 0.48717948717948717,
      "acc_norm_stderr": 0.03588610523192216
    },
    "community|acva:Arabic_Calligraphy|5": {
      "acc_norm": 0.792156862745098,
      "acc_norm_stderr": 0.025459893390593798
    },
    "community|acva:Arabic_Ceremony|5": {
      "acc_norm": 0.7297297297297297,
      "acc_norm_stderr": 0.032739439990023544
    },
    "community|acva:Arabic_Clothing|5": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.035498710803677086
    },
    "community|acva:Arabic_Culture|5": {
      "acc_norm": 0.8923076923076924,
      "acc_norm_stderr": 0.02225608622379727
    },
    "community|acva:Arabic_Food|5": {
      "acc_norm": 0.6717948717948717,
      "acc_norm_stderr": 0.03371243782413706
    },
    "community|acva:Arabic_Funeral|5": {
      "acc_norm": 0.6842105263157895,
      "acc_norm_stderr": 0.04794350420740798
    },
    "community|acva:Arabic_Geography|5": {
      "acc_norm": 0.7862068965517242,
      "acc_norm_stderr": 0.034165204477475494
    },
    "community|acva:Arabic_History|5": {
      "acc_norm": 0.7333333333333333,
      "acc_norm_stderr": 0.03174930436412671
    },
    "community|acva:Arabic_Language_Origin|5": {
      "acc_norm": 0.7473684210526316,
      "acc_norm_stderr": 0.04481746243373483
    },
    "community|acva:Arabic_Literature|5": {
      "acc_norm": 0.8620689655172413,
      "acc_norm_stderr": 0.028735632183908087
    },
    "community|acva:Arabic_Math|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.028718326344709492
    },
    "community|acva:Arabic_Medicine|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.03333333333333329
    },
    "community|acva:Arabic_Music|5": {
      "acc_norm": 0.8201438848920863,
      "acc_norm_stderr": 0.03269400212756783
    },
    "community|acva:Arabic_Ornament|5": {
      "acc_norm": 0.9025641025641026,
      "acc_norm_stderr": 0.021291078346298967
    },
    "community|acva:Arabic_Philosophy|5": {
      "acc_norm": 0.7241379310344828,
      "acc_norm_stderr": 0.037245636197746325
    },
    "community|acva:Arabic_Physics_and_Chemistry|5": {
      "acc_norm": 0.8358974358974359,
      "acc_norm_stderr": 0.026590917036710786
    },
    "community|acva:Arabic_Wedding|5": {
      "acc_norm": 0.841025641025641,
      "acc_norm_stderr": 0.026252296135516585
    },
    "community|acva:Bahrain|5": {
      "acc_norm": 0.8666666666666667,
      "acc_norm_stderr": 0.05124707431905384
    },
    "community|acva:Comoros|5": {
      "acc_norm": 0.7555555555555555,
      "acc_norm_stderr": 0.06478835438716998
    },
    "community|acva:Egypt_modern|5": {
      "acc_norm": 0.9157894736842105,
      "acc_norm_stderr": 0.028642906658451376
    },
    "community|acva:InfluenceFromAncientEgypt|5": {
      "acc_norm": 0.8307692307692308,
      "acc_norm_stderr": 0.02692024455476408
    },
    "community|acva:InfluenceFromByzantium|5": {
      "acc_norm": 0.7034482758620689,
      "acc_norm_stderr": 0.03806142687309992
    },
    "community|acva:InfluenceFromChina|5": {
      "acc_norm": 0.8051282051282052,
      "acc_norm_stderr": 0.028438464807264478
    },
    "community|acva:InfluenceFromGreece|5": {
      "acc_norm": 0.8461538461538461,
      "acc_norm_stderr": 0.025904017400572674
    },
    "community|acva:InfluenceFromIslam|5": {
      "acc_norm": 0.9241379310344827,
      "acc_norm_stderr": 0.022064774506267173
    },
    "community|acva:InfluenceFromPersia|5": {
      "acc_norm": 0.96,
      "acc_norm_stderr": 0.014855627054164128
    },
    "community|acva:InfluenceFromRome|5": {
      "acc_norm": 0.676923076923077,
      "acc_norm_stderr": 0.03357544396403131
    },
    "community|acva:Iraq|5": {
      "acc_norm": 0.8470588235294118,
      "acc_norm_stderr": 0.039271668724530855
    },
    "community|acva:Islam_Education|5": {
      "acc_norm": 0.8461538461538461,
      "acc_norm_stderr": 0.02590401740057269
    },
    "community|acva:Islam_branches_and_schools|5": {
      "acc_norm": 0.7942857142857143,
      "acc_norm_stderr": 0.030644036974771957
    },
    "community|acva:Islamic_law_system|5": {
      "acc_norm": 0.8153846153846154,
      "acc_norm_stderr": 0.027855716655754165
    },
    "community|acva:Jordan|5": {
      "acc_norm": 0.7555555555555555,
      "acc_norm_stderr": 0.06478835438716998
    },
    "community|acva:Kuwait|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.06030226891555274
    },
    "community|acva:Lebanon|5": {
      "acc_norm": 0.8444444444444444,
      "acc_norm_stderr": 0.05463890236888295
    },
    "community|acva:Libya|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.06030226891555273
    },
    "community|acva:Mauritania|5": {
      "acc_norm": 0.8444444444444444,
      "acc_norm_stderr": 0.05463890236888295
    },
    "community|acva:Mesopotamia_civilization|5": {
      "acc_norm": 0.8064516129032258,
      "acc_norm_stderr": 0.031836372336763036
    },
    "community|acva:Morocco|5": {
      "acc_norm": 0.8222222222222222,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Oman|5": {
      "acc_norm": 0.9333333333333333,
      "acc_norm_stderr": 0.037605071654517735
    },
    "community|acva:Palestine|5": {
      "acc_norm": 0.8823529411764706,
      "acc_norm_stderr": 0.03515378262748218
    },
    "community|acva:Qatar|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.06030226891555273
    },
    "community|acva:Saudi_Arabia|5": {
      "acc_norm": 0.6974358974358974,
      "acc_norm_stderr": 0.03298070870085619
    },
    "community|acva:Somalia|5": {
      "acc_norm": 0.7777777777777778,
      "acc_norm_stderr": 0.06267511942419626
    },
    "community|acva:Sudan|5": {
      "acc_norm": 0.8444444444444444,
      "acc_norm_stderr": 0.05463890236888295
    },
    "community|acva:Syria|5": {
      "acc_norm": 0.8666666666666667,
      "acc_norm_stderr": 0.05124707431905384
    },
    "community|acva:Tunisia|5": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.06030226891555274
    },
    "community|acva:United_Arab_Emirates|5": {
      "acc_norm": 0.8117647058823529,
      "acc_norm_stderr": 0.04265068080823367
    },
    "community|acva:Yemen|5": {
      "acc_norm": 0.7,
      "acc_norm_stderr": 0.15275252316519466
    },
    "community|acva:communication|5": {
      "acc_norm": 0.7307692307692307,
      "acc_norm_stderr": 0.023280867547130758
    },
    "community|acva:computer_and_phone|5": {
      "acc_norm": 0.6779661016949152,
      "acc_norm_stderr": 0.027250916894486114
    },
    "community|acva:daily_life|5": {
      "acc_norm": 0.8130563798219584,
      "acc_norm_stderr": 0.021268948348414647
    },
    "community|acva:entertainment|5": {
      "acc_norm": 0.847457627118644,
      "acc_norm_stderr": 0.02096913810616074
    },
    "community|alghafa:mcq_exams_test_ar|5": {
      "acc_norm": 0.41651705565529623,
      "acc_norm_stderr": 0.020907065602152285
    },
    "community|alghafa:meta_ar_dialects|5": {
      "acc_norm": 0.567191844300278,
      "acc_norm_stderr": 0.006746169295188675
    },
    "community|alghafa:meta_ar_msa|5": {
      "acc_norm": 0.7318435754189944,
      "acc_norm_stderr": 0.014816119635317006
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": {
      "acc_norm": 0.92,
      "acc_norm_stderr": 0.03153719382878881
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": {
      "acc_norm": 0.9066666666666666,
      "acc_norm_stderr": 0.023831373802832918
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": {
      "acc_norm": 0.8733333333333333,
      "acc_norm_stderr": 0.027247587445967645
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": {
      "acc_norm": 0.8191369606003752,
      "acc_norm_stderr": 0.004304979638117819
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|5": {
      "acc_norm": 0.6251876563803169,
      "acc_norm_stderr": 0.006252501010306697
    },
    "community|alghafa:multiple_choice_sentiment_task|5": {
      "acc_norm": 0.4180232558139535,
      "acc_norm_stderr": 0.011896387409582185
    },
    "community|arabic_exams|5": {
      "acc_norm": 0.4301675977653631,
      "acc_norm_stderr": 0.021385037819322314
    },
    "community|arabic_mmlu:abstract_algebra|5": {
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "community|arabic_mmlu:anatomy|5": {
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.039992628766177214
    },
    "community|arabic_mmlu:astronomy|5": {
      "acc_norm": 0.4276315789473684,
      "acc_norm_stderr": 0.04026097083296559
    },
    "community|arabic_mmlu:business_ethics|5": {
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956912
    },
    "community|arabic_mmlu:clinical_knowledge|5": {
      "acc_norm": 0.539622641509434,
      "acc_norm_stderr": 0.030676096599389174
    },
    "community|arabic_mmlu:college_biology|5": {
      "acc_norm": 0.4305555555555556,
      "acc_norm_stderr": 0.04140685639111503
    },
    "community|arabic_mmlu:college_chemistry|5": {
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "community|arabic_mmlu:college_computer_science|5": {
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "community|arabic_mmlu:college_mathematics|5": {
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "community|arabic_mmlu:college_medicine|5": {
      "acc_norm": 0.3815028901734104,
      "acc_norm_stderr": 0.03703851193099521
    },
    "community|arabic_mmlu:college_physics|5": {
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364395
    },
    "community|arabic_mmlu:computer_security|5": {
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "community|arabic_mmlu:conceptual_physics|5": {
      "acc_norm": 0.4340425531914894,
      "acc_norm_stderr": 0.03240038086792747
    },
    "community|arabic_mmlu:econometrics|5": {
      "acc_norm": 0.2894736842105263,
      "acc_norm_stderr": 0.04266339443159394
    },
    "community|arabic_mmlu:electrical_engineering|5": {
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878151
    },
    "community|arabic_mmlu:elementary_mathematics|5": {
      "acc_norm": 0.41005291005291006,
      "acc_norm_stderr": 0.02533120243894443
    },
    "community|arabic_mmlu:formal_logic|5": {
      "acc_norm": 0.4603174603174603,
      "acc_norm_stderr": 0.04458029125470973
    },
    "community|arabic_mmlu:global_facts|5": {
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "community|arabic_mmlu:high_school_biology|5": {
      "acc_norm": 0.49032258064516127,
      "acc_norm_stderr": 0.028438677998909558
    },
    "community|arabic_mmlu:high_school_chemistry|5": {
      "acc_norm": 0.3251231527093596,
      "acc_norm_stderr": 0.032957975663112704
    },
    "community|arabic_mmlu:high_school_computer_science|5": {
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "community|arabic_mmlu:high_school_european_history|5": {
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.033175059300091805
    },
    "community|arabic_mmlu:high_school_geography|5": {
      "acc_norm": 0.5858585858585859,
      "acc_norm_stderr": 0.03509438348879629
    },
    "community|arabic_mmlu:high_school_government_and_politics|5": {
      "acc_norm": 0.48704663212435234,
      "acc_norm_stderr": 0.0360722806104775
    },
    "community|arabic_mmlu:high_school_macroeconomics|5": {
      "acc_norm": 0.49230769230769234,
      "acc_norm_stderr": 0.025348006031534785
    },
    "community|arabic_mmlu:high_school_mathematics|5": {
      "acc_norm": 0.34074074074074073,
      "acc_norm_stderr": 0.028897748741131143
    },
    "community|arabic_mmlu:high_school_microeconomics|5": {
      "acc_norm": 0.46218487394957986,
      "acc_norm_stderr": 0.032385469487589795
    },
    "community|arabic_mmlu:high_school_physics|5": {
      "acc_norm": 0.32450331125827814,
      "acc_norm_stderr": 0.038227469376587525
    },
    "community|arabic_mmlu:high_school_psychology|5": {
      "acc_norm": 0.47706422018348627,
      "acc_norm_stderr": 0.0214147570581755
    },
    "community|arabic_mmlu:high_school_statistics|5": {
      "acc_norm": 0.3101851851851852,
      "acc_norm_stderr": 0.031546962856566295
    },
    "community|arabic_mmlu:high_school_us_history|5": {
      "acc_norm": 0.23039215686274508,
      "acc_norm_stderr": 0.02955429260569507
    },
    "community|arabic_mmlu:high_school_world_history|5": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03068582059661078
    },
    "community|arabic_mmlu:human_aging|5": {
      "acc_norm": 0.4618834080717489,
      "acc_norm_stderr": 0.03346015011973228
    },
    "community|arabic_mmlu:human_sexuality|5": {
      "acc_norm": 0.4961832061068702,
      "acc_norm_stderr": 0.043851623256015534
    },
    "community|arabic_mmlu:international_law|5": {
      "acc_norm": 0.6859504132231405,
      "acc_norm_stderr": 0.04236964753041018
    },
    "community|arabic_mmlu:jurisprudence|5": {
      "acc_norm": 0.5833333333333334,
      "acc_norm_stderr": 0.04766075165356461
    },
    "community|arabic_mmlu:logical_fallacies|5": {
      "acc_norm": 0.50920245398773,
      "acc_norm_stderr": 0.039277056007874414
    },
    "community|arabic_mmlu:machine_learning|5": {
      "acc_norm": 0.38392857142857145,
      "acc_norm_stderr": 0.04616143075028547
    },
    "community|arabic_mmlu:management|5": {
      "acc_norm": 0.5922330097087378,
      "acc_norm_stderr": 0.048657775704107696
    },
    "community|arabic_mmlu:marketing|5": {
      "acc_norm": 0.7350427350427351,
      "acc_norm_stderr": 0.02891120880274945
    },
    "community|arabic_mmlu:medical_genetics|5": {
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.050211673156867795
    },
    "community|arabic_mmlu:miscellaneous|5": {
      "acc_norm": 0.5210727969348659,
      "acc_norm_stderr": 0.0178640767862129
    },
    "community|arabic_mmlu:moral_disputes|5": {
      "acc_norm": 0.48265895953757226,
      "acc_norm_stderr": 0.026902900458666647
    },
    "community|arabic_mmlu:moral_scenarios|5": {
      "acc_norm": 0.288268156424581,
      "acc_norm_stderr": 0.01514913286020944
    },
    "community|arabic_mmlu:nutrition|5": {
      "acc_norm": 0.5392156862745098,
      "acc_norm_stderr": 0.028541722692618874
    },
    "community|arabic_mmlu:philosophy|5": {
      "acc_norm": 0.4887459807073955,
      "acc_norm_stderr": 0.028390897396863533
    },
    "community|arabic_mmlu:prehistory|5": {
      "acc_norm": 0.4506172839506173,
      "acc_norm_stderr": 0.027684721415656203
    },
    "community|arabic_mmlu:professional_accounting|5": {
      "acc_norm": 0.3191489361702128,
      "acc_norm_stderr": 0.027807990141320196
    },
    "community|arabic_mmlu:professional_law|5": {
      "acc_norm": 0.3135593220338983,
      "acc_norm_stderr": 0.011849234291459315
    },
    "community|arabic_mmlu:professional_medicine|5": {
      "acc_norm": 0.41911764705882354,
      "acc_norm_stderr": 0.02997280717046462
    },
    "community|arabic_mmlu:professional_psychology|5": {
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.019722058939618065
    },
    "community|arabic_mmlu:public_relations|5": {
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.04769300568972745
    },
    "community|arabic_mmlu:security_studies|5": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.03136250240935893
    },
    "community|arabic_mmlu:sociology|5": {
      "acc_norm": 0.5970149253731343,
      "acc_norm_stderr": 0.03468343295111126
    },
    "community|arabic_mmlu:us_foreign_policy|5": {
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.04688261722621503
    },
    "community|arabic_mmlu:virology|5": {
      "acc_norm": 0.43373493975903615,
      "acc_norm_stderr": 0.03858158940685516
    },
    "community|arabic_mmlu:world_religions|5": {
      "acc_norm": 0.4619883040935672,
      "acc_norm_stderr": 0.03823727092882307
    },
    "community|arc_challenge_okapi_ar|5": {
      "acc_norm": 0.5801724137931035,
      "acc_norm_stderr": 0.014496805279151325
    },
    "community|arc_easy_ar|5": {
      "acc_norm": 0.6920473773265652,
      "acc_norm_stderr": 0.009496815620208756
    },
    "community|boolq_ar|5": {
      "acc_norm": 0.6797546012269938,
      "acc_norm_stderr": 0.008172884228832208
    },
    "community|copa_ext_ar|5": {
      "acc_norm": 0.7666666666666667,
      "acc_norm_stderr": 0.044832884310575985
    },
    "community|hellaswag_okapi_ar|5": {
      "acc_norm": 0.3041107839930215,
      "acc_norm_stderr": 0.004803981162654304
    },
    "community|mmlu_okapi_ar|5": {
      "acc_norm": 0.4088833862106322,
      "acc_norm_stderr": 0.004324853958485614
    },
    "community|openbook_qa_ext_ar|5": {
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.022356767682931537
    },
    "community|piqa_ar|5": {
      "acc_norm": 0.6399345335515548,
      "acc_norm_stderr": 0.011214909522120894
    },
    "community|race_ar|5": {
      "acc_norm": 0.6193954148914587,
      "acc_norm_stderr": 0.006916488376747647
    },
    "community|sciq_ar|5": {
      "acc_norm": 0.7326633165829146,
      "acc_norm_stderr": 0.014037452817597452
    },
    "community|toxigen_ar|5": {
      "acc_norm": 0.5229946524064171,
      "acc_norm_stderr": 0.01634318925655896
    },
    "lighteval|xstory_cloze:ar|0": {
      "acc": 0.6101919258769027,
      "acc_stderr": 0.012550764190647004
    },
    "community|acva:_average|5": {
      "acc_norm": 0.794735997721388,
      "acc_norm_stderr": 0.038497275572199584
    },
    "community|alghafa:_average|5": {
      "acc_norm": 0.6975444831299128,
      "acc_norm_stderr": 0.01639326418536156
    },
    "community|arabic_mmlu:_average|5": {
      "acc_norm": 0.4415851559901855,
      "acc_norm_stderr": 0.03612965860656394
    },
    "all": {
      "acc_norm": 0.6211418265251508,
      "acc_norm_stderr": 0.033957014496067355,
      "acc": 0.6101919258769027,
      "acc_stderr": 0.012550764190647004
    }
  },
  "versions": {
    "community|acva:Algeria|5": 0,
    "community|acva:Ancient_Egypt|5": 0,
    "community|acva:Arab_Empire|5": 0,
    "community|acva:Arabic_Architecture|5": 0,
    "community|acva:Arabic_Art|5": 0,
    "community|acva:Arabic_Astronomy|5": 0,
    "community|acva:Arabic_Calligraphy|5": 0,
    "community|acva:Arabic_Ceremony|5": 0,
    "community|acva:Arabic_Clothing|5": 0,
    "community|acva:Arabic_Culture|5": 0,
    "community|acva:Arabic_Food|5": 0,
    "community|acva:Arabic_Funeral|5": 0,
    "community|acva:Arabic_Geography|5": 0,
    "community|acva:Arabic_History|5": 0,
    "community|acva:Arabic_Language_Origin|5": 0,
    "community|acva:Arabic_Literature|5": 0,
    "community|acva:Arabic_Math|5": 0,
    "community|acva:Arabic_Medicine|5": 0,
    "community|acva:Arabic_Music|5": 0,
    "community|acva:Arabic_Ornament|5": 0,
    "community|acva:Arabic_Philosophy|5": 0,
    "community|acva:Arabic_Physics_and_Chemistry|5": 0,
    "community|acva:Arabic_Wedding|5": 0,
    "community|acva:Bahrain|5": 0,
    "community|acva:Comoros|5": 0,
    "community|acva:Egypt_modern|5": 0,
    "community|acva:InfluenceFromAncientEgypt|5": 0,
    "community|acva:InfluenceFromByzantium|5": 0,
    "community|acva:InfluenceFromChina|5": 0,
    "community|acva:InfluenceFromGreece|5": 0,
    "community|acva:InfluenceFromIslam|5": 0,
    "community|acva:InfluenceFromPersia|5": 0,
    "community|acva:InfluenceFromRome|5": 0,
    "community|acva:Iraq|5": 0,
    "community|acva:Islam_Education|5": 0,
    "community|acva:Islam_branches_and_schools|5": 0,
    "community|acva:Islamic_law_system|5": 0,
    "community|acva:Jordan|5": 0,
    "community|acva:Kuwait|5": 0,
    "community|acva:Lebanon|5": 0,
    "community|acva:Libya|5": 0,
    "community|acva:Mauritania|5": 0,
    "community|acva:Mesopotamia_civilization|5": 0,
    "community|acva:Morocco|5": 0,
    "community|acva:Oman|5": 0,
    "community|acva:Palestine|5": 0,
    "community|acva:Qatar|5": 0,
    "community|acva:Saudi_Arabia|5": 0,
    "community|acva:Somalia|5": 0,
    "community|acva:Sudan|5": 0,
    "community|acva:Syria|5": 0,
    "community|acva:Tunisia|5": 0,
    "community|acva:United_Arab_Emirates|5": 0,
    "community|acva:Yemen|5": 0,
    "community|acva:communication|5": 0,
    "community|acva:computer_and_phone|5": 0,
    "community|acva:daily_life|5": 0,
    "community|acva:entertainment|5": 0,
    "community|alghafa:mcq_exams_test_ar|5": 0,
    "community|alghafa:meta_ar_dialects|5": 0,
    "community|alghafa:meta_ar_msa|5": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|5": 0,
    "community|alghafa:multiple_choice_sentiment_task|5": 0,
    "community|arabic_exams|5": 0,
    "community|arabic_mmlu:abstract_algebra|5": 0,
    "community|arabic_mmlu:anatomy|5": 0,
    "community|arabic_mmlu:astronomy|5": 0,
    "community|arabic_mmlu:business_ethics|5": 0,
    "community|arabic_mmlu:clinical_knowledge|5": 0,
    "community|arabic_mmlu:college_biology|5": 0,
    "community|arabic_mmlu:college_chemistry|5": 0,
    "community|arabic_mmlu:college_computer_science|5": 0,
    "community|arabic_mmlu:college_mathematics|5": 0,
    "community|arabic_mmlu:college_medicine|5": 0,
    "community|arabic_mmlu:college_physics|5": 0,
    "community|arabic_mmlu:computer_security|5": 0,
    "community|arabic_mmlu:conceptual_physics|5": 0,
    "community|arabic_mmlu:econometrics|5": 0,
    "community|arabic_mmlu:electrical_engineering|5": 0,
    "community|arabic_mmlu:elementary_mathematics|5": 0,
    "community|arabic_mmlu:formal_logic|5": 0,
    "community|arabic_mmlu:global_facts|5": 0,
    "community|arabic_mmlu:high_school_biology|5": 0,
    "community|arabic_mmlu:high_school_chemistry|5": 0,
    "community|arabic_mmlu:high_school_computer_science|5": 0,
    "community|arabic_mmlu:high_school_european_history|5": 0,
    "community|arabic_mmlu:high_school_geography|5": 0,
    "community|arabic_mmlu:high_school_government_and_politics|5": 0,
    "community|arabic_mmlu:high_school_macroeconomics|5": 0,
    "community|arabic_mmlu:high_school_mathematics|5": 0,
    "community|arabic_mmlu:high_school_microeconomics|5": 0,
    "community|arabic_mmlu:high_school_physics|5": 0,
    "community|arabic_mmlu:high_school_psychology|5": 0,
    "community|arabic_mmlu:high_school_statistics|5": 0,
    "community|arabic_mmlu:high_school_us_history|5": 0,
    "community|arabic_mmlu:high_school_world_history|5": 0,
    "community|arabic_mmlu:human_aging|5": 0,
    "community|arabic_mmlu:human_sexuality|5": 0,
    "community|arabic_mmlu:international_law|5": 0,
    "community|arabic_mmlu:jurisprudence|5": 0,
    "community|arabic_mmlu:logical_fallacies|5": 0,
    "community|arabic_mmlu:machine_learning|5": 0,
    "community|arabic_mmlu:management|5": 0,
    "community|arabic_mmlu:marketing|5": 0,
    "community|arabic_mmlu:medical_genetics|5": 0,
    "community|arabic_mmlu:miscellaneous|5": 0,
    "community|arabic_mmlu:moral_disputes|5": 0,
    "community|arabic_mmlu:moral_scenarios|5": 0,
    "community|arabic_mmlu:nutrition|5": 0,
    "community|arabic_mmlu:philosophy|5": 0,
    "community|arabic_mmlu:prehistory|5": 0,
    "community|arabic_mmlu:professional_accounting|5": 0,
    "community|arabic_mmlu:professional_law|5": 0,
    "community|arabic_mmlu:professional_medicine|5": 0,
    "community|arabic_mmlu:professional_psychology|5": 0,
    "community|arabic_mmlu:public_relations|5": 0,
    "community|arabic_mmlu:security_studies|5": 0,
    "community|arabic_mmlu:sociology|5": 0,
    "community|arabic_mmlu:us_foreign_policy|5": 0,
    "community|arabic_mmlu:virology|5": 0,
    "community|arabic_mmlu:world_religions|5": 0,
    "community|arc_challenge_okapi_ar|5": 0,
    "community|arc_easy_ar|5": 0,
    "community|boolq_ar|5": 0,
    "community|copa_ext_ar|5": 0,
    "community|hellaswag_okapi_ar|5": 0,
    "community|mmlu_okapi_ar|5": 0,
    "community|openbook_qa_ext_ar|5": 0,
    "community|piqa_ar|5": 0,
    "community|race_ar|5": 0,
    "community|sciq_ar|5": 0,
    "community|toxigen_ar|5": 0,
    "lighteval|xstory_cloze:ar|0": 0
  },
  "config_tasks": {
    "community|acva:Algeria": {
      "name": "acva:Algeria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Algeria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Ancient_Egypt": {
      "name": "acva:Ancient_Egypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Ancient_Egypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 315,
      "effective_num_docs": 315,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arab_Empire": {
      "name": "acva:Arab_Empire",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arab_Empire",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Architecture": {
      "name": "acva:Arabic_Architecture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Architecture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Art": {
      "name": "acva:Arabic_Art",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Art",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Astronomy": {
      "name": "acva:Arabic_Astronomy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Calligraphy": {
      "name": "acva:Arabic_Calligraphy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Calligraphy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 255,
      "effective_num_docs": 255,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ceremony": {
      "name": "acva:Arabic_Ceremony",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ceremony",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 185,
      "effective_num_docs": 185,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Clothing": {
      "name": "acva:Arabic_Clothing",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Clothing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Culture": {
      "name": "acva:Arabic_Culture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Culture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Food": {
      "name": "acva:Arabic_Food",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Food",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Funeral": {
      "name": "acva:Arabic_Funeral",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Funeral",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Geography": {
      "name": "acva:Arabic_Geography",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_History": {
      "name": "acva:Arabic_History",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_History",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Language_Origin": {
      "name": "acva:Arabic_Language_Origin",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Language_Origin",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Literature": {
      "name": "acva:Arabic_Literature",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Literature",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Math": {
      "name": "acva:Arabic_Math",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Math",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Medicine": {
      "name": "acva:Arabic_Medicine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Music": {
      "name": "acva:Arabic_Music",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Music",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 139,
      "effective_num_docs": 139,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ornament": {
      "name": "acva:Arabic_Ornament",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ornament",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Philosophy": {
      "name": "acva:Arabic_Philosophy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry": {
      "name": "acva:Arabic_Physics_and_Chemistry",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Physics_and_Chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Wedding": {
      "name": "acva:Arabic_Wedding",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Wedding",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Bahrain": {
      "name": "acva:Bahrain",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Bahrain",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Comoros": {
      "name": "acva:Comoros",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Comoros",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Egypt_modern": {
      "name": "acva:Egypt_modern",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Egypt_modern",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromAncientEgypt": {
      "name": "acva:InfluenceFromAncientEgypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromAncientEgypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromByzantium": {
      "name": "acva:InfluenceFromByzantium",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromByzantium",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromChina": {
      "name": "acva:InfluenceFromChina",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromChina",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromGreece": {
      "name": "acva:InfluenceFromGreece",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromGreece",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromIslam": {
      "name": "acva:InfluenceFromIslam",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromIslam",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromPersia": {
      "name": "acva:InfluenceFromPersia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromPersia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromRome": {
      "name": "acva:InfluenceFromRome",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromRome",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Iraq": {
      "name": "acva:Iraq",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Iraq",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_Education": {
      "name": "acva:Islam_Education",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_Education",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_branches_and_schools": {
      "name": "acva:Islam_branches_and_schools",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_branches_and_schools",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islamic_law_system": {
      "name": "acva:Islamic_law_system",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islamic_law_system",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Jordan": {
      "name": "acva:Jordan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Jordan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Kuwait": {
      "name": "acva:Kuwait",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Kuwait",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Lebanon": {
      "name": "acva:Lebanon",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Lebanon",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Libya": {
      "name": "acva:Libya",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Libya",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mauritania": {
      "name": "acva:Mauritania",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mauritania",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mesopotamia_civilization": {
      "name": "acva:Mesopotamia_civilization",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mesopotamia_civilization",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 155,
      "effective_num_docs": 155,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Morocco": {
      "name": "acva:Morocco",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Morocco",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Oman": {
      "name": "acva:Oman",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Oman",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Palestine": {
      "name": "acva:Palestine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Palestine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Qatar": {
      "name": "acva:Qatar",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Qatar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Saudi_Arabia": {
      "name": "acva:Saudi_Arabia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Saudi_Arabia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Somalia": {
      "name": "acva:Somalia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Somalia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Sudan": {
      "name": "acva:Sudan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Sudan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Syria": {
      "name": "acva:Syria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Syria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Tunisia": {
      "name": "acva:Tunisia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Tunisia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:United_Arab_Emirates": {
      "name": "acva:United_Arab_Emirates",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "United_Arab_Emirates",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Yemen": {
      "name": "acva:Yemen",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Yemen",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 10,
      "effective_num_docs": 10,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:communication": {
      "name": "acva:communication",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "communication",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 364,
      "effective_num_docs": 364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:computer_and_phone": {
      "name": "acva:computer_and_phone",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "computer_and_phone",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:daily_life": {
      "name": "acva:daily_life",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "daily_life",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 337,
      "effective_num_docs": 337,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:entertainment": {
      "name": "acva:entertainment",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "entertainment",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_exams": {
      "name": "arabic_exams",
      "prompt_function": "arabic_exams",
      "hf_repo": "OALL/Arabic_EXAMS",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 537,
      "effective_num_docs": 537,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:abstract_algebra": {
      "name": "arabic_mmlu:abstract_algebra",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:anatomy": {
      "name": "arabic_mmlu:anatomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:astronomy": {
      "name": "arabic_mmlu:astronomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:business_ethics": {
      "name": "arabic_mmlu:business_ethics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:clinical_knowledge": {
      "name": "arabic_mmlu:clinical_knowledge",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_biology": {
      "name": "arabic_mmlu:college_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_chemistry": {
      "name": "arabic_mmlu:college_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_computer_science": {
      "name": "arabic_mmlu:college_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_mathematics": {
      "name": "arabic_mmlu:college_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_medicine": {
      "name": "arabic_mmlu:college_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_physics": {
      "name": "arabic_mmlu:college_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:computer_security": {
      "name": "arabic_mmlu:computer_security",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:conceptual_physics": {
      "name": "arabic_mmlu:conceptual_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:econometrics": {
      "name": "arabic_mmlu:econometrics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:electrical_engineering": {
      "name": "arabic_mmlu:electrical_engineering",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:elementary_mathematics": {
      "name": "arabic_mmlu:elementary_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:formal_logic": {
      "name": "arabic_mmlu:formal_logic",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:global_facts": {
      "name": "arabic_mmlu:global_facts",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_biology": {
      "name": "arabic_mmlu:high_school_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_chemistry": {
      "name": "arabic_mmlu:high_school_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_computer_science": {
      "name": "arabic_mmlu:high_school_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_european_history": {
      "name": "arabic_mmlu:high_school_european_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_geography": {
      "name": "arabic_mmlu:high_school_geography",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics": {
      "name": "arabic_mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics": {
      "name": "arabic_mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_mathematics": {
      "name": "arabic_mmlu:high_school_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_microeconomics": {
      "name": "arabic_mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_physics": {
      "name": "arabic_mmlu:high_school_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_psychology": {
      "name": "arabic_mmlu:high_school_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_statistics": {
      "name": "arabic_mmlu:high_school_statistics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_us_history": {
      "name": "arabic_mmlu:high_school_us_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_world_history": {
      "name": "arabic_mmlu:high_school_world_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_aging": {
      "name": "arabic_mmlu:human_aging",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_sexuality": {
      "name": "arabic_mmlu:human_sexuality",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:international_law": {
      "name": "arabic_mmlu:international_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:jurisprudence": {
      "name": "arabic_mmlu:jurisprudence",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:logical_fallacies": {
      "name": "arabic_mmlu:logical_fallacies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:machine_learning": {
      "name": "arabic_mmlu:machine_learning",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:management": {
      "name": "arabic_mmlu:management",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:marketing": {
      "name": "arabic_mmlu:marketing",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:medical_genetics": {
      "name": "arabic_mmlu:medical_genetics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:miscellaneous": {
      "name": "arabic_mmlu:miscellaneous",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_disputes": {
      "name": "arabic_mmlu:moral_disputes",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_scenarios": {
      "name": "arabic_mmlu:moral_scenarios",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:nutrition": {
      "name": "arabic_mmlu:nutrition",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:philosophy": {
      "name": "arabic_mmlu:philosophy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:prehistory": {
      "name": "arabic_mmlu:prehistory",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_accounting": {
      "name": "arabic_mmlu:professional_accounting",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_law": {
      "name": "arabic_mmlu:professional_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_medicine": {
      "name": "arabic_mmlu:professional_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_psychology": {
      "name": "arabic_mmlu:professional_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:public_relations": {
      "name": "arabic_mmlu:public_relations",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:security_studies": {
      "name": "arabic_mmlu:security_studies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:sociology": {
      "name": "arabic_mmlu:sociology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:us_foreign_policy": {
      "name": "arabic_mmlu:us_foreign_policy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:virology": {
      "name": "arabic_mmlu:virology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:world_religions": {
      "name": "arabic_mmlu:world_religions",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_challenge_okapi_ar": {
      "name": "arc_challenge_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_challenge_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1160,
      "effective_num_docs": 1160,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_easy_ar": {
      "name": "arc_easy_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_easy_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 2364,
      "effective_num_docs": 2364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|copa_ext_ar": {
      "name": "copa_ext_ar",
      "prompt_function": "copa_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "copa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 90,
      "effective_num_docs": 90,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|mmlu_okapi_ar": {
      "name": "mmlu_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "mmlu_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 12923,
      "effective_num_docs": 12923,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|openbook_qa_ext_ar": {
      "name": "openbook_qa_ext_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "openbook_qa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 495,
      "effective_num_docs": 495,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|piqa_ar": {
      "name": "piqa_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "piqa_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1833,
      "effective_num_docs": 1833,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|sciq_ar": {
      "name": "sciq_ar",
      "prompt_function": "sciq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "sciq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 995,
      "effective_num_docs": 995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|toxigen_ar": {
      "name": "toxigen_ar",
      "prompt_function": "toxigen_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "toxigen_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 935,
      "effective_num_docs": 935,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "lighteval|xstory_cloze:ar": {
      "name": "xstory_cloze:ar",
      "prompt_function": "storycloze",
      "hf_repo": "juletxara/xstory_cloze",
      "hf_subset": "ar",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "training",
        "eval"
      ],
      "evaluation_splits": [
        "eval"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "lighteval"
      ],
      "original_num_docs": 1511,
      "effective_num_docs": 1511,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|acva:Algeria|5": {
      "hashes": {
        "hash_examples": "da5a3003cd46f6f9",
        "hash_full_prompts": "c29c94a4778b91af",
        "hash_input_tokens": "7f16c59c2758f013",
        "hash_cont_tokens": "6484d37ce3fd6e86"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Ancient_Egypt|5": {
      "hashes": {
        "hash_examples": "52d6f767fede195b",
        "hash_full_prompts": "180fa0424d9930c1",
        "hash_input_tokens": "368b656a83288345",
        "hash_cont_tokens": "526da4804efcbb75"
      },
      "truncated": 0,
      "non_truncated": 315,
      "padded": 630,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arab_Empire|5": {
      "hashes": {
        "hash_examples": "8dacff6a79804a75",
        "hash_full_prompts": "887aee46e82b83be",
        "hash_input_tokens": "fe0d945641b4b312",
        "hash_cont_tokens": "58f3d8c86868d8ad"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 530,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Architecture|5": {
      "hashes": {
        "hash_examples": "df286cd862d9f6bb",
        "hash_full_prompts": "81711e01be90b5e6",
        "hash_input_tokens": "63944f232381d565",
        "hash_cont_tokens": "4c5704480f5c61c4"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Art|5": {
      "hashes": {
        "hash_examples": "112883d764118a49",
        "hash_full_prompts": "5edc75896c16741a",
        "hash_input_tokens": "c6da74fda7234353",
        "hash_cont_tokens": "149290002e3b9a0a"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Astronomy|5": {
      "hashes": {
        "hash_examples": "20dcdf2454bf8671",
        "hash_full_prompts": "a256f8ebf1979ebe",
        "hash_input_tokens": "1357ff832d3c529d",
        "hash_cont_tokens": "7ade76fb4fbdb753"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Calligraphy|5": {
      "hashes": {
        "hash_examples": "3a9f9d1ebe868a15",
        "hash_full_prompts": "6fa5839f835a8d91",
        "hash_input_tokens": "80bb3b7fdf98ac10",
        "hash_cont_tokens": "7784536f9e4237a6"
      },
      "truncated": 0,
      "non_truncated": 255,
      "padded": 510,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ceremony|5": {
      "hashes": {
        "hash_examples": "c927630f8d2f44da",
        "hash_full_prompts": "5c855368b76ce9a2",
        "hash_input_tokens": "1bcdf429c7333f9b",
        "hash_cont_tokens": "2a0eb312e263836c"
      },
      "truncated": 0,
      "non_truncated": 185,
      "padded": 370,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Clothing|5": {
      "hashes": {
        "hash_examples": "6ad0740c2ac6ac92",
        "hash_full_prompts": "07a8647e8f9c8d1b",
        "hash_input_tokens": "8bb49e3b26ad1c40",
        "hash_cont_tokens": "3d171c4c8902cfac"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Culture|5": {
      "hashes": {
        "hash_examples": "2177bd857ad872ae",
        "hash_full_prompts": "c1cf28608e1fb425",
        "hash_input_tokens": "276a19b6df79b8f1",
        "hash_cont_tokens": "d7807cf6bd7608da"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Food|5": {
      "hashes": {
        "hash_examples": "a6ada65b71d7c9c5",
        "hash_full_prompts": "86ba76526b296be2",
        "hash_input_tokens": "ecbd9ff7ccef4281",
        "hash_cont_tokens": "46abd96061b17868"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Funeral|5": {
      "hashes": {
        "hash_examples": "fcee39dc29eaae91",
        "hash_full_prompts": "cb5c8f47c7040564",
        "hash_input_tokens": "ba58d7e9557c4c42",
        "hash_cont_tokens": "7068715fb47d287a"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Geography|5": {
      "hashes": {
        "hash_examples": "d36eda7c89231c02",
        "hash_full_prompts": "28de532ce8af96b4",
        "hash_input_tokens": "bb4cc47ea51f27a5",
        "hash_cont_tokens": "49ce5c6cec2c95e4"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_History|5": {
      "hashes": {
        "hash_examples": "6354ac0d6db6a5fc",
        "hash_full_prompts": "6d6bed6651718cbd",
        "hash_input_tokens": "28e14fa81bdf5890",
        "hash_cont_tokens": "eb7a6028599682f3"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Language_Origin|5": {
      "hashes": {
        "hash_examples": "ddc967c8aca34402",
        "hash_full_prompts": "43331d29ba5ee173",
        "hash_input_tokens": "4d9196288d0a13f3",
        "hash_cont_tokens": "7396b152266fbdbe"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Literature|5": {
      "hashes": {
        "hash_examples": "4305379fd46be5d8",
        "hash_full_prompts": "bfa6c0c0b2cc3bf3",
        "hash_input_tokens": "325f730e51fe9f47",
        "hash_cont_tokens": "2adad87b6602a5e4"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Math|5": {
      "hashes": {
        "hash_examples": "dec621144f4d28be",
        "hash_full_prompts": "fc31377795d0278a",
        "hash_input_tokens": "579bc8414df1fc5e",
        "hash_cont_tokens": "0285860597dd0dd1"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Medicine|5": {
      "hashes": {
        "hash_examples": "2b344cdae9495ff2",
        "hash_full_prompts": "dd28c6072634e8b4",
        "hash_input_tokens": "4fb76dd76aa1ff64",
        "hash_cont_tokens": "f429652e45339b61"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Music|5": {
      "hashes": {
        "hash_examples": "0c54624d881944ce",
        "hash_full_prompts": "15efa221a080fba6",
        "hash_input_tokens": "2b1b8c9d6881adf4",
        "hash_cont_tokens": "30edadc939d3c7ba"
      },
      "truncated": 0,
      "non_truncated": 139,
      "padded": 278,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ornament|5": {
      "hashes": {
        "hash_examples": "251a4a84289d8bc1",
        "hash_full_prompts": "0726b58b307c920e",
        "hash_input_tokens": "9dfbde32e9824b7c",
        "hash_cont_tokens": "f8b9246b501b6971"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Philosophy|5": {
      "hashes": {
        "hash_examples": "3f86fb9c94c13d22",
        "hash_full_prompts": "4f314ee6b028a296",
        "hash_input_tokens": "7f2902ff4a0f1004",
        "hash_cont_tokens": "2b299921d8aba933"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry|5": {
      "hashes": {
        "hash_examples": "8fec65af3695b62a",
        "hash_full_prompts": "4946c67f4133bcd2",
        "hash_input_tokens": "24b7302ccf069da3",
        "hash_cont_tokens": "d7ac4386932ef97d"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Wedding|5": {
      "hashes": {
        "hash_examples": "9cc3477184d7a4b8",
        "hash_full_prompts": "7d55bb3d704ed684",
        "hash_input_tokens": "9ba9ffda41f71171",
        "hash_cont_tokens": "c88d434e6a67fd82"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Bahrain|5": {
      "hashes": {
        "hash_examples": "c92e803a0fa8b9e2",
        "hash_full_prompts": "be1b4f346995ec0a",
        "hash_input_tokens": "77acac5dbe4daca3",
        "hash_cont_tokens": "03a5bf3cd3e7e9a8"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Comoros|5": {
      "hashes": {
        "hash_examples": "06e5d4bba8e54cae",
        "hash_full_prompts": "5002502694357eaf",
        "hash_input_tokens": "5228e7beab47a556",
        "hash_cont_tokens": "6b6e33a0f80ca736"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Egypt_modern|5": {
      "hashes": {
        "hash_examples": "c6ec369164f93446",
        "hash_full_prompts": "d36abc64c365b67a",
        "hash_input_tokens": "4a09e31af678ee96",
        "hash_cont_tokens": "76a8a932c7dbc0f8"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromAncientEgypt|5": {
      "hashes": {
        "hash_examples": "b9d56d74818b9bd4",
        "hash_full_prompts": "291e1066a5795d9a",
        "hash_input_tokens": "2d097787df4c2482",
        "hash_cont_tokens": "c88d434e6a67fd82"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromByzantium|5": {
      "hashes": {
        "hash_examples": "5316c9624e7e59b8",
        "hash_full_prompts": "3787d57f746e98d0",
        "hash_input_tokens": "f9bf2299f230fc8c",
        "hash_cont_tokens": "4863ec58b1264af9"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromChina|5": {
      "hashes": {
        "hash_examples": "87894bce95a56411",
        "hash_full_prompts": "503a8884b533989e",
        "hash_input_tokens": "110df0dae9bbab77",
        "hash_cont_tokens": "e63f4ef7c32e9230"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromGreece|5": {
      "hashes": {
        "hash_examples": "0baa78a27e469312",
        "hash_full_prompts": "81e9040116fe5fa2",
        "hash_input_tokens": "3cd4e96338313003",
        "hash_cont_tokens": "6d642af6a0ad7ec2"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromIslam|5": {
      "hashes": {
        "hash_examples": "0c2532cde6541ff2",
        "hash_full_prompts": "26910f4b59987e00",
        "hash_input_tokens": "1e873c11d60953b1",
        "hash_cont_tokens": "a070ab1bac65ac2f"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromPersia|5": {
      "hashes": {
        "hash_examples": "efcd8112dc53c6e5",
        "hash_full_prompts": "14e956e7740010ad",
        "hash_input_tokens": "edf97c4d52a18fba",
        "hash_cont_tokens": "72384c2e9ee0f866"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromRome|5": {
      "hashes": {
        "hash_examples": "9db61480e2e85fd3",
        "hash_full_prompts": "031043cb3d0601e7",
        "hash_input_tokens": "87181d3f6b075e77",
        "hash_cont_tokens": "12e38d21bb4835ff"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Iraq|5": {
      "hashes": {
        "hash_examples": "96dac3dfa8d2f41f",
        "hash_full_prompts": "314e7d813cc0ebb0",
        "hash_input_tokens": "07ee13faeee71094",
        "hash_cont_tokens": "a30fae2e732c6022"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_Education|5": {
      "hashes": {
        "hash_examples": "0d80355f6a4cb51b",
        "hash_full_prompts": "5c72236d1ab83ca8",
        "hash_input_tokens": "8ec4ef58ef0aeff7",
        "hash_cont_tokens": "593b02b95d801477"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_branches_and_schools|5": {
      "hashes": {
        "hash_examples": "5cedce1be2c3ad50",
        "hash_full_prompts": "6073825f4be45d10",
        "hash_input_tokens": "643ec90b4cd216a2",
        "hash_cont_tokens": "5f9ac53328d427ef"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islamic_law_system|5": {
      "hashes": {
        "hash_examples": "c0e6db8bc84e105e",
        "hash_full_prompts": "d4a26fdef76078d0",
        "hash_input_tokens": "cb1bffa57939bb89",
        "hash_cont_tokens": "806a176aef691a13"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Jordan|5": {
      "hashes": {
        "hash_examples": "33deb5b4e5ddd6a1",
        "hash_full_prompts": "3f74d1fb89882c0c",
        "hash_input_tokens": "50744c69efc117df",
        "hash_cont_tokens": "ce6cee3822c60242"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Kuwait|5": {
      "hashes": {
        "hash_examples": "eb41773346d7c46c",
        "hash_full_prompts": "75a56fd0b085c6b2",
        "hash_input_tokens": "ddfb159ea3187dc6",
        "hash_cont_tokens": "29840e78dadb50df"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Lebanon|5": {
      "hashes": {
        "hash_examples": "25932dbf4c13d34f",
        "hash_full_prompts": "2bcf2e3b8ff223fe",
        "hash_input_tokens": "1632ca2ce49a136e",
        "hash_cont_tokens": "c22797523996d7ae"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Libya|5": {
      "hashes": {
        "hash_examples": "f2c4db63cd402926",
        "hash_full_prompts": "e7d725abcb4313af",
        "hash_input_tokens": "e7fde696798b1c5b",
        "hash_cont_tokens": "4ca8206ee8f187d8"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mauritania|5": {
      "hashes": {
        "hash_examples": "8723ab5fdf286b54",
        "hash_full_prompts": "f0e7d75900f14fcf",
        "hash_input_tokens": "cfd85ce87268a497",
        "hash_cont_tokens": "cdc7448c2bca87d4"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mesopotamia_civilization|5": {
      "hashes": {
        "hash_examples": "c33f5502a6130ca9",
        "hash_full_prompts": "8de23dd7e0dc6435",
        "hash_input_tokens": "ed0b275bf54a7a58",
        "hash_cont_tokens": "2a6ff0bfc5b1af5c"
      },
      "truncated": 0,
      "non_truncated": 155,
      "padded": 310,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Morocco|5": {
      "hashes": {
        "hash_examples": "588a5ed27904b1ae",
        "hash_full_prompts": "24847db696527ca5",
        "hash_input_tokens": "5663befae9b28f53",
        "hash_cont_tokens": "cdc7448c2bca87d4"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Oman|5": {
      "hashes": {
        "hash_examples": "d447c52b94248b69",
        "hash_full_prompts": "a5b8786aba97e97f",
        "hash_input_tokens": "8e5b247f0bd390e3",
        "hash_cont_tokens": "870c09232598738f"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Palestine|5": {
      "hashes": {
        "hash_examples": "19197e076ad14ff5",
        "hash_full_prompts": "c4070490cd3bda3e",
        "hash_input_tokens": "480e45a671874250",
        "hash_cont_tokens": "6bc297e5db30df3b"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Qatar|5": {
      "hashes": {
        "hash_examples": "cf0736fa185b28f6",
        "hash_full_prompts": "a37061703a491998",
        "hash_input_tokens": "9ce0fb0b84fb1e6b",
        "hash_cont_tokens": "0dbcf3fd50fdaef6"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Saudi_Arabia|5": {
      "hashes": {
        "hash_examples": "69beda6e1b85a08d",
        "hash_full_prompts": "33655b54c7bcc6db",
        "hash_input_tokens": "45b994b3083fdcb5",
        "hash_cont_tokens": "cf81e4b0f4d77963"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Somalia|5": {
      "hashes": {
        "hash_examples": "b387940c65784fbf",
        "hash_full_prompts": "9f380e51f126e8e3",
        "hash_input_tokens": "ce3fd90218f22ec2",
        "hash_cont_tokens": "03a5bf3cd3e7e9a8"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Sudan|5": {
      "hashes": {
        "hash_examples": "e02c32b9d2dd0c3f",
        "hash_full_prompts": "ed1ff80cdafbb906",
        "hash_input_tokens": "b5b48a844117764c",
        "hash_cont_tokens": "108c20e8fbcf8a53"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Syria|5": {
      "hashes": {
        "hash_examples": "60a6f8fe73bda4bb",
        "hash_full_prompts": "24478f579bfa86a2",
        "hash_input_tokens": "127cfdb96c500069",
        "hash_cont_tokens": "c22797523996d7ae"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Tunisia|5": {
      "hashes": {
        "hash_examples": "34bb15d3830c5649",
        "hash_full_prompts": "33ee55e0c12fcd78",
        "hash_input_tokens": "66f12458093b78ff",
        "hash_cont_tokens": "108c20e8fbcf8a53"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:United_Arab_Emirates|5": {
      "hashes": {
        "hash_examples": "98a0ba78172718ce",
        "hash_full_prompts": "513edd41b5b39486",
        "hash_input_tokens": "117d84de34b56e62",
        "hash_cont_tokens": "8c8353801553c773"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Yemen|5": {
      "hashes": {
        "hash_examples": "18e9bcccbb4ced7a",
        "hash_full_prompts": "5bb795ad5c240147",
        "hash_input_tokens": "3b9b52b904d3f2e6",
        "hash_cont_tokens": "5a8eb2425bc7eba7"
      },
      "truncated": 0,
      "non_truncated": 10,
      "padded": 20,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:communication|5": {
      "hashes": {
        "hash_examples": "9ff28ab5eab5c97b",
        "hash_full_prompts": "e6adba04b96e808d",
        "hash_input_tokens": "0cf8e5f32048057e",
        "hash_cont_tokens": "aa0f33f080a9125d"
      },
      "truncated": 0,
      "non_truncated": 364,
      "padded": 728,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:computer_and_phone|5": {
      "hashes": {
        "hash_examples": "37bac2f086aaf6c2",
        "hash_full_prompts": "20ca19372ec6bb12",
        "hash_input_tokens": "c907cc766adfbeaa",
        "hash_cont_tokens": "fcec79b94fdb49c7"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:daily_life|5": {
      "hashes": {
        "hash_examples": "bf07363c1c252e2f",
        "hash_full_prompts": "971ea143f1975413",
        "hash_input_tokens": "88a93c2f7af96ee6",
        "hash_cont_tokens": "3e9d74ff116c0fba"
      },
      "truncated": 0,
      "non_truncated": 337,
      "padded": 674,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:entertainment|5": {
      "hashes": {
        "hash_examples": "37077bc00f0ac56a",
        "hash_full_prompts": "00278d9786c51665",
        "hash_input_tokens": "11facdbfa5e9bc06",
        "hash_cont_tokens": "37a502e0dcee40f9"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:mcq_exams_test_ar|5": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "afb81428d520b7b9",
        "hash_input_tokens": "ef3a080f9a24c01c",
        "hash_cont_tokens": "ee1b53982c3aa963"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2228,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|5": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "0941d95a4f024485",
        "hash_input_tokens": "68c656aca1653dc3",
        "hash_cont_tokens": "653755d19e9994d7"
      },
      "truncated": 0,
      "non_truncated": 5395,
      "padded": 21491,
      "non_padded": 89,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_msa|5": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "c72e27fe401bf91e",
        "hash_input_tokens": "adbb0eae900fd740",
        "hash_cont_tokens": "a1d33ba52d373aee"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3556,
      "non_padded": 24,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "9f3b87aa7650fbe9",
        "hash_input_tokens": "a16d378001497c67",
        "hash_cont_tokens": "72d149d362c484ee"
      },
      "truncated": 0,
      "non_truncated": 75,
      "padded": 150,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "c968b407b946a908",
        "hash_input_tokens": "42b419cc1b9929fb",
        "hash_cont_tokens": "bba7b6b5f4b50035"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "0304687a49443a1c",
        "hash_input_tokens": "3338ad93a3e572d8",
        "hash_cont_tokens": "02abe91b1c4823c0"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "f2c03fad137267e7",
        "hash_input_tokens": "64f9553a00cd1b33",
        "hash_cont_tokens": "d5242ed11cf12ab5"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|5": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "fc2bbf3f82ee99d5",
        "hash_input_tokens": "a9ce9f618105e0ad",
        "hash_cont_tokens": "3dbee4474486152a"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17985,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|5": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "1d942001b16272af",
        "hash_input_tokens": "ab7dac2458c3c357",
        "hash_cont_tokens": "05ae36d51d8d95ea"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5160,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_exams|5": {
      "hashes": {
        "hash_examples": "6d721df351722656",
        "hash_full_prompts": "9241f1fcb58c868b",
        "hash_input_tokens": "76b88dd1836a1228",
        "hash_cont_tokens": "a628ab4017667645"
      },
      "truncated": 0,
      "non_truncated": 537,
      "padded": 2148,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "f2ddca8f45c0a511",
        "hash_full_prompts": "7dae76e9c966c8ee",
        "hash_input_tokens": "4fa7ebe3eeeb40ee",
        "hash_cont_tokens": "5369b1aaad52758d"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "dfdbc1b83107668d",
        "hash_full_prompts": "96b301d0d23a23b2",
        "hash_input_tokens": "63ef4bb486bb4beb",
        "hash_cont_tokens": "709a27c01da865b6"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "9736a606002a848e",
        "hash_full_prompts": "18aab550cbf246fb",
        "hash_input_tokens": "2538acce271b715e",
        "hash_cont_tokens": "3205cdab4e98fc7c"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "735e452fbb6dc63d",
        "hash_full_prompts": "0b6d539453c08221",
        "hash_input_tokens": "6bdab74dbf739b5d",
        "hash_cont_tokens": "73489838311ef2b3"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "6ab0ca4da98aedcf",
        "hash_full_prompts": "35e95a711827936f",
        "hash_input_tokens": "669e5a5fbab2e0aa",
        "hash_cont_tokens": "b37d0c796f14e058"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "17e4e390848018a4",
        "hash_full_prompts": "27484c8bd9570e0e",
        "hash_input_tokens": "f50362a48b6780b1",
        "hash_cont_tokens": "218025eb0c512c9b"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "4abb169f6dfd234b",
        "hash_full_prompts": "bb78f6101049e7f6",
        "hash_input_tokens": "1a85c292708bfe4c",
        "hash_cont_tokens": "b163255e28f7f51f"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "a369e2e941358a1e",
        "hash_full_prompts": "154a49a7f108784b",
        "hash_input_tokens": "dbb8a3bf42a82961",
        "hash_cont_tokens": "b6708df06c9882b0"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "d7be03b8b6020bff",
        "hash_full_prompts": "e6a4d0be1daf1046",
        "hash_input_tokens": "7e689ed996baf5c4",
        "hash_cont_tokens": "5639bb02955bc343"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "0518a00f097346bf",
        "hash_full_prompts": "77eb0c4b4114f286",
        "hash_input_tokens": "ae7a090a43a299c1",
        "hash_cont_tokens": "ce778d5b729b6d13"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "5d842cd49bc70e12",
        "hash_full_prompts": "790ba8d9706901f1",
        "hash_input_tokens": "bdd7db082ef29846",
        "hash_cont_tokens": "6ee096443bab6e78"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "8e85d9f85be9b32f",
        "hash_full_prompts": "82e46497a611b83c",
        "hash_input_tokens": "b80b7af789dc3cd1",
        "hash_cont_tokens": "25b603a28c62f6fb"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "7964b55a0a49502b",
        "hash_full_prompts": "7ba8d0ef2ee4c1ad",
        "hash_input_tokens": "a00627247713967e",
        "hash_cont_tokens": "2ff86a5d10a2127a"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "1e192eae38347257",
        "hash_full_prompts": "aaf20c2b20aa94cb",
        "hash_input_tokens": "a0082e356f1da24a",
        "hash_cont_tokens": "75813f241cbf8ccb"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "cf97671d5c441da1",
        "hash_full_prompts": "0be6a375839169cf",
        "hash_input_tokens": "7cccd015356754ab",
        "hash_cont_tokens": "238a9cbac681f7a6"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "6f49107ed43c40c5",
        "hash_full_prompts": "40389b2e90ff570d",
        "hash_input_tokens": "9bdd579c4f845a86",
        "hash_cont_tokens": "d00485b6b9b1a7b2"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "7922c376008ba77b",
        "hash_full_prompts": "fdb00d752feca5dc",
        "hash_input_tokens": "6ccb639b27640b9d",
        "hash_cont_tokens": "19e8801ef04061cd"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "11f9813185047d5b",
        "hash_full_prompts": "c77b6431a7de1a69",
        "hash_input_tokens": "5c33b16c015f502c",
        "hash_cont_tokens": "5f5d338db1664931"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "2a804b1d90cbe66e",
        "hash_full_prompts": "73da779f407e8b11",
        "hash_input_tokens": "f729c756688d609f",
        "hash_cont_tokens": "785865f9ca0dced1"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "0032168adabc53b4",
        "hash_full_prompts": "f32312ef923f9535",
        "hash_input_tokens": "0d859f67567d2d55",
        "hash_cont_tokens": "d30d155b83b8beee"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "f2fb8740f9df980f",
        "hash_full_prompts": "190b084262e9d0e0",
        "hash_input_tokens": "347f07d0e226d1ce",
        "hash_cont_tokens": "7d6dcd74fb3d5a4b"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "73509021e7e66435",
        "hash_full_prompts": "efd792379887f040",
        "hash_input_tokens": "0941037635b55ad2",
        "hash_cont_tokens": "e82891868d8aa3cd"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "9e08d1894940ff42",
        "hash_full_prompts": "f4585f814c7f4de2",
        "hash_input_tokens": "0d682deba2593d64",
        "hash_cont_tokens": "3c2d08575146932d"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "64b7e97817ca6c76",
        "hash_full_prompts": "add1904e5a48de31",
        "hash_input_tokens": "6dec084eedc2e852",
        "hash_cont_tokens": "a7b7dba49dc6450c"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "9f582da8534bd2ef",
        "hash_full_prompts": "6ec9972b6419a57d",
        "hash_input_tokens": "3b72add12df60fa3",
        "hash_cont_tokens": "606c1a2137551055"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd54f1c10d423c51",
        "hash_full_prompts": "7dbb722d48b01d1a",
        "hash_input_tokens": "05758494fef7b734",
        "hash_cont_tokens": "f18ea16235393e7a"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "7037896925aaf42f",
        "hash_full_prompts": "cf06d07948536fde",
        "hash_input_tokens": "f9e36fdd6424ccf5",
        "hash_cont_tokens": "660389c47a0e76aa"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "60c3776215167dae",
        "hash_full_prompts": "e6d5f073ef80880e",
        "hash_input_tokens": "f90b3b43aabb757a",
        "hash_cont_tokens": "bdbeaa07f4cc342f"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "61176bfd5da1298f",
        "hash_full_prompts": "ee7f4f2064eae0a4",
        "hash_input_tokens": "0f619756dc5ece92",
        "hash_cont_tokens": "c468eef9700dcf3e"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "40dfeebd1ea10f76",
        "hash_full_prompts": "4514903d4f8f54bb",
        "hash_input_tokens": "f242872966598c17",
        "hash_cont_tokens": "59fd7d38a40f252d"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "03daa510ba917f4d",
        "hash_full_prompts": "44063e13b88958e9",
        "hash_input_tokens": "6795730c599a9946",
        "hash_cont_tokens": "aebae83b40caede1"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "be075ffd579f43c2",
        "hash_full_prompts": "a10fe5b5e82cae03",
        "hash_input_tokens": "1705be82cc67c8e0",
        "hash_cont_tokens": "cd821cb9a5c13f56"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "caa5b69f640bd1ef",
        "hash_full_prompts": "ef1e39e2ed4b7062",
        "hash_input_tokens": "3c989d9c8a827e78",
        "hash_cont_tokens": "7bf358fd838eb005"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "5ed2e38fb25a3767",
        "hash_full_prompts": "97ed1d98bc4fe65a",
        "hash_input_tokens": "476dc877411e5eef",
        "hash_cont_tokens": "8c45b597fb2c4a20"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 517,
      "non_padded": 7,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "4e3e9e28d1b96484",
        "hash_full_prompts": "10b514961edfc393",
        "hash_input_tokens": "532b7ed009757220",
        "hash_cont_tokens": "7a71b3c8eea40099"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e264b755366310b3",
        "hash_full_prompts": "2e06fff9ab533915",
        "hash_input_tokens": "919314c80b825f87",
        "hash_cont_tokens": "d36ff334e7c82fbb"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 420,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "a4ab6965a3e38071",
        "hash_full_prompts": "23ecda0d323885aa",
        "hash_input_tokens": "40d9d27a0591a0c7",
        "hash_cont_tokens": "636d46707e1a84a1"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "b92320efa6636b40",
        "hash_full_prompts": "d176e1708e5d03dd",
        "hash_input_tokens": "58159a07478537b7",
        "hash_cont_tokens": "9459a05b142f955c"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:management|5": {
      "hashes": {
        "hash_examples": "c9ee4872a850fe20",
        "hash_full_prompts": "2925872de18555d3",
        "hash_input_tokens": "92400f55177ca405",
        "hash_cont_tokens": "5654fad380e893a8"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "0c151b70f6a047e3",
        "hash_full_prompts": "14844c0e262b85c6",
        "hash_input_tokens": "d5f320a154c2d214",
        "hash_cont_tokens": "5bda5f245db38172"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 936,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "513f6cb8fca3a24e",
        "hash_full_prompts": "7651deeb7bf367df",
        "hash_input_tokens": "efd6a9c80cc69b84",
        "hash_cont_tokens": "4148b5a26c2d45d0"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "259a190d635331db",
        "hash_full_prompts": "bc4aa9321c563043",
        "hash_input_tokens": "e086bb79dfc40514",
        "hash_cont_tokens": "ba1c2fe12703c820"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3128,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "b85052c48a0b7bc3",
        "hash_full_prompts": "3bfcc2ad730416a4",
        "hash_input_tokens": "f3238e8b419ef12c",
        "hash_cont_tokens": "3651587a421fb96c"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "28d0b069ef00dd00",
        "hash_full_prompts": "21227b832f92977e",
        "hash_input_tokens": "2b954b70447bcc87",
        "hash_cont_tokens": "72de15da472475af"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3519,
      "non_padded": 61,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "00c9bc5f1d305b2f",
        "hash_full_prompts": "5f7a9b4dabc0305e",
        "hash_input_tokens": "38f5d74328cd67a8",
        "hash_cont_tokens": "e8a6aee2a612d3a6"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1224,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a458c08454a3fd5f",
        "hash_full_prompts": "02e6601b66c6ffac",
        "hash_input_tokens": "9e83954b69e2014a",
        "hash_cont_tokens": "ca47071a4fe1a01b"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1244,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "d6a0ecbdbb670e9c",
        "hash_full_prompts": "5093e3964d99c472",
        "hash_input_tokens": "8bccac94eb64d43a",
        "hash_cont_tokens": "5afbd6d58d1e34cd"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1296,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "b4a95fe480b6540e",
        "hash_full_prompts": "ca667d2f4db1c8ee",
        "hash_input_tokens": "763cebcd3a159657",
        "hash_cont_tokens": "9a665c069b715c6a"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1128,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "c2be9651cdbdde3b",
        "hash_full_prompts": "b0ea37516b2917c1",
        "hash_input_tokens": "3e6cca6b0796a497",
        "hash_cont_tokens": "f1a9b1418ad18732"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6136,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "26ce92416288f273",
        "hash_full_prompts": "cb6d0ea35235b449",
        "hash_input_tokens": "a5af1c9b3182113b",
        "hash_cont_tokens": "9e6aa585f9c04e4e"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "71ea5f182ea9a641",
        "hash_full_prompts": "571222520787adc2",
        "hash_input_tokens": "af1f4f9684420087",
        "hash_cont_tokens": "6a07f60a297e1acd"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2448,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "125adc21f91f8d77",
        "hash_full_prompts": "917adaee95dc1ca4",
        "hash_input_tokens": "676c901d2d5f0755",
        "hash_cont_tokens": "b69d8365e6f3f4eb"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 440,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "3c18b216c099fb26",
        "hash_full_prompts": "b1805f9a0657f718",
        "hash_input_tokens": "81955f5c925f4843",
        "hash_cont_tokens": "6a568819794b6914"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "3f2a9634cef7417d",
        "hash_full_prompts": "b3fccb5fe12ce4f8",
        "hash_input_tokens": "220cd786fe78d307",
        "hash_cont_tokens": "9658e519b295ce64"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 804,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "22249da54056475e",
        "hash_full_prompts": "d1ed3520cfcb7c44",
        "hash_input_tokens": "3b2c03723b426f30",
        "hash_cont_tokens": "b2e0631a38ffef23"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:virology|5": {
      "hashes": {
        "hash_examples": "9d194b9471dc624e",
        "hash_full_prompts": "780f7b0e04e47c82",
        "hash_input_tokens": "5f18926dd482f8a5",
        "hash_cont_tokens": "0046ac3faecbec5f"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 652,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "229e5fe50082b064",
        "hash_full_prompts": "0ed2b106f1fca3f3",
        "hash_input_tokens": "6340add8c5b3a5db",
        "hash_cont_tokens": "29075a897251ced4"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_challenge_okapi_ar|5": {
      "hashes": {
        "hash_examples": "ab893807673bc355",
        "hash_full_prompts": "cd42d148b1c3e15f",
        "hash_input_tokens": "a5deea3343ac9368",
        "hash_cont_tokens": "1b2108617b8b2072"
      },
      "truncated": 0,
      "non_truncated": 1160,
      "padded": 4640,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_easy_ar|5": {
      "hashes": {
        "hash_examples": "acb688624acc3d04",
        "hash_full_prompts": "d99289bd63ebdf59",
        "hash_input_tokens": "a5838ab4645593af",
        "hash_cont_tokens": "e0485a137d9ec804"
      },
      "truncated": 0,
      "non_truncated": 2364,
      "padded": 9456,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|boolq_ar|5": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "df202c70a074556c",
        "hash_input_tokens": "556f450eda4bb3cc",
        "hash_cont_tokens": "07f834739b254e05"
      },
      "truncated": 0,
      "non_truncated": 3260,
      "padded": 6500,
      "non_padded": 20,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|copa_ext_ar|5": {
      "hashes": {
        "hash_examples": "9bb83301bb72eecf",
        "hash_full_prompts": "7ef156dc64bf3171",
        "hash_input_tokens": "fa808dbb47794ffd",
        "hash_cont_tokens": "a7eb38860be8d3a1"
      },
      "truncated": 0,
      "non_truncated": 90,
      "padded": 180,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|5": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "02da55f34584b064",
        "hash_input_tokens": "2544cfa9e71a6ffb",
        "hash_cont_tokens": "42e8b9bde189bbcb"
      },
      "truncated": 0,
      "non_truncated": 9171,
      "padded": 36684,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|mmlu_okapi_ar|5": {
      "hashes": {
        "hash_examples": "723b5371ad6fdc31",
        "hash_full_prompts": "a08c0e69d6187615",
        "hash_input_tokens": "f3c52cef170165ff",
        "hash_cont_tokens": "662b0456c846a7f9"
      },
      "truncated": 0,
      "non_truncated": 12923,
      "padded": 51266,
      "non_padded": 426,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|openbook_qa_ext_ar|5": {
      "hashes": {
        "hash_examples": "923d41eb0aca93eb",
        "hash_full_prompts": "4ec4310687d06c92",
        "hash_input_tokens": "15ec80106ec08d68",
        "hash_cont_tokens": "9d597c7c79659a12"
      },
      "truncated": 0,
      "non_truncated": 495,
      "padded": 1933,
      "non_padded": 47,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|piqa_ar|5": {
      "hashes": {
        "hash_examples": "94bc205a520d3ea0",
        "hash_full_prompts": "368106b19af7b6d3",
        "hash_input_tokens": "1cd25615c1ad65b1",
        "hash_cont_tokens": "4fd0a96877b27c6f"
      },
      "truncated": 0,
      "non_truncated": 1833,
      "padded": 3666,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|race_ar|5": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "a5790ecd9a3889d7",
        "hash_input_tokens": "f747b6ead3a94863",
        "hash_cont_tokens": "527babe48f20f83b"
      },
      "truncated": 0,
      "non_truncated": 4929,
      "padded": 19715,
      "non_padded": 1,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|sciq_ar|5": {
      "hashes": {
        "hash_examples": "430c34cdec154280",
        "hash_full_prompts": "2330d6e873adb64b",
        "hash_input_tokens": "ed8886b1a102a515",
        "hash_cont_tokens": "f749865bed5ff535"
      },
      "truncated": 0,
      "non_truncated": 995,
      "padded": 3975,
      "non_padded": 5,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|toxigen_ar|5": {
      "hashes": {
        "hash_examples": "1e139513004a9a2e",
        "hash_full_prompts": "1073f8d7361001b3",
        "hash_input_tokens": "c40d1c494e697b04",
        "hash_cont_tokens": "9afdecdfd147f204"
      },
      "truncated": 0,
      "non_truncated": 935,
      "padded": 1870,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "lighteval|xstory_cloze:ar|0": {
      "hashes": {
        "hash_examples": "865426a22c787481",
        "hash_full_prompts": "865426a22c787481",
        "hash_input_tokens": "bbe8a920567c49b0",
        "hash_cont_tokens": "3d63e467278c8498"
      },
      "truncated": 0,
      "non_truncated": 1511,
      "padded": 3022,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "0dea42e28d3919f0",
      "hash_full_prompts": "28a45f3b0b3c6385",
      "hash_input_tokens": "a9f7c457bc56319d",
      "hash_cont_tokens": "2d769355a7f81c81"
    },
    "truncated": 0,
    "non_truncated": 85887,
    "padded": 286607,
    "non_padded": 708,
    "num_truncated_few_shots": 0
  }
}