{
  "config_general": {
    "lighteval_sha": "4d8c620733802aba964d87aafb12739c8f72bc7d",
    "num_fewshot_seeds": 1,
    "override_batch_size": 2,
    "max_samples": null,
    "job_id": "",
    "start_time": 439965.21534893,
    "end_time": 450079.473525033,
    "total_evaluation_time_secondes": "10114.258176103001",
    "model_name": "google/gemma-2-9b-it",
    "model_sha": "1937c70277fcc5f7fb0fc772fc5bc69378996e71",
    "model_dtype": "torch.bfloat16",
    "model_size": "17.21 GB",
    "config": null
  },
  "results": {
    "community|alghafa:mcq_exams_test_ar|5": {
      "acc_norm": 0.5080789946140036,
      "acc_norm_stderr": 0.021201954990201367
    },
    "community|alghafa:meta_ar_dialects|5": {
      "acc_norm": 0.7375347544022243,
      "acc_norm_stderr": 0.0059906186770693525
    },
    "community|alghafa:meta_ar_msa|5": {
      "acc_norm": 0.8659217877094972,
      "acc_norm_stderr": 0.01139592755075933
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": {
      "acc_norm": 0.92,
      "acc_norm_stderr": 0.03153719382878881
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": {
      "acc_norm": 0.9333333333333333,
      "acc_norm_stderr": 0.02043523444594048
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": {
      "acc_norm": 0.9,
      "acc_norm_stderr": 0.0245769576155712
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": {
      "acc_norm": 0.8823014383989993,
      "acc_norm_stderr": 0.0036042258680382125
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|5": {
      "acc_norm": 0.6015012510425355,
      "acc_norm_stderr": 0.006323730880188185
    },
    "community|alghafa:multiple_choice_sentiment_task|5": {
      "acc_norm": 0.4180232558139535,
      "acc_norm_stderr": 0.011896387409582182
    },
    "community|alghafa:_average|5": {
      "acc_norm": 0.7518549794793941,
      "acc_norm_stderr": 0.015218025696237682
    },
    "all": {
      "acc_norm": 0.7518549794793941,
      "acc_norm_stderr": 0.015218025696237682
    }
  },
  "versions": {
    "community|alghafa:mcq_exams_test_ar|5": 0,
    "community|alghafa:meta_ar_dialects|5": 0,
    "community|alghafa:meta_ar_msa|5": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|5": 0,
    "community|alghafa:multiple_choice_sentiment_task|5": 0
  },
  "config_tasks": {
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|alghafa:mcq_exams_test_ar|5": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "afb81428d520b7b9",
        "hash_input_tokens": "7a7e579eb5899cf9",
        "hash_cont_tokens": "1fab22047758ad27"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2213,
      "non_padded": 15,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|5": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "0941d95a4f024485",
        "hash_input_tokens": "56cf7d55ddd7f658",
        "hash_cont_tokens": "b18b10681bbfddad"
      },
      "truncated": 0,
      "non_truncated": 5395,
      "padded": 21568,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_msa|5": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "c72e27fe401bf91e",
        "hash_input_tokens": "3a32c4366b057775",
        "hash_cont_tokens": "ab1e630b4fce8b57"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3563,
      "non_padded": 17,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|5": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "9f3b87aa7650fbe9",
        "hash_input_tokens": "d4033d986c48fca1",
        "hash_cont_tokens": "a05cfc77c23206cd"
      },
      "truncated": 0,
      "non_truncated": 75,
      "padded": 148,
      "non_padded": 2,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|5": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "c968b407b946a908",
        "hash_input_tokens": "cb613e9455db1f2e",
        "hash_cont_tokens": "16b90d811eb777ab"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 748,
      "non_padded": 2,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|5": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "0304687a49443a1c",
        "hash_input_tokens": "5b26f2e8b82a3b60",
        "hash_cont_tokens": "6143cf0b10f97e99"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|5": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "f2c03fad137267e7",
        "hash_input_tokens": "a8dad610f42a6ba5",
        "hash_cont_tokens": "164341aa03c26248"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|5": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "fc2bbf3f82ee99d5",
        "hash_input_tokens": "2685b4b544a8c0b0",
        "hash_cont_tokens": "ea2bd7f7244fd643"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17618,
      "non_padded": 367,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|5": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "1d942001b16272af",
        "hash_input_tokens": "c7075107a014099d",
        "hash_cont_tokens": "f18dec4c5aa7787c"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5160,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "3b59f3a8541ff2cd",
      "hash_full_prompts": "66a7e56fa4a793d0",
      "hash_input_tokens": "a328ce6f4e763df7",
      "hash_cont_tokens": "51fb9e83fe0a6a60"
    },
    "truncated": 0,
    "non_truncated": 22932,
    "padded": 67758,
    "non_padded": 415,
    "num_truncated_few_shots": 0
  }
}