{
  "config_general": {
    "lighteval_sha": "2c6fcd34da05fa752b6107ecd4c20a0de41a02ba",
    "num_fewshot_seeds": 1,
    "override_batch_size": 2,
    "max_samples": null,
    "job_id": "",
    "start_time": 102495.172947633,
    "end_time": 121508.384444625,
    "total_evaluation_time_secondes": "19013.211496992008",
    "model_name": "google/gemma-2-9b-it",
    "model_sha": "32e4f3ede2e468e868e20cb3fdae8761f2594544",
    "model_dtype": "torch.bfloat16",
    "model_size": "17.21 GB",
    "config": null
  },
  "results": {
    "community|acva:Algeria|0": {
      "acc_norm": 0.7025641025641025,
      "acc_norm_stderr": 0.0328200171783881
    },
    "community|acva:Ancient_Egypt|0": {
      "acc_norm": 0.1619047619047619,
      "acc_norm_stderr": 0.020787950482866274
    },
    "community|acva:Arab_Empire|0": {
      "acc_norm": 0.32452830188679244,
      "acc_norm_stderr": 0.028815615713432115
    },
    "community|acva:Arabic_Architecture|0": {
      "acc_norm": 0.5384615384615384,
      "acc_norm_stderr": 0.03579154352544571
    },
    "community|acva:Arabic_Art|0": {
      "acc_norm": 0.4153846153846154,
      "acc_norm_stderr": 0.03538013280575029
    },
    "community|acva:Arabic_Astronomy|0": {
      "acc_norm": 0.46153846153846156,
      "acc_norm_stderr": 0.0357915435254457
    },
    "community|acva:Arabic_Calligraphy|0": {
      "acc_norm": 0.6862745098039216,
      "acc_norm_stderr": 0.02911434198875567
    },
    "community|acva:Arabic_Ceremony|0": {
      "acc_norm": 0.572972972972973,
      "acc_norm_stderr": 0.03646580777990099
    },
    "community|acva:Arabic_Clothing|0": {
      "acc_norm": 0.5692307692307692,
      "acc_norm_stderr": 0.03555213252058761
    },
    "community|acva:Arabic_Culture|0": {
      "acc_norm": 0.28717948717948716,
      "acc_norm_stderr": 0.032483733385398866
    },
    "community|acva:Arabic_Food|0": {
      "acc_norm": 0.717948717948718,
      "acc_norm_stderr": 0.03230798601799116
    },
    "community|acva:Arabic_Funeral|0": {
      "acc_norm": 0.4421052631578947,
      "acc_norm_stderr": 0.051224183891818126
    },
    "community|acva:Arabic_Geography|0": {
      "acc_norm": 0.6896551724137931,
      "acc_norm_stderr": 0.038552896163789485
    },
    "community|acva:Arabic_History|0": {
      "acc_norm": 0.31794871794871793,
      "acc_norm_stderr": 0.03343383454355787
    },
    "community|acva:Arabic_Language_Origin|0": {
      "acc_norm": 0.6210526315789474,
      "acc_norm_stderr": 0.050036822652392066
    },
    "community|acva:Arabic_Literature|0": {
      "acc_norm": 0.7310344827586207,
      "acc_norm_stderr": 0.036951833116502325
    },
    "community|acva:Arabic_Math|0": {
      "acc_norm": 0.3230769230769231,
      "acc_norm_stderr": 0.03357544396403133
    },
    "community|acva:Arabic_Medicine|0": {
      "acc_norm": 0.4896551724137931,
      "acc_norm_stderr": 0.04165774775728763
    },
    "community|acva:Arabic_Music|0": {
      "acc_norm": 0.28776978417266186,
      "acc_norm_stderr": 0.038538361792333886
    },
    "community|acva:Arabic_Ornament|0": {
      "acc_norm": 0.6307692307692307,
      "acc_norm_stderr": 0.034648411418637566
    },
    "community|acva:Arabic_Philosophy|0": {
      "acc_norm": 0.593103448275862,
      "acc_norm_stderr": 0.04093793981266236
    },
    "community|acva:Arabic_Physics_and_Chemistry|0": {
      "acc_norm": 0.764102564102564,
      "acc_norm_stderr": 0.03048151676172155
    },
    "community|acva:Arabic_Wedding|0": {
      "acc_norm": 0.517948717948718,
      "acc_norm_stderr": 0.03587477098773825
    },
    "community|acva:Bahrain|0": {
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.0752101433090355
    },
    "community|acva:Comoros|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.07385489458759965
    },
    "community|acva:Egypt_modern|0": {
      "acc_norm": 0.5894736842105263,
      "acc_norm_stderr": 0.05073863564551209
    },
    "community|acva:InfluenceFromAncientEgypt|0": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.035172622905632896
    },
    "community|acva:InfluenceFromByzantium|0": {
      "acc_norm": 0.7172413793103448,
      "acc_norm_stderr": 0.03752833958003337
    },
    "community|acva:InfluenceFromChina|0": {
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.0317493043641267
    },
    "community|acva:InfluenceFromGreece|0": {
      "acc_norm": 0.6358974358974359,
      "acc_norm_stderr": 0.03454653867786389
    },
    "community|acva:InfluenceFromIslam|0": {
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.03878352372138621
    },
    "community|acva:InfluenceFromPersia|0": {
      "acc_norm": 0.6971428571428572,
      "acc_norm_stderr": 0.03483414676585986
    },
    "community|acva:InfluenceFromRome|0": {
      "acc_norm": 0.5846153846153846,
      "acc_norm_stderr": 0.03538013280575029
    },
    "community|acva:Iraq|0": {
      "acc_norm": 0.5529411764705883,
      "acc_norm_stderr": 0.054247803536170265
    },
    "community|acva:Islam_Education|0": {
      "acc_norm": 0.6153846153846154,
      "acc_norm_stderr": 0.03492896993742303
    },
    "community|acva:Islam_branches_and_schools|0": {
      "acc_norm": 0.5828571428571429,
      "acc_norm_stderr": 0.03738082164536353
    },
    "community|acva:Islamic_law_system|0": {
      "acc_norm": 0.6102564102564103,
      "acc_norm_stderr": 0.035014247762563705
    },
    "community|acva:Jordan|0": {
      "acc_norm": 0.5111111111111111,
      "acc_norm_stderr": 0.07535922203472523
    },
    "community|acva:Kuwait|0": {
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.07216392363431011
    },
    "community|acva:Lebanon|0": {
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.06979205927323111
    },
    "community|acva:Libya|0": {
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.07491109582924915
    },
    "community|acva:Mauritania|0": {
      "acc_norm": 0.5111111111111111,
      "acc_norm_stderr": 0.07535922203472523
    },
    "community|acva:Mesopotamia_civilization|0": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.03947710169758611
    },
    "community|acva:Morocco|0": {
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Oman|0": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Palestine|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.05345224838248487
    },
    "community|acva:Qatar|0": {
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.07535922203472523
    },
    "community|acva:Saudi_Arabia|0": {
      "acc_norm": 0.4256410256410256,
      "acc_norm_stderr": 0.03549871080367707
    },
    "community|acva:Somalia|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.07385489458759965
    },
    "community|acva:Sudan|0": {
      "acc_norm": 0.5111111111111111,
      "acc_norm_stderr": 0.07535922203472523
    },
    "community|acva:Syria|0": {
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.07491109582924914
    },
    "community|acva:Tunisia|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.07385489458759964
    },
    "community|acva:United_Arab_Emirates|0": {
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.049714956160500964
    },
    "community|acva:Yemen|0": {
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.16666666666666666
    },
    "community|acva:communication|0": {
      "acc_norm": 0.5274725274725275,
      "acc_norm_stderr": 0.026203550546661278
    },
    "community|acva:computer_and_phone|0": {
      "acc_norm": 0.46779661016949153,
      "acc_norm_stderr": 0.029100046852442867
    },
    "community|acva:daily_life|0": {
      "acc_norm": 0.21068249258160238,
      "acc_norm_stderr": 0.022246948687324377
    },
    "community|acva:entertainment|0": {
      "acc_norm": 0.2983050847457627,
      "acc_norm_stderr": 0.02668276477279597
    },
    "community|alghafa:mcq_exams_test_ar|0": {
      "acc_norm": 0.3770197486535009,
      "acc_norm_stderr": 0.020553311001569808
    },
    "community|alghafa:meta_ar_dialects|0": {
      "acc_norm": 0.4031510658016682,
      "acc_norm_stderr": 0.006678987742224282
    },
    "community|alghafa:meta_ar_msa|0": {
      "acc_norm": 0.4402234636871508,
      "acc_norm_stderr": 0.01660256461504994
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": {
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.05807730170189531
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": {
      "acc_norm": 0.64,
      "acc_norm_stderr": 0.03932313218491396
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": {
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.040830308521485996
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": {
      "acc_norm": 0.818511569731082,
      "acc_norm_stderr": 0.004310769600006414
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|0": {
      "acc_norm": 0.5641367806505421,
      "acc_norm_stderr": 0.00640484980273765
    },
    "community|alghafa:multiple_choice_sentiment_task|0": {
      "acc_norm": 0.39244186046511625,
      "acc_norm_stderr": 0.011777243283058179
    },
    "community|arabic_exams|0": {
      "acc_norm": 0.5065176908752328,
      "acc_norm_stderr": 0.02159487569233193
    },
    "community|arabic_mmlu:abstract_algebra|0": {
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "community|arabic_mmlu:anatomy|0": {
      "acc_norm": 0.4740740740740741,
      "acc_norm_stderr": 0.04313531696750574
    },
    "community|arabic_mmlu:astronomy|0": {
      "acc_norm": 0.6710526315789473,
      "acc_norm_stderr": 0.03823428969926603
    },
    "community|arabic_mmlu:business_ethics|0": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.04923659639173309
    },
    "community|arabic_mmlu:clinical_knowledge|0": {
      "acc_norm": 0.5811320754716981,
      "acc_norm_stderr": 0.03036505082911521
    },
    "community|arabic_mmlu:college_biology|0": {
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.04076663253918567
    },
    "community|arabic_mmlu:college_chemistry|0": {
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "community|arabic_mmlu:college_computer_science|0": {
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "community|arabic_mmlu:college_mathematics|0": {
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939098
    },
    "community|arabic_mmlu:college_medicine|0": {
      "acc_norm": 0.4624277456647399,
      "acc_norm_stderr": 0.0380168510452446
    },
    "community|arabic_mmlu:college_physics|0": {
      "acc_norm": 0.4411764705882353,
      "acc_norm_stderr": 0.049406356306056595
    },
    "community|arabic_mmlu:computer_security|0": {
      "acc_norm": 0.61,
      "acc_norm_stderr": 0.04902071300001975
    },
    "community|arabic_mmlu:conceptual_physics|0": {
      "acc_norm": 0.5872340425531914,
      "acc_norm_stderr": 0.03218471141400351
    },
    "community|arabic_mmlu:econometrics|0": {
      "acc_norm": 0.45614035087719296,
      "acc_norm_stderr": 0.046854730419077895
    },
    "community|arabic_mmlu:electrical_engineering|0": {
      "acc_norm": 0.5448275862068965,
      "acc_norm_stderr": 0.04149886942192117
    },
    "community|arabic_mmlu:elementary_mathematics|0": {
      "acc_norm": 0.5264550264550265,
      "acc_norm_stderr": 0.02571523981134675
    },
    "community|arabic_mmlu:formal_logic|0": {
      "acc_norm": 0.4603174603174603,
      "acc_norm_stderr": 0.04458029125470973
    },
    "community|arabic_mmlu:global_facts|0": {
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "community|arabic_mmlu:high_school_biology|0": {
      "acc_norm": 0.7,
      "acc_norm_stderr": 0.026069362295335137
    },
    "community|arabic_mmlu:high_school_chemistry|0": {
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.034819048444388045
    },
    "community|arabic_mmlu:high_school_computer_science|0": {
      "acc_norm": 0.65,
      "acc_norm_stderr": 0.047937248544110196
    },
    "community|arabic_mmlu:high_school_european_history|0": {
      "acc_norm": 0.24242424242424243,
      "acc_norm_stderr": 0.03346409881055953
    },
    "community|arabic_mmlu:high_school_geography|0": {
      "acc_norm": 0.6767676767676768,
      "acc_norm_stderr": 0.03332299921070644
    },
    "community|arabic_mmlu:high_school_government_and_politics|0": {
      "acc_norm": 0.6476683937823834,
      "acc_norm_stderr": 0.03447478286414357
    },
    "community|arabic_mmlu:high_school_macroeconomics|0": {
      "acc_norm": 0.676923076923077,
      "acc_norm_stderr": 0.02371088850197056
    },
    "community|arabic_mmlu:high_school_mathematics|0": {
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.029443169323031537
    },
    "community|arabic_mmlu:high_school_microeconomics|0": {
      "acc_norm": 0.6008403361344538,
      "acc_norm_stderr": 0.03181110032413925
    },
    "community|arabic_mmlu:high_school_physics|0": {
      "acc_norm": 0.3576158940397351,
      "acc_norm_stderr": 0.03913453431177258
    },
    "community|arabic_mmlu:high_school_psychology|0": {
      "acc_norm": 0.6330275229357798,
      "acc_norm_stderr": 0.02066467565952053
    },
    "community|arabic_mmlu:high_school_statistics|0": {
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.03398110890294636
    },
    "community|arabic_mmlu:high_school_us_history|0": {
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.032282103870378935
    },
    "community|arabic_mmlu:high_school_world_history|0": {
      "acc_norm": 0.34177215189873417,
      "acc_norm_stderr": 0.030874537537553617
    },
    "community|arabic_mmlu:human_aging|0": {
      "acc_norm": 0.6053811659192825,
      "acc_norm_stderr": 0.03280400504755291
    },
    "community|arabic_mmlu:human_sexuality|0": {
      "acc_norm": 0.549618320610687,
      "acc_norm_stderr": 0.04363643698524779
    },
    "community|arabic_mmlu:international_law|0": {
      "acc_norm": 0.7355371900826446,
      "acc_norm_stderr": 0.04026187527591205
    },
    "community|arabic_mmlu:jurisprudence|0": {
      "acc_norm": 0.5833333333333334,
      "acc_norm_stderr": 0.04766075165356461
    },
    "community|arabic_mmlu:logical_fallacies|0": {
      "acc_norm": 0.5521472392638037,
      "acc_norm_stderr": 0.03906947479456606
    },
    "community|arabic_mmlu:machine_learning|0": {
      "acc_norm": 0.3392857142857143,
      "acc_norm_stderr": 0.04493949068613539
    },
    "community|arabic_mmlu:management|0": {
      "acc_norm": 0.7087378640776699,
      "acc_norm_stderr": 0.044986763205729224
    },
    "community|arabic_mmlu:marketing|0": {
      "acc_norm": 0.8034188034188035,
      "acc_norm_stderr": 0.02603538609895129
    },
    "community|arabic_mmlu:medical_genetics|0": {
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.049604496374885836
    },
    "community|arabic_mmlu:miscellaneous|0": {
      "acc_norm": 0.6462324393358876,
      "acc_norm_stderr": 0.017098184708161906
    },
    "community|arabic_mmlu:moral_disputes|0": {
      "acc_norm": 0.6416184971098265,
      "acc_norm_stderr": 0.025816756791584204
    },
    "community|arabic_mmlu:moral_scenarios|0": {
      "acc_norm": 0.2670391061452514,
      "acc_norm_stderr": 0.014796502622562551
    },
    "community|arabic_mmlu:nutrition|0": {
      "acc_norm": 0.6274509803921569,
      "acc_norm_stderr": 0.027684181883302895
    },
    "community|arabic_mmlu:philosophy|0": {
      "acc_norm": 0.6109324758842444,
      "acc_norm_stderr": 0.027690337536485372
    },
    "community|arabic_mmlu:prehistory|0": {
      "acc_norm": 0.5709876543209876,
      "acc_norm_stderr": 0.027538925613470863
    },
    "community|arabic_mmlu:professional_accounting|0": {
      "acc_norm": 0.4148936170212766,
      "acc_norm_stderr": 0.0293922365846125
    },
    "community|arabic_mmlu:professional_law|0": {
      "acc_norm": 0.34159061277705344,
      "acc_norm_stderr": 0.012112391320842858
    },
    "community|arabic_mmlu:professional_medicine|0": {
      "acc_norm": 0.2867647058823529,
      "acc_norm_stderr": 0.027472274473233818
    },
    "community|arabic_mmlu:professional_psychology|0": {
      "acc_norm": 0.5228758169934641,
      "acc_norm_stderr": 0.02020665318788479
    },
    "community|arabic_mmlu:public_relations|0": {
      "acc_norm": 0.6454545454545455,
      "acc_norm_stderr": 0.045820048415054174
    },
    "community|arabic_mmlu:security_studies|0": {
      "acc_norm": 0.6530612244897959,
      "acc_norm_stderr": 0.030472526026726496
    },
    "community|arabic_mmlu:sociology|0": {
      "acc_norm": 0.7164179104477612,
      "acc_norm_stderr": 0.03187187537919795
    },
    "community|arabic_mmlu:us_foreign_policy|0": {
      "acc_norm": 0.77,
      "acc_norm_stderr": 0.04229525846816506
    },
    "community|arabic_mmlu:virology|0": {
      "acc_norm": 0.39759036144578314,
      "acc_norm_stderr": 0.038099730845402184
    },
    "community|arabic_mmlu:world_religions|0": {
      "acc_norm": 0.631578947368421,
      "acc_norm_stderr": 0.036996580176568775
    },
    "community|arc_challenge_okapi_ar|0": {
      "acc_norm": 0.55,
      "acc_norm_stderr": 0.014613218690048174
    },
    "community|arc_easy_ar|0": {
      "acc_norm": 0.5245346869712352,
      "acc_norm_stderr": 0.010273411053090993
    },
    "community|boolq_ar|0": {
      "acc_norm": 0.839877300613497,
      "acc_norm_stderr": 0.006423802692591652
    },
    "community|copa_ext_ar|0": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.05192907868894985
    },
    "community|hellaswag_okapi_ar|0": {
      "acc_norm": 0.3233017119180024,
      "acc_norm_stderr": 0.00488446355154371
    },
    "community|mmlu_okapi_ar|0": {
      "acc_norm": 0.3909308983982048,
      "acc_norm_stderr": 0.004292580164967273
    },
    "community|openbook_qa_ext_ar|0": {
      "acc_norm": 0.503030303030303,
      "acc_norm_stderr": 0.02249565037865304
    },
    "community|piqa_ar|0": {
      "acc_norm": 0.7054009819967266,
      "acc_norm_stderr": 0.010650523199799256
    },
    "community|race_ar|0": {
      "acc_norm": 0.4554676404950294,
      "acc_norm_stderr": 0.007094229937665132
    },
    "community|sciq_ar|0": {
      "acc_norm": 0.5105527638190954,
      "acc_norm_stderr": 0.015855504448096237
    },
    "community|toxigen_ar|0": {
      "acc_norm": 0.8235294117647058,
      "acc_norm_stderr": 0.012473900792792059
    },
    "lighteval|xstory_cloze:ar|0": {
      "acc": 0.6677696889477167,
      "acc_stderr": 0.012121168923544583
    },
    "community|acva:_average|0": {
      "acc_norm": 0.48976914713710473,
      "acc_norm_stderr": 0.04766759596907139
    },
    "community|alghafa:_average|0": {
      "acc_norm": 0.5217204987765622,
      "acc_norm_stderr": 0.022728718716993505
    },
    "community|arabic_mmlu:_average|0": {
      "acc_norm": 0.5333156182390374,
      "acc_norm_stderr": 0.035951811281511675
    },
    "all": {
      "acc_norm": 0.5164281518562376,
      "acc_norm_stderr": 0.03824348174261601,
      "acc": 0.6677696889477167,
      "acc_stderr": 0.012121168923544583
    }
  },
  "versions": {
    "community|acva:Algeria|0": 0,
    "community|acva:Ancient_Egypt|0": 0,
    "community|acva:Arab_Empire|0": 0,
    "community|acva:Arabic_Architecture|0": 0,
    "community|acva:Arabic_Art|0": 0,
    "community|acva:Arabic_Astronomy|0": 0,
    "community|acva:Arabic_Calligraphy|0": 0,
    "community|acva:Arabic_Ceremony|0": 0,
    "community|acva:Arabic_Clothing|0": 0,
    "community|acva:Arabic_Culture|0": 0,
    "community|acva:Arabic_Food|0": 0,
    "community|acva:Arabic_Funeral|0": 0,
    "community|acva:Arabic_Geography|0": 0,
    "community|acva:Arabic_History|0": 0,
    "community|acva:Arabic_Language_Origin|0": 0,
    "community|acva:Arabic_Literature|0": 0,
    "community|acva:Arabic_Math|0": 0,
    "community|acva:Arabic_Medicine|0": 0,
    "community|acva:Arabic_Music|0": 0,
    "community|acva:Arabic_Ornament|0": 0,
    "community|acva:Arabic_Philosophy|0": 0,
    "community|acva:Arabic_Physics_and_Chemistry|0": 0,
    "community|acva:Arabic_Wedding|0": 0,
    "community|acva:Bahrain|0": 0,
    "community|acva:Comoros|0": 0,
    "community|acva:Egypt_modern|0": 0,
    "community|acva:InfluenceFromAncientEgypt|0": 0,
    "community|acva:InfluenceFromByzantium|0": 0,
    "community|acva:InfluenceFromChina|0": 0,
    "community|acva:InfluenceFromGreece|0": 0,
    "community|acva:InfluenceFromIslam|0": 0,
    "community|acva:InfluenceFromPersia|0": 0,
    "community|acva:InfluenceFromRome|0": 0,
    "community|acva:Iraq|0": 0,
    "community|acva:Islam_Education|0": 0,
    "community|acva:Islam_branches_and_schools|0": 0,
    "community|acva:Islamic_law_system|0": 0,
    "community|acva:Jordan|0": 0,
    "community|acva:Kuwait|0": 0,
    "community|acva:Lebanon|0": 0,
    "community|acva:Libya|0": 0,
    "community|acva:Mauritania|0": 0,
    "community|acva:Mesopotamia_civilization|0": 0,
    "community|acva:Morocco|0": 0,
    "community|acva:Oman|0": 0,
    "community|acva:Palestine|0": 0,
    "community|acva:Qatar|0": 0,
    "community|acva:Saudi_Arabia|0": 0,
    "community|acva:Somalia|0": 0,
    "community|acva:Sudan|0": 0,
    "community|acva:Syria|0": 0,
    "community|acva:Tunisia|0": 0,
    "community|acva:United_Arab_Emirates|0": 0,
    "community|acva:Yemen|0": 0,
    "community|acva:communication|0": 0,
    "community|acva:computer_and_phone|0": 0,
    "community|acva:daily_life|0": 0,
    "community|acva:entertainment|0": 0,
    "community|alghafa:mcq_exams_test_ar|0": 0,
    "community|alghafa:meta_ar_dialects|0": 0,
    "community|alghafa:meta_ar_msa|0": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|0": 0,
    "community|alghafa:multiple_choice_sentiment_task|0": 0,
    "community|arabic_exams|0": 0,
    "community|arabic_mmlu:abstract_algebra|0": 0,
    "community|arabic_mmlu:anatomy|0": 0,
    "community|arabic_mmlu:astronomy|0": 0,
    "community|arabic_mmlu:business_ethics|0": 0,
    "community|arabic_mmlu:clinical_knowledge|0": 0,
    "community|arabic_mmlu:college_biology|0": 0,
    "community|arabic_mmlu:college_chemistry|0": 0,
    "community|arabic_mmlu:college_computer_science|0": 0,
    "community|arabic_mmlu:college_mathematics|0": 0,
    "community|arabic_mmlu:college_medicine|0": 0,
    "community|arabic_mmlu:college_physics|0": 0,
    "community|arabic_mmlu:computer_security|0": 0,
    "community|arabic_mmlu:conceptual_physics|0": 0,
    "community|arabic_mmlu:econometrics|0": 0,
    "community|arabic_mmlu:electrical_engineering|0": 0,
    "community|arabic_mmlu:elementary_mathematics|0": 0,
    "community|arabic_mmlu:formal_logic|0": 0,
    "community|arabic_mmlu:global_facts|0": 0,
    "community|arabic_mmlu:high_school_biology|0": 0,
    "community|arabic_mmlu:high_school_chemistry|0": 0,
    "community|arabic_mmlu:high_school_computer_science|0": 0,
    "community|arabic_mmlu:high_school_european_history|0": 0,
    "community|arabic_mmlu:high_school_geography|0": 0,
    "community|arabic_mmlu:high_school_government_and_politics|0": 0,
    "community|arabic_mmlu:high_school_macroeconomics|0": 0,
    "community|arabic_mmlu:high_school_mathematics|0": 0,
    "community|arabic_mmlu:high_school_microeconomics|0": 0,
    "community|arabic_mmlu:high_school_physics|0": 0,
    "community|arabic_mmlu:high_school_psychology|0": 0,
    "community|arabic_mmlu:high_school_statistics|0": 0,
    "community|arabic_mmlu:high_school_us_history|0": 0,
    "community|arabic_mmlu:high_school_world_history|0": 0,
    "community|arabic_mmlu:human_aging|0": 0,
    "community|arabic_mmlu:human_sexuality|0": 0,
    "community|arabic_mmlu:international_law|0": 0,
    "community|arabic_mmlu:jurisprudence|0": 0,
    "community|arabic_mmlu:logical_fallacies|0": 0,
    "community|arabic_mmlu:machine_learning|0": 0,
    "community|arabic_mmlu:management|0": 0,
    "community|arabic_mmlu:marketing|0": 0,
    "community|arabic_mmlu:medical_genetics|0": 0,
    "community|arabic_mmlu:miscellaneous|0": 0,
    "community|arabic_mmlu:moral_disputes|0": 0,
    "community|arabic_mmlu:moral_scenarios|0": 0,
    "community|arabic_mmlu:nutrition|0": 0,
    "community|arabic_mmlu:philosophy|0": 0,
    "community|arabic_mmlu:prehistory|0": 0,
    "community|arabic_mmlu:professional_accounting|0": 0,
    "community|arabic_mmlu:professional_law|0": 0,
    "community|arabic_mmlu:professional_medicine|0": 0,
    "community|arabic_mmlu:professional_psychology|0": 0,
    "community|arabic_mmlu:public_relations|0": 0,
    "community|arabic_mmlu:security_studies|0": 0,
    "community|arabic_mmlu:sociology|0": 0,
    "community|arabic_mmlu:us_foreign_policy|0": 0,
    "community|arabic_mmlu:virology|0": 0,
    "community|arabic_mmlu:world_religions|0": 0,
    "community|arc_challenge_okapi_ar|0": 0,
    "community|arc_easy_ar|0": 0,
    "community|boolq_ar|0": 0,
    "community|copa_ext_ar|0": 0,
    "community|hellaswag_okapi_ar|0": 0,
    "community|mmlu_okapi_ar|0": 0,
    "community|openbook_qa_ext_ar|0": 0,
    "community|piqa_ar|0": 0,
    "community|race_ar|0": 0,
    "community|sciq_ar|0": 0,
    "community|toxigen_ar|0": 0,
    "lighteval|xstory_cloze:ar|0": 0
  },
  "config_tasks": {
    "community|acva:Algeria": {
      "name": "acva:Algeria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Algeria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Ancient_Egypt": {
      "name": "acva:Ancient_Egypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Ancient_Egypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 315,
      "effective_num_docs": 315,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arab_Empire": {
      "name": "acva:Arab_Empire",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arab_Empire",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Architecture": {
      "name": "acva:Arabic_Architecture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Architecture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Art": {
      "name": "acva:Arabic_Art",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Art",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Astronomy": {
      "name": "acva:Arabic_Astronomy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Calligraphy": {
      "name": "acva:Arabic_Calligraphy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Calligraphy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 255,
      "effective_num_docs": 255,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ceremony": {
      "name": "acva:Arabic_Ceremony",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ceremony",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 185,
      "effective_num_docs": 185,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Clothing": {
      "name": "acva:Arabic_Clothing",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Clothing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Culture": {
      "name": "acva:Arabic_Culture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Culture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Food": {
      "name": "acva:Arabic_Food",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Food",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Funeral": {
      "name": "acva:Arabic_Funeral",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Funeral",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Geography": {
      "name": "acva:Arabic_Geography",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_History": {
      "name": "acva:Arabic_History",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_History",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Language_Origin": {
      "name": "acva:Arabic_Language_Origin",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Language_Origin",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Literature": {
      "name": "acva:Arabic_Literature",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Literature",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Math": {
      "name": "acva:Arabic_Math",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Math",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Medicine": {
      "name": "acva:Arabic_Medicine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Music": {
      "name": "acva:Arabic_Music",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Music",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 139,
      "effective_num_docs": 139,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ornament": {
      "name": "acva:Arabic_Ornament",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ornament",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Philosophy": {
      "name": "acva:Arabic_Philosophy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry": {
      "name": "acva:Arabic_Physics_and_Chemistry",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Physics_and_Chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Wedding": {
      "name": "acva:Arabic_Wedding",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Wedding",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Bahrain": {
      "name": "acva:Bahrain",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Bahrain",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Comoros": {
      "name": "acva:Comoros",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Comoros",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Egypt_modern": {
      "name": "acva:Egypt_modern",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Egypt_modern",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromAncientEgypt": {
      "name": "acva:InfluenceFromAncientEgypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromAncientEgypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromByzantium": {
      "name": "acva:InfluenceFromByzantium",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromByzantium",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromChina": {
      "name": "acva:InfluenceFromChina",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromChina",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromGreece": {
      "name": "acva:InfluenceFromGreece",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromGreece",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromIslam": {
      "name": "acva:InfluenceFromIslam",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromIslam",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromPersia": {
      "name": "acva:InfluenceFromPersia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromPersia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromRome": {
      "name": "acva:InfluenceFromRome",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromRome",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Iraq": {
      "name": "acva:Iraq",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Iraq",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_Education": {
      "name": "acva:Islam_Education",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_Education",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_branches_and_schools": {
      "name": "acva:Islam_branches_and_schools",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_branches_and_schools",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islamic_law_system": {
      "name": "acva:Islamic_law_system",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islamic_law_system",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Jordan": {
      "name": "acva:Jordan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Jordan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Kuwait": {
      "name": "acva:Kuwait",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Kuwait",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Lebanon": {
      "name": "acva:Lebanon",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Lebanon",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Libya": {
      "name": "acva:Libya",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Libya",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mauritania": {
      "name": "acva:Mauritania",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mauritania",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mesopotamia_civilization": {
      "name": "acva:Mesopotamia_civilization",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mesopotamia_civilization",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 155,
      "effective_num_docs": 155,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Morocco": {
      "name": "acva:Morocco",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Morocco",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Oman": {
      "name": "acva:Oman",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Oman",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Palestine": {
      "name": "acva:Palestine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Palestine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Qatar": {
      "name": "acva:Qatar",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Qatar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Saudi_Arabia": {
      "name": "acva:Saudi_Arabia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Saudi_Arabia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Somalia": {
      "name": "acva:Somalia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Somalia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Sudan": {
      "name": "acva:Sudan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Sudan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Syria": {
      "name": "acva:Syria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Syria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Tunisia": {
      "name": "acva:Tunisia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Tunisia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:United_Arab_Emirates": {
      "name": "acva:United_Arab_Emirates",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "United_Arab_Emirates",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Yemen": {
      "name": "acva:Yemen",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Yemen",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 10,
      "effective_num_docs": 10,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:communication": {
      "name": "acva:communication",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "communication",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 364,
      "effective_num_docs": 364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:computer_and_phone": {
      "name": "acva:computer_and_phone",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "computer_and_phone",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:daily_life": {
      "name": "acva:daily_life",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "daily_life",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 337,
      "effective_num_docs": 337,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:entertainment": {
      "name": "acva:entertainment",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "entertainment",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_exams": {
      "name": "arabic_exams",
      "prompt_function": "arabic_exams",
      "hf_repo": "OALL/Arabic_EXAMS",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 537,
      "effective_num_docs": 537,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:abstract_algebra": {
      "name": "arabic_mmlu:abstract_algebra",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:anatomy": {
      "name": "arabic_mmlu:anatomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:astronomy": {
      "name": "arabic_mmlu:astronomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:business_ethics": {
      "name": "arabic_mmlu:business_ethics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:clinical_knowledge": {
      "name": "arabic_mmlu:clinical_knowledge",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_biology": {
      "name": "arabic_mmlu:college_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_chemistry": {
      "name": "arabic_mmlu:college_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_computer_science": {
      "name": "arabic_mmlu:college_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_mathematics": {
      "name": "arabic_mmlu:college_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_medicine": {
      "name": "arabic_mmlu:college_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_physics": {
      "name": "arabic_mmlu:college_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:computer_security": {
      "name": "arabic_mmlu:computer_security",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:conceptual_physics": {
      "name": "arabic_mmlu:conceptual_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:econometrics": {
      "name": "arabic_mmlu:econometrics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:electrical_engineering": {
      "name": "arabic_mmlu:electrical_engineering",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:elementary_mathematics": {
      "name": "arabic_mmlu:elementary_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:formal_logic": {
      "name": "arabic_mmlu:formal_logic",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:global_facts": {
      "name": "arabic_mmlu:global_facts",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_biology": {
      "name": "arabic_mmlu:high_school_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_chemistry": {
      "name": "arabic_mmlu:high_school_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_computer_science": {
      "name": "arabic_mmlu:high_school_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_european_history": {
      "name": "arabic_mmlu:high_school_european_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_geography": {
      "name": "arabic_mmlu:high_school_geography",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics": {
      "name": "arabic_mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics": {
      "name": "arabic_mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_mathematics": {
      "name": "arabic_mmlu:high_school_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_microeconomics": {
      "name": "arabic_mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_physics": {
      "name": "arabic_mmlu:high_school_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_psychology": {
      "name": "arabic_mmlu:high_school_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_statistics": {
      "name": "arabic_mmlu:high_school_statistics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_us_history": {
      "name": "arabic_mmlu:high_school_us_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_world_history": {
      "name": "arabic_mmlu:high_school_world_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_aging": {
      "name": "arabic_mmlu:human_aging",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_sexuality": {
      "name": "arabic_mmlu:human_sexuality",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:international_law": {
      "name": "arabic_mmlu:international_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:jurisprudence": {
      "name": "arabic_mmlu:jurisprudence",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:logical_fallacies": {
      "name": "arabic_mmlu:logical_fallacies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:machine_learning": {
      "name": "arabic_mmlu:machine_learning",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:management": {
      "name": "arabic_mmlu:management",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:marketing": {
      "name": "arabic_mmlu:marketing",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:medical_genetics": {
      "name": "arabic_mmlu:medical_genetics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:miscellaneous": {
      "name": "arabic_mmlu:miscellaneous",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_disputes": {
      "name": "arabic_mmlu:moral_disputes",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_scenarios": {
      "name": "arabic_mmlu:moral_scenarios",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:nutrition": {
      "name": "arabic_mmlu:nutrition",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:philosophy": {
      "name": "arabic_mmlu:philosophy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:prehistory": {
      "name": "arabic_mmlu:prehistory",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_accounting": {
      "name": "arabic_mmlu:professional_accounting",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_law": {
      "name": "arabic_mmlu:professional_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_medicine": {
      "name": "arabic_mmlu:professional_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_psychology": {
      "name": "arabic_mmlu:professional_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:public_relations": {
      "name": "arabic_mmlu:public_relations",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:security_studies": {
      "name": "arabic_mmlu:security_studies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:sociology": {
      "name": "arabic_mmlu:sociology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:us_foreign_policy": {
      "name": "arabic_mmlu:us_foreign_policy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:virology": {
      "name": "arabic_mmlu:virology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:world_religions": {
      "name": "arabic_mmlu:world_religions",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_challenge_okapi_ar": {
      "name": "arc_challenge_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_challenge_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1160,
      "effective_num_docs": 1160,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_easy_ar": {
      "name": "arc_easy_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_easy_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 2364,
      "effective_num_docs": 2364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|copa_ext_ar": {
      "name": "copa_ext_ar",
      "prompt_function": "copa_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "copa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 90,
      "effective_num_docs": 90,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|mmlu_okapi_ar": {
      "name": "mmlu_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "mmlu_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 12923,
      "effective_num_docs": 12923,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|openbook_qa_ext_ar": {
      "name": "openbook_qa_ext_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "openbook_qa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 495,
      "effective_num_docs": 495,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|piqa_ar": {
      "name": "piqa_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "piqa_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1833,
      "effective_num_docs": 1833,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|sciq_ar": {
      "name": "sciq_ar",
      "prompt_function": "sciq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "sciq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 995,
      "effective_num_docs": 995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|toxigen_ar": {
      "name": "toxigen_ar",
      "prompt_function": "toxigen_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "toxigen_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 935,
      "effective_num_docs": 935,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "lighteval|xstory_cloze:ar": {
      "name": "xstory_cloze:ar",
      "prompt_function": "storycloze",
      "hf_repo": "juletxara/xstory_cloze",
      "hf_subset": "ar",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "training",
        "eval"
      ],
      "evaluation_splits": [
        "eval"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "lighteval"
      ],
      "original_num_docs": 1511,
      "effective_num_docs": 1511,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|acva:Algeria|0": {
      "hashes": {
        "hash_examples": "da5a3003cd46f6f9",
        "hash_full_prompts": "da5a3003cd46f6f9",
        "hash_input_tokens": "0ef2353005963a09",
        "hash_cont_tokens": "b68a341a8fadaf30"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Ancient_Egypt|0": {
      "hashes": {
        "hash_examples": "52d6f767fede195b",
        "hash_full_prompts": "52d6f767fede195b",
        "hash_input_tokens": "63b1f8d27bb8b538",
        "hash_cont_tokens": "d77f707f3c622392"
      },
      "truncated": 0,
      "non_truncated": 315,
      "padded": 630,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arab_Empire|0": {
      "hashes": {
        "hash_examples": "8dacff6a79804a75",
        "hash_full_prompts": "8dacff6a79804a75",
        "hash_input_tokens": "96ba55d15ed1709c",
        "hash_cont_tokens": "082f4286bf69c9c0"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 530,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Architecture|0": {
      "hashes": {
        "hash_examples": "df286cd862d9f6bb",
        "hash_full_prompts": "df286cd862d9f6bb",
        "hash_input_tokens": "8ac8f5c4e64259f8",
        "hash_cont_tokens": "2e1b11ce9013e3f3"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Art|0": {
      "hashes": {
        "hash_examples": "112883d764118a49",
        "hash_full_prompts": "112883d764118a49",
        "hash_input_tokens": "a407902ad391406c",
        "hash_cont_tokens": "4d260cda414e7f25"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Astronomy|0": {
      "hashes": {
        "hash_examples": "20dcdf2454bf8671",
        "hash_full_prompts": "20dcdf2454bf8671",
        "hash_input_tokens": "77edb77dae6439b3",
        "hash_cont_tokens": "e86d5477d089b7b0"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Calligraphy|0": {
      "hashes": {
        "hash_examples": "3a9f9d1ebe868a15",
        "hash_full_prompts": "3a9f9d1ebe868a15",
        "hash_input_tokens": "f4cc49e65aef44c8",
        "hash_cont_tokens": "b7c67826433a4ce2"
      },
      "truncated": 0,
      "non_truncated": 255,
      "padded": 510,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ceremony|0": {
      "hashes": {
        "hash_examples": "c927630f8d2f44da",
        "hash_full_prompts": "c927630f8d2f44da",
        "hash_input_tokens": "4a64161f6b520ced",
        "hash_cont_tokens": "451d23167310eacb"
      },
      "truncated": 0,
      "non_truncated": 185,
      "padded": 370,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Clothing|0": {
      "hashes": {
        "hash_examples": "6ad0740c2ac6ac92",
        "hash_full_prompts": "6ad0740c2ac6ac92",
        "hash_input_tokens": "9e0f611a0aaa8266",
        "hash_cont_tokens": "712a87eeea162dfa"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Culture|0": {
      "hashes": {
        "hash_examples": "2177bd857ad872ae",
        "hash_full_prompts": "2177bd857ad872ae",
        "hash_input_tokens": "7bedac54a59ad71e",
        "hash_cont_tokens": "02613a8d26afc282"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Food|0": {
      "hashes": {
        "hash_examples": "a6ada65b71d7c9c5",
        "hash_full_prompts": "a6ada65b71d7c9c5",
        "hash_input_tokens": "55bb77829db93e45",
        "hash_cont_tokens": "712a87eeea162dfa"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Funeral|0": {
      "hashes": {
        "hash_examples": "fcee39dc29eaae91",
        "hash_full_prompts": "fcee39dc29eaae91",
        "hash_input_tokens": "0ae0a99470d9f8f0",
        "hash_cont_tokens": "3a9666539a674320"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Geography|0": {
      "hashes": {
        "hash_examples": "d36eda7c89231c02",
        "hash_full_prompts": "d36eda7c89231c02",
        "hash_input_tokens": "27376dde25062551",
        "hash_cont_tokens": "b9a10fac3c80020d"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_History|0": {
      "hashes": {
        "hash_examples": "6354ac0d6db6a5fc",
        "hash_full_prompts": "6354ac0d6db6a5fc",
        "hash_input_tokens": "156178c6257fa4ca",
        "hash_cont_tokens": "dd62586c13eb3a2d"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Language_Origin|0": {
      "hashes": {
        "hash_examples": "ddc967c8aca34402",
        "hash_full_prompts": "ddc967c8aca34402",
        "hash_input_tokens": "11a6cb048980bb65",
        "hash_cont_tokens": "8e095f138ca290ac"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Literature|0": {
      "hashes": {
        "hash_examples": "4305379fd46be5d8",
        "hash_full_prompts": "4305379fd46be5d8",
        "hash_input_tokens": "28da9fbfff70f50f",
        "hash_cont_tokens": "0b1feb622802c907"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Math|0": {
      "hashes": {
        "hash_examples": "dec621144f4d28be",
        "hash_full_prompts": "dec621144f4d28be",
        "hash_input_tokens": "aa01d25ce8710522",
        "hash_cont_tokens": "36a92ff952c013ae"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Medicine|0": {
      "hashes": {
        "hash_examples": "2b344cdae9495ff2",
        "hash_full_prompts": "2b344cdae9495ff2",
        "hash_input_tokens": "1c666e45d50e38bd",
        "hash_cont_tokens": "251be022df4edfc7"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Music|0": {
      "hashes": {
        "hash_examples": "0c54624d881944ce",
        "hash_full_prompts": "0c54624d881944ce",
        "hash_input_tokens": "9dd70f562ec8609d",
        "hash_cont_tokens": "17ecf48d38efbecd"
      },
      "truncated": 0,
      "non_truncated": 139,
      "padded": 278,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ornament|0": {
      "hashes": {
        "hash_examples": "251a4a84289d8bc1",
        "hash_full_prompts": "251a4a84289d8bc1",
        "hash_input_tokens": "593b1c41a865d6c8",
        "hash_cont_tokens": "dd62586c13eb3a2d"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Philosophy|0": {
      "hashes": {
        "hash_examples": "3f86fb9c94c13d22",
        "hash_full_prompts": "3f86fb9c94c13d22",
        "hash_input_tokens": "67f579ef12ad6188",
        "hash_cont_tokens": "14641dc83450841f"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry|0": {
      "hashes": {
        "hash_examples": "8fec65af3695b62a",
        "hash_full_prompts": "8fec65af3695b62a",
        "hash_input_tokens": "fe0c160a8956f385",
        "hash_cont_tokens": "e76c4368e098f4ba"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Wedding|0": {
      "hashes": {
        "hash_examples": "9cc3477184d7a4b8",
        "hash_full_prompts": "9cc3477184d7a4b8",
        "hash_input_tokens": "703591d63f1c67ff",
        "hash_cont_tokens": "59780a91ee917dc8"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Bahrain|0": {
      "hashes": {
        "hash_examples": "c92e803a0fa8b9e2",
        "hash_full_prompts": "c92e803a0fa8b9e2",
        "hash_input_tokens": "8e82b45e3894ceb2",
        "hash_cont_tokens": "b504dfa0db7122d2"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Comoros|0": {
      "hashes": {
        "hash_examples": "06e5d4bba8e54cae",
        "hash_full_prompts": "06e5d4bba8e54cae",
        "hash_input_tokens": "52874da922eacc75",
        "hash_cont_tokens": "fec39e25d7899207"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Egypt_modern|0": {
      "hashes": {
        "hash_examples": "c6ec369164f93446",
        "hash_full_prompts": "c6ec369164f93446",
        "hash_input_tokens": "ca757f63e5596372",
        "hash_cont_tokens": "9ac8efec980a3fbc"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromAncientEgypt|0": {
      "hashes": {
        "hash_examples": "b9d56d74818b9bd4",
        "hash_full_prompts": "b9d56d74818b9bd4",
        "hash_input_tokens": "502bfe37ebc95e7c",
        "hash_cont_tokens": "b30fc8b372f234c9"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromByzantium|0": {
      "hashes": {
        "hash_examples": "5316c9624e7e59b8",
        "hash_full_prompts": "5316c9624e7e59b8",
        "hash_input_tokens": "0ac8ec2196326511",
        "hash_cont_tokens": "6524718b18404765"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromChina|0": {
      "hashes": {
        "hash_examples": "87894bce95a56411",
        "hash_full_prompts": "87894bce95a56411",
        "hash_input_tokens": "afa4e076f64bed25",
        "hash_cont_tokens": "02613a8d26afc282"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromGreece|0": {
      "hashes": {
        "hash_examples": "0baa78a27e469312",
        "hash_full_prompts": "0baa78a27e469312",
        "hash_input_tokens": "40452ef1892bd108",
        "hash_cont_tokens": "74e4f508a073bdf4"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromIslam|0": {
      "hashes": {
        "hash_examples": "0c2532cde6541ff2",
        "hash_full_prompts": "0c2532cde6541ff2",
        "hash_input_tokens": "b037214327d306c0",
        "hash_cont_tokens": "22d2ea5a8b7fc666"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromPersia|0": {
      "hashes": {
        "hash_examples": "efcd8112dc53c6e5",
        "hash_full_prompts": "efcd8112dc53c6e5",
        "hash_input_tokens": "447ec5f08413f00d",
        "hash_cont_tokens": "68fd778a4eba65b6"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromRome|0": {
      "hashes": {
        "hash_examples": "9db61480e2e85fd3",
        "hash_full_prompts": "9db61480e2e85fd3",
        "hash_input_tokens": "867d73461ad0c8e2",
        "hash_cont_tokens": "44de9d2ba222ab11"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Iraq|0": {
      "hashes": {
        "hash_examples": "96dac3dfa8d2f41f",
        "hash_full_prompts": "96dac3dfa8d2f41f",
        "hash_input_tokens": "beebc31d1f945465",
        "hash_cont_tokens": "57ebe2249a622a55"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_Education|0": {
      "hashes": {
        "hash_examples": "0d80355f6a4cb51b",
        "hash_full_prompts": "0d80355f6a4cb51b",
        "hash_input_tokens": "077431c6ea00b6d8",
        "hash_cont_tokens": "b68a341a8fadaf30"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_branches_and_schools|0": {
      "hashes": {
        "hash_examples": "5cedce1be2c3ad50",
        "hash_full_prompts": "5cedce1be2c3ad50",
        "hash_input_tokens": "4084d9ce336c60d4",
        "hash_cont_tokens": "58b4bed22eb58497"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islamic_law_system|0": {
      "hashes": {
        "hash_examples": "c0e6db8bc84e105e",
        "hash_full_prompts": "c0e6db8bc84e105e",
        "hash_input_tokens": "a3b8d7ba97b6353c",
        "hash_cont_tokens": "02613a8d26afc282"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Jordan|0": {
      "hashes": {
        "hash_examples": "33deb5b4e5ddd6a1",
        "hash_full_prompts": "33deb5b4e5ddd6a1",
        "hash_input_tokens": "18736ae0373dd755",
        "hash_cont_tokens": "5b1bf44b31e933e3"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Kuwait|0": {
      "hashes": {
        "hash_examples": "eb41773346d7c46c",
        "hash_full_prompts": "eb41773346d7c46c",
        "hash_input_tokens": "52f58f11e679881e",
        "hash_cont_tokens": "71ac776def0cf3c6"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Lebanon|0": {
      "hashes": {
        "hash_examples": "25932dbf4c13d34f",
        "hash_full_prompts": "25932dbf4c13d34f",
        "hash_input_tokens": "ca37b47aa6eb78ab",
        "hash_cont_tokens": "26b2c111609cb69b"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Libya|0": {
      "hashes": {
        "hash_examples": "f2c4db63cd402926",
        "hash_full_prompts": "f2c4db63cd402926",
        "hash_input_tokens": "c0bf7524355eab5c",
        "hash_cont_tokens": "b504dfa0db7122d2"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mauritania|0": {
      "hashes": {
        "hash_examples": "8723ab5fdf286b54",
        "hash_full_prompts": "8723ab5fdf286b54",
        "hash_input_tokens": "4799cca5e2d5eddc",
        "hash_cont_tokens": "59bf29d90ce8d477"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mesopotamia_civilization|0": {
      "hashes": {
        "hash_examples": "c33f5502a6130ca9",
        "hash_full_prompts": "c33f5502a6130ca9",
        "hash_input_tokens": "75e1f717313fdb6a",
        "hash_cont_tokens": "0c6758dfae8e41e7"
      },
      "truncated": 0,
      "non_truncated": 155,
      "padded": 310,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Morocco|0": {
      "hashes": {
        "hash_examples": "588a5ed27904b1ae",
        "hash_full_prompts": "588a5ed27904b1ae",
        "hash_input_tokens": "724d6985dcf8e9a8",
        "hash_cont_tokens": "b504dfa0db7122d2"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Oman|0": {
      "hashes": {
        "hash_examples": "d447c52b94248b69",
        "hash_full_prompts": "d447c52b94248b69",
        "hash_input_tokens": "100a3f0ff9ae215b",
        "hash_cont_tokens": "fec39e25d7899207"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Palestine|0": {
      "hashes": {
        "hash_examples": "19197e076ad14ff5",
        "hash_full_prompts": "19197e076ad14ff5",
        "hash_input_tokens": "5cdfeb0208d3e624",
        "hash_cont_tokens": "a885946e06ea4499"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Qatar|0": {
      "hashes": {
        "hash_examples": "cf0736fa185b28f6",
        "hash_full_prompts": "cf0736fa185b28f6",
        "hash_input_tokens": "acfb9cf282bd2d93",
        "hash_cont_tokens": "37065307e9603135"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Saudi_Arabia|0": {
      "hashes": {
        "hash_examples": "69beda6e1b85a08d",
        "hash_full_prompts": "69beda6e1b85a08d",
        "hash_input_tokens": "5c07fb4180c42e87",
        "hash_cont_tokens": "ce991e989027e650"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Somalia|0": {
      "hashes": {
        "hash_examples": "b387940c65784fbf",
        "hash_full_prompts": "b387940c65784fbf",
        "hash_input_tokens": "bdca6f64477a079b",
        "hash_cont_tokens": "9704b852247e1964"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Sudan|0": {
      "hashes": {
        "hash_examples": "e02c32b9d2dd0c3f",
        "hash_full_prompts": "e02c32b9d2dd0c3f",
        "hash_input_tokens": "1fdb7fe6c26b1c4f",
        "hash_cont_tokens": "b7d1e698bb8c1f38"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Syria|0": {
      "hashes": {
        "hash_examples": "60a6f8fe73bda4bb",
        "hash_full_prompts": "60a6f8fe73bda4bb",
        "hash_input_tokens": "310be20c505aab88",
        "hash_cont_tokens": "b504dfa0db7122d2"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Tunisia|0": {
      "hashes": {
        "hash_examples": "34bb15d3830c5649",
        "hash_full_prompts": "34bb15d3830c5649",
        "hash_input_tokens": "1ad10e71d59ab2ca",
        "hash_cont_tokens": "49e8e24c808690e8"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:United_Arab_Emirates|0": {
      "hashes": {
        "hash_examples": "98a0ba78172718ce",
        "hash_full_prompts": "98a0ba78172718ce",
        "hash_input_tokens": "995d3940b2fbf2b8",
        "hash_cont_tokens": "4971c0c062e422a3"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Yemen|0": {
      "hashes": {
        "hash_examples": "18e9bcccbb4ced7a",
        "hash_full_prompts": "18e9bcccbb4ced7a",
        "hash_input_tokens": "d83b9c2923549d82",
        "hash_cont_tokens": "a7ad0a859c72c336"
      },
      "truncated": 0,
      "non_truncated": 10,
      "padded": 20,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:communication|0": {
      "hashes": {
        "hash_examples": "9ff28ab5eab5c97b",
        "hash_full_prompts": "9ff28ab5eab5c97b",
        "hash_input_tokens": "538d32b8e791c484",
        "hash_cont_tokens": "a1a0daa61a61d041"
      },
      "truncated": 0,
      "non_truncated": 364,
      "padded": 728,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:computer_and_phone|0": {
      "hashes": {
        "hash_examples": "37bac2f086aaf6c2",
        "hash_full_prompts": "37bac2f086aaf6c2",
        "hash_input_tokens": "1d1817b262b08b31",
        "hash_cont_tokens": "c3992b2adaf1ea68"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:daily_life|0": {
      "hashes": {
        "hash_examples": "bf07363c1c252e2f",
        "hash_full_prompts": "bf07363c1c252e2f",
        "hash_input_tokens": "89eae1708f1eb67e",
        "hash_cont_tokens": "fd483b4e85feeac4"
      },
      "truncated": 0,
      "non_truncated": 337,
      "padded": 674,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:entertainment|0": {
      "hashes": {
        "hash_examples": "37077bc00f0ac56a",
        "hash_full_prompts": "37077bc00f0ac56a",
        "hash_input_tokens": "0d144bfe01b9d52b",
        "hash_cont_tokens": "a5624a2ad3513dec"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:mcq_exams_test_ar|0": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "c07a5e78c5c0b8fe",
        "hash_input_tokens": "d6e3bab35134c770",
        "hash_cont_tokens": "d1cba040bbf4f18d"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2228,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|0": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "c0b6081f83e14064",
        "hash_input_tokens": "492eb37646ab3050",
        "hash_cont_tokens": "b1cf20e1b6e52156"
      },
      "truncated": 0,
      "non_truncated": 5395,
      "padded": 21580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_msa|0": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "64eb78a7c5b7484b",
        "hash_input_tokens": "d78798bd7c685789",
        "hash_cont_tokens": "4bb427682aa2b3d4"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "54fc3502c1c02c06",
        "hash_input_tokens": "6b255127b4b579fd",
        "hash_cont_tokens": "4812fa919e61803f"
      },
      "truncated": 0,
      "non_truncated": 75,
      "padded": 150,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "46572d83696552ae",
        "hash_input_tokens": "e24356c6261a8827",
        "hash_cont_tokens": "dc49923be19493ae"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "f430d97ff715bc1c",
        "hash_input_tokens": "5d292faf2fb831eb",
        "hash_cont_tokens": "76e2e5e36df3bf74"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "6b70a7416584f98c",
        "hash_input_tokens": "24870024681a6188",
        "hash_cont_tokens": "a8d28dd22fd46377"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|0": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "bc2005cc9d2f436e",
        "hash_input_tokens": "0e284e9d56c353b6",
        "hash_cont_tokens": "25b4128c2f310dab"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17985,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|0": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "6fb0e254ea5945d8",
        "hash_input_tokens": "589f56cc96c3d6a3",
        "hash_cont_tokens": "d5cd812873e58d9e"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5160,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_exams|0": {
      "hashes": {
        "hash_examples": "6d721df351722656",
        "hash_full_prompts": "6d721df351722656",
        "hash_input_tokens": "df09e6af933eb694",
        "hash_cont_tokens": "cf8017e10d9506b3"
      },
      "truncated": 0,
      "non_truncated": 537,
      "padded": 2148,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "f2ddca8f45c0a511",
        "hash_full_prompts": "f2ddca8f45c0a511",
        "hash_input_tokens": "a8af3086fa74e284",
        "hash_cont_tokens": "f109f1c2b4a9c655"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:anatomy|0": {
      "hashes": {
        "hash_examples": "dfdbc1b83107668d",
        "hash_full_prompts": "dfdbc1b83107668d",
        "hash_input_tokens": "45f3065f28181ec4",
        "hash_cont_tokens": "a3cae18b240a4651"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:astronomy|0": {
      "hashes": {
        "hash_examples": "9736a606002a848e",
        "hash_full_prompts": "9736a606002a848e",
        "hash_input_tokens": "f097044535d529c9",
        "hash_cont_tokens": "a2c54ff0a639d385"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:business_ethics|0": {
      "hashes": {
        "hash_examples": "735e452fbb6dc63d",
        "hash_full_prompts": "735e452fbb6dc63d",
        "hash_input_tokens": "210b452dc5d97e49",
        "hash_cont_tokens": "a7dca420e867e271"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "6ab0ca4da98aedcf",
        "hash_full_prompts": "6ab0ca4da98aedcf",
        "hash_input_tokens": "8c5e603ecfd8b596",
        "hash_cont_tokens": "f8ae562a342a882d"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_biology|0": {
      "hashes": {
        "hash_examples": "17e4e390848018a4",
        "hash_full_prompts": "17e4e390848018a4",
        "hash_input_tokens": "7e34d4ce5a52931a",
        "hash_cont_tokens": "197cc174a15dedf6"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_chemistry|0": {
      "hashes": {
        "hash_examples": "4abb169f6dfd234b",
        "hash_full_prompts": "4abb169f6dfd234b",
        "hash_input_tokens": "4af9d8a5195b5d4b",
        "hash_cont_tokens": "4c005b9903880149"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_computer_science|0": {
      "hashes": {
        "hash_examples": "a369e2e941358a1e",
        "hash_full_prompts": "a369e2e941358a1e",
        "hash_input_tokens": "368d876ad62dd86f",
        "hash_cont_tokens": "e86e06e5ca64a6d3"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_mathematics|0": {
      "hashes": {
        "hash_examples": "d7be03b8b6020bff",
        "hash_full_prompts": "d7be03b8b6020bff",
        "hash_input_tokens": "a352533cd8f39a0d",
        "hash_cont_tokens": "40ee6cbe4ea6269a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_medicine|0": {
      "hashes": {
        "hash_examples": "0518a00f097346bf",
        "hash_full_prompts": "0518a00f097346bf",
        "hash_input_tokens": "60b45ce6951886dc",
        "hash_cont_tokens": "9f46c4d9cef78f89"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_physics|0": {
      "hashes": {
        "hash_examples": "5d842cd49bc70e12",
        "hash_full_prompts": "5d842cd49bc70e12",
        "hash_input_tokens": "0b7401d56db010c2",
        "hash_cont_tokens": "e74e146e81e55b5b"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:computer_security|0": {
      "hashes": {
        "hash_examples": "8e85d9f85be9b32f",
        "hash_full_prompts": "8e85d9f85be9b32f",
        "hash_input_tokens": "b807996ef2823457",
        "hash_cont_tokens": "1195321124e7aba4"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "7964b55a0a49502b",
        "hash_full_prompts": "7964b55a0a49502b",
        "hash_input_tokens": "0f50e9a2f53f348b",
        "hash_cont_tokens": "eebb2b44e72c9e7f"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:econometrics|0": {
      "hashes": {
        "hash_examples": "1e192eae38347257",
        "hash_full_prompts": "1e192eae38347257",
        "hash_input_tokens": "cc91a3212e77af25",
        "hash_cont_tokens": "ec24f4ae2c06a9bd"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "cf97671d5c441da1",
        "hash_full_prompts": "cf97671d5c441da1",
        "hash_input_tokens": "f9594db0a23b850a",
        "hash_cont_tokens": "18b7821722149786"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "6f49107ed43c40c5",
        "hash_full_prompts": "6f49107ed43c40c5",
        "hash_input_tokens": "582888a3988b5a8d",
        "hash_cont_tokens": "2cc7579b840f95d4"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:formal_logic|0": {
      "hashes": {
        "hash_examples": "7922c376008ba77b",
        "hash_full_prompts": "7922c376008ba77b",
        "hash_input_tokens": "8b2df92e34554985",
        "hash_cont_tokens": "cc223f0fff6e3eda"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:global_facts|0": {
      "hashes": {
        "hash_examples": "11f9813185047d5b",
        "hash_full_prompts": "11f9813185047d5b",
        "hash_input_tokens": "cdec47a406ee28af",
        "hash_cont_tokens": "1195321124e7aba4"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_biology|0": {
      "hashes": {
        "hash_examples": "2a804b1d90cbe66e",
        "hash_full_prompts": "2a804b1d90cbe66e",
        "hash_input_tokens": "efc8eaff6f3b9832",
        "hash_cont_tokens": "f82daf4e53c26fde"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "0032168adabc53b4",
        "hash_full_prompts": "0032168adabc53b4",
        "hash_input_tokens": "9db0c143c3936480",
        "hash_cont_tokens": "eb07aed5023e8665"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "f2fb8740f9df980f",
        "hash_full_prompts": "f2fb8740f9df980f",
        "hash_input_tokens": "e95387a81f088d45",
        "hash_cont_tokens": "f0503b36d69db59a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "73509021e7e66435",
        "hash_full_prompts": "73509021e7e66435",
        "hash_input_tokens": "ef0593f69aafac5e",
        "hash_cont_tokens": "d7934f284a055ecf"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_geography|0": {
      "hashes": {
        "hash_examples": "9e08d1894940ff42",
        "hash_full_prompts": "9e08d1894940ff42",
        "hash_input_tokens": "65dcbc752b42594d",
        "hash_cont_tokens": "aaafb99f6dde2b0c"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "64b7e97817ca6c76",
        "hash_full_prompts": "64b7e97817ca6c76",
        "hash_input_tokens": "fb09bb237eff8800",
        "hash_cont_tokens": "f7dd0838eac5d187"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "9f582da8534bd2ef",
        "hash_full_prompts": "9f582da8534bd2ef",
        "hash_input_tokens": "7a544483e56b2a0d",
        "hash_cont_tokens": "874e29756f6c2776"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "fd54f1c10d423c51",
        "hash_full_prompts": "fd54f1c10d423c51",
        "hash_input_tokens": "0c53c1901a5db971",
        "hash_cont_tokens": "9272bb182e52d280"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "7037896925aaf42f",
        "hash_full_prompts": "7037896925aaf42f",
        "hash_input_tokens": "0b6bfdb4205b7345",
        "hash_cont_tokens": "2113063ee56466db"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_physics|0": {
      "hashes": {
        "hash_examples": "60c3776215167dae",
        "hash_full_prompts": "60c3776215167dae",
        "hash_input_tokens": "9911fc444a3277ba",
        "hash_cont_tokens": "d2c1ad8e71d4c02c"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "61176bfd5da1298f",
        "hash_full_prompts": "61176bfd5da1298f",
        "hash_input_tokens": "b52d3a50598920de",
        "hash_cont_tokens": "1fced82d5ea1c0e2"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "40dfeebd1ea10f76",
        "hash_full_prompts": "40dfeebd1ea10f76",
        "hash_input_tokens": "2cab069a5b6ca970",
        "hash_cont_tokens": "239e12487596009b"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "03daa510ba917f4d",
        "hash_full_prompts": "03daa510ba917f4d",
        "hash_input_tokens": "67c22c63874f292c",
        "hash_cont_tokens": "f74080afa7de2cc8"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "be075ffd579f43c2",
        "hash_full_prompts": "be075ffd579f43c2",
        "hash_input_tokens": "54306ddf95d61983",
        "hash_cont_tokens": "8cdf1228c8fdc86b"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_aging|0": {
      "hashes": {
        "hash_examples": "caa5b69f640bd1ef",
        "hash_full_prompts": "caa5b69f640bd1ef",
        "hash_input_tokens": "60021e813541ee10",
        "hash_cont_tokens": "ee52bd33861197cd"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_sexuality|0": {
      "hashes": {
        "hash_examples": "5ed2e38fb25a3767",
        "hash_full_prompts": "5ed2e38fb25a3767",
        "hash_input_tokens": "2480cc7593f5a595",
        "hash_cont_tokens": "219a54da8fbb609c"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 524,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:international_law|0": {
      "hashes": {
        "hash_examples": "4e3e9e28d1b96484",
        "hash_full_prompts": "4e3e9e28d1b96484",
        "hash_input_tokens": "aad905acb79bf4a8",
        "hash_cont_tokens": "cd7c89cafc5f98dc"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:jurisprudence|0": {
      "hashes": {
        "hash_examples": "e264b755366310b3",
        "hash_full_prompts": "e264b755366310b3",
        "hash_input_tokens": "0bbe60c25aa60e47",
        "hash_cont_tokens": "c7282d6fa017a2e9"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 425,
      "non_padded": 7,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "a4ab6965a3e38071",
        "hash_full_prompts": "a4ab6965a3e38071",
        "hash_input_tokens": "b04e515bb858f23b",
        "hash_cont_tokens": "f59f09f36b9518f8"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:machine_learning|0": {
      "hashes": {
        "hash_examples": "b92320efa6636b40",
        "hash_full_prompts": "b92320efa6636b40",
        "hash_input_tokens": "2e76387faf1fa737",
        "hash_cont_tokens": "06e5ed7500e1dd2f"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 432,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:management|0": {
      "hashes": {
        "hash_examples": "c9ee4872a850fe20",
        "hash_full_prompts": "c9ee4872a850fe20",
        "hash_input_tokens": "04096f6fa838014a",
        "hash_cont_tokens": "d28dc26f48437609"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 408,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:marketing|0": {
      "hashes": {
        "hash_examples": "0c151b70f6a047e3",
        "hash_full_prompts": "0c151b70f6a047e3",
        "hash_input_tokens": "e6691ecf5cc4167a",
        "hash_cont_tokens": "d60b4f5359e5071d"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 920,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:medical_genetics|0": {
      "hashes": {
        "hash_examples": "513f6cb8fca3a24e",
        "hash_full_prompts": "513f6cb8fca3a24e",
        "hash_input_tokens": "27780f5a45fb7546",
        "hash_cont_tokens": "1195321124e7aba4"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:miscellaneous|0": {
      "hashes": {
        "hash_examples": "259a190d635331db",
        "hash_full_prompts": "259a190d635331db",
        "hash_input_tokens": "fe8cc88f69150514",
        "hash_cont_tokens": "541c5b14ad15d52e"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3132,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_disputes|0": {
      "hashes": {
        "hash_examples": "b85052c48a0b7bc3",
        "hash_full_prompts": "b85052c48a0b7bc3",
        "hash_input_tokens": "8f332f35b7430447",
        "hash_cont_tokens": "7401410a2784b188"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1376,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "28d0b069ef00dd00",
        "hash_full_prompts": "28d0b069ef00dd00",
        "hash_input_tokens": "8aa94b4756313334",
        "hash_cont_tokens": "df8ed645eef4f648"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:nutrition|0": {
      "hashes": {
        "hash_examples": "00c9bc5f1d305b2f",
        "hash_full_prompts": "00c9bc5f1d305b2f",
        "hash_input_tokens": "af48c06e967ddf18",
        "hash_cont_tokens": "2756b6bd154b56a9"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1196,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:philosophy|0": {
      "hashes": {
        "hash_examples": "a458c08454a3fd5f",
        "hash_full_prompts": "a458c08454a3fd5f",
        "hash_input_tokens": "dfdec12244cf79eb",
        "hash_cont_tokens": "44ef72fe8c9476b7"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1240,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:prehistory|0": {
      "hashes": {
        "hash_examples": "d6a0ecbdbb670e9c",
        "hash_full_prompts": "d6a0ecbdbb670e9c",
        "hash_input_tokens": "b6e03c3f46e468ae",
        "hash_cont_tokens": "a527d81300006800"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1288,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_accounting|0": {
      "hashes": {
        "hash_examples": "b4a95fe480b6540e",
        "hash_full_prompts": "b4a95fe480b6540e",
        "hash_input_tokens": "d45d7265b54e442a",
        "hash_cont_tokens": "1178ce80910e64f3"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1124,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_law|0": {
      "hashes": {
        "hash_examples": "c2be9651cdbdde3b",
        "hash_full_prompts": "c2be9651cdbdde3b",
        "hash_input_tokens": "fec4c9377a433227",
        "hash_cont_tokens": "9b5aad5715d0418f"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6128,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_medicine|0": {
      "hashes": {
        "hash_examples": "26ce92416288f273",
        "hash_full_prompts": "26ce92416288f273",
        "hash_input_tokens": "b619a38b7f046da4",
        "hash_cont_tokens": "02be4c53e81bc198"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_psychology|0": {
      "hashes": {
        "hash_examples": "71ea5f182ea9a641",
        "hash_full_prompts": "71ea5f182ea9a641",
        "hash_input_tokens": "bf3486d161b81d6e",
        "hash_cont_tokens": "be01786da85ca947"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2416,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:public_relations|0": {
      "hashes": {
        "hash_examples": "125adc21f91f8d77",
        "hash_full_prompts": "125adc21f91f8d77",
        "hash_input_tokens": "9379aae25f047bf0",
        "hash_cont_tokens": "70f38c7ea1c2d2e5"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 436,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:security_studies|0": {
      "hashes": {
        "hash_examples": "3c18b216c099fb26",
        "hash_full_prompts": "3c18b216c099fb26",
        "hash_input_tokens": "522202aadb46ad19",
        "hash_cont_tokens": "7c006355c2f73c9a"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:sociology|0": {
      "hashes": {
        "hash_examples": "3f2a9634cef7417d",
        "hash_full_prompts": "3f2a9634cef7417d",
        "hash_input_tokens": "39ac32c44199270d",
        "hash_cont_tokens": "6cc232059f050931"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 784,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "22249da54056475e",
        "hash_full_prompts": "22249da54056475e",
        "hash_input_tokens": "ed841fe021d9755e",
        "hash_cont_tokens": "68b66bc6664622bb"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 392,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:virology|0": {
      "hashes": {
        "hash_examples": "9d194b9471dc624e",
        "hash_full_prompts": "9d194b9471dc624e",
        "hash_input_tokens": "562d857c27d71ee0",
        "hash_cont_tokens": "efe1a15552b2a035"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 664,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:world_religions|0": {
      "hashes": {
        "hash_examples": "229e5fe50082b064",
        "hash_full_prompts": "229e5fe50082b064",
        "hash_input_tokens": "c776778b16108f67",
        "hash_cont_tokens": "b44025ab89f68b0d"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_challenge_okapi_ar|0": {
      "hashes": {
        "hash_examples": "ab893807673bc355",
        "hash_full_prompts": "ab893807673bc355",
        "hash_input_tokens": "832cbf1d78c4f667",
        "hash_cont_tokens": "bb91a34363d0341e"
      },
      "truncated": 0,
      "non_truncated": 1160,
      "padded": 4598,
      "non_padded": 42,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_easy_ar|0": {
      "hashes": {
        "hash_examples": "acb688624acc3d04",
        "hash_full_prompts": "acb688624acc3d04",
        "hash_input_tokens": "a4dc4d1527b6b69a",
        "hash_cont_tokens": "47728cf2cf02d9d8"
      },
      "truncated": 0,
      "non_truncated": 2364,
      "padded": 9407,
      "non_padded": 49,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|boolq_ar|0": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "48355a67867e0c32",
        "hash_input_tokens": "19631a52ed6ac2a3",
        "hash_cont_tokens": "071e9dddfbfcd91b"
      },
      "truncated": 0,
      "non_truncated": 3260,
      "padded": 6479,
      "non_padded": 41,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|copa_ext_ar|0": {
      "hashes": {
        "hash_examples": "9bb83301bb72eecf",
        "hash_full_prompts": "9bb83301bb72eecf",
        "hash_input_tokens": "c807b7c8ab918b16",
        "hash_cont_tokens": "91f181163a875b73"
      },
      "truncated": 0,
      "non_truncated": 90,
      "padded": 180,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|0": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "6e8cf57a322dfadd",
        "hash_input_tokens": "903be6ea78490819",
        "hash_cont_tokens": "dc648be387347605"
      },
      "truncated": 0,
      "non_truncated": 9171,
      "padded": 36598,
      "non_padded": 86,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|mmlu_okapi_ar|0": {
      "hashes": {
        "hash_examples": "723b5371ad6fdc31",
        "hash_full_prompts": "723b5371ad6fdc31",
        "hash_input_tokens": "fbd6bf014b042ea8",
        "hash_cont_tokens": "7ca386d0e5cba2d6"
      },
      "truncated": 0,
      "non_truncated": 12923,
      "padded": 51063,
      "non_padded": 629,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|openbook_qa_ext_ar|0": {
      "hashes": {
        "hash_examples": "923d41eb0aca93eb",
        "hash_full_prompts": "923d41eb0aca93eb",
        "hash_input_tokens": "5281a8dac689a813",
        "hash_cont_tokens": "e9fa2105267d1700"
      },
      "truncated": 0,
      "non_truncated": 495,
      "padded": 1948,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|piqa_ar|0": {
      "hashes": {
        "hash_examples": "94bc205a520d3ea0",
        "hash_full_prompts": "94bc205a520d3ea0",
        "hash_input_tokens": "5af0495af392c57c",
        "hash_cont_tokens": "225602a9c0fddcb8"
      },
      "truncated": 0,
      "non_truncated": 1833,
      "padded": 3604,
      "non_padded": 62,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|race_ar|0": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "de65130bae647516",
        "hash_input_tokens": "257f992164b57c42",
        "hash_cont_tokens": "986d1509a19c08e0"
      },
      "truncated": 0,
      "non_truncated": 4929,
      "padded": 19692,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|sciq_ar|0": {
      "hashes": {
        "hash_examples": "4f4da53c2c859ac8",
        "hash_full_prompts": "4f4da53c2c859ac8",
        "hash_input_tokens": "5328c28bd3e07c07",
        "hash_cont_tokens": "a215b6c514b9c0cc"
      },
      "truncated": 0,
      "non_truncated": 995,
      "padded": 3951,
      "non_padded": 29,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|toxigen_ar|0": {
      "hashes": {
        "hash_examples": "1e139513004a9a2e",
        "hash_full_prompts": "1e139513004a9a2e",
        "hash_input_tokens": "16ca0cee87dcea28",
        "hash_cont_tokens": "d38f3ef7bf0c0d3c"
      },
      "truncated": 0,
      "non_truncated": 935,
      "padded": 1843,
      "non_padded": 27,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "lighteval|xstory_cloze:ar|0": {
      "hashes": {
        "hash_examples": "865426a22c787481",
        "hash_full_prompts": "865426a22c787481",
        "hash_input_tokens": "99b8ba1d9fad8264",
        "hash_cont_tokens": "cc3bc8fad0f4775f"
      },
      "truncated": 0,
      "non_truncated": 1511,
      "padded": 2978,
      "non_padded": 44,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "f3f05fc95b7ed68f",
      "hash_full_prompts": "f3f05fc95b7ed68f",
      "hash_input_tokens": "527174dbcc2a6370",
      "hash_cont_tokens": "c0ed68e52bac58e5"
    },
    "truncated": 0,
    "non_truncated": 85887,
    "padded": 286083,
    "non_padded": 1232,
    "num_truncated_few_shots": 0
  }
}