{
  "config_general": {
    "lighteval_sha": "1bcde543f10ace8ab221df11e8989b527674a774",
    "num_fewshot_seeds": 1,
    "override_batch_size": 4,
    "max_samples": null,
    "job_id": "",
    "start_time": 75335.273158783,
    "end_time": 78183.566628998,
    "total_evaluation_time_secondes": "2848.293470214994",
    "model_name": "meta-llama/Meta-Llama-3-8B-Instruct",
    "model_sha": "e1945c40cd546c78e41f1151f4db032b271faeaa",
    "model_dtype": "torch.bfloat16",
    "model_size": "14.96 GB",
    "config": null
  },
  "results": {
    "leaderboard|arc:challenge|0": {
      "acc": 0.5349829351535836,
      "acc_stderr": 0.014575583922019675,
      "acc_norm": 0.5588737201365188,
      "acc_norm_stderr": 0.014509747749064663
    },
    "leaderboard|hellaswag|0": {
      "acc": 0.5794662417845051,
      "acc_stderr": 0.004926358564494566,
      "acc_norm": 0.7552280422226648,
      "acc_norm_stderr": 0.00429073211466202
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276
    },
    "leaderboard|mmlu:anatomy|5": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.04072314811876837
    },
    "leaderboard|mmlu:astronomy|5": {
      "acc": 0.7236842105263158,
      "acc_stderr": 0.03639057569952929
    },
    "leaderboard|mmlu:business_ethics|5": {
      "acc": 0.7,
      "acc_stderr": 0.046056618647183814
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "acc": 0.7547169811320755,
      "acc_stderr": 0.026480357179895702
    },
    "leaderboard|mmlu:college_biology|5": {
      "acc": 0.8055555555555556,
      "acc_stderr": 0.03309615177059006
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956912
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "acc": 0.59,
      "acc_stderr": 0.04943110704237102
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "acc": 0.41,
      "acc_stderr": 0.049431107042371025
    },
    "leaderboard|mmlu:college_medicine|5": {
      "acc": 0.630057803468208,
      "acc_stderr": 0.0368122963339432
    },
    "leaderboard|mmlu:college_physics|5": {
      "acc": 0.4803921568627451,
      "acc_stderr": 0.04971358884367406
    },
    "leaderboard|mmlu:computer_security|5": {
      "acc": 0.76,
      "acc_stderr": 0.04292346959909281
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "acc": 0.6170212765957447,
      "acc_stderr": 0.03177821250236922
    },
    "leaderboard|mmlu:econometrics|5": {
      "acc": 0.6052631578947368,
      "acc_stderr": 0.045981880578165414
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "acc": 0.6275862068965518,
      "acc_stderr": 0.04028731532947559
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "acc": 0.4417989417989418,
      "acc_stderr": 0.025576257061253833
    },
    "leaderboard|mmlu:formal_logic|5": {
      "acc": 0.5238095238095238,
      "acc_stderr": 0.04467062628403273
    },
    "leaderboard|mmlu:global_facts|5": {
      "acc": 0.43,
      "acc_stderr": 0.049756985195624284
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "acc": 0.7967741935483871,
      "acc_stderr": 0.02289168798455494
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "acc": 0.5123152709359606,
      "acc_stderr": 0.035169204442208966
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "acc": 0.78,
      "acc_stderr": 0.04163331998932262
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "acc": 0.7575757575757576,
      "acc_stderr": 0.03346409881055953
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "acc": 0.8383838383838383,
      "acc_stderr": 0.02622591986362927
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "acc": 0.9222797927461139,
      "acc_stderr": 0.01932180555722315
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.023901157979402544
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "acc": 0.37777777777777777,
      "acc_stderr": 0.029560707392465715
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "acc": 0.7773109243697479,
      "acc_stderr": 0.027025433498882392
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "acc": 0.4304635761589404,
      "acc_stderr": 0.040428099613956346
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "acc": 0.8495412844036697,
      "acc_stderr": 0.015328563932669235
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.034076320938540516
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "acc": 0.8529411764705882,
      "acc_stderr": 0.024857478080250437
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "acc": 0.8565400843881856,
      "acc_stderr": 0.02281829182101701
    },
    "leaderboard|mmlu:human_aging|5": {
      "acc": 0.7309417040358744,
      "acc_stderr": 0.029763779406874965
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "acc": 0.816793893129771,
      "acc_stderr": 0.03392770926494733
    },
    "leaderboard|mmlu:international_law|5": {
      "acc": 0.8264462809917356,
      "acc_stderr": 0.03457272836917669
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "acc": 0.7592592592592593,
      "acc_stderr": 0.04133119440243839
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "acc": 0.7484662576687117,
      "acc_stderr": 0.03408997886857529
    },
    "leaderboard|mmlu:machine_learning|5": {
      "acc": 0.5357142857142857,
      "acc_stderr": 0.04733667890053756
    },
    "leaderboard|mmlu:management|5": {
      "acc": 0.8349514563106796,
      "acc_stderr": 0.036756688322331886
    },
    "leaderboard|mmlu:marketing|5": {
      "acc": 0.9102564102564102,
      "acc_stderr": 0.018724301741941635
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "acc": 0.82,
      "acc_stderr": 0.038612291966536955
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "acc": 0.8378033205619413,
      "acc_stderr": 0.013182222616720883
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "acc": 0.7283236994219653,
      "acc_stderr": 0.02394851290546836
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "acc": 0.464804469273743,
      "acc_stderr": 0.01668102093107667
    },
    "leaderboard|mmlu:nutrition|5": {
      "acc": 0.7450980392156863,
      "acc_stderr": 0.0249541843248799
    },
    "leaderboard|mmlu:philosophy|5": {
      "acc": 0.7202572347266881,
      "acc_stderr": 0.02549425935069491
    },
    "leaderboard|mmlu:prehistory|5": {
      "acc": 0.7561728395061729,
      "acc_stderr": 0.023891879541959603
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "acc": 0.5390070921985816,
      "acc_stderr": 0.02973659252642444
    },
    "leaderboard|mmlu:professional_law|5": {
      "acc": 0.48565840938722293,
      "acc_stderr": 0.012764981829524267
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "acc": 0.7426470588235294,
      "acc_stderr": 0.02655651947004151
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "acc": 0.7156862745098039,
      "acc_stderr": 0.018249024411207664
    },
    "leaderboard|mmlu:public_relations|5": {
      "acc": 0.6727272727272727,
      "acc_stderr": 0.04494290866252091
    },
    "leaderboard|mmlu:security_studies|5": {
      "acc": 0.746938775510204,
      "acc_stderr": 0.027833023871399677
    },
    "leaderboard|mmlu:sociology|5": {
      "acc": 0.8507462686567164,
      "acc_stderr": 0.02519692987482708
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "acc": 0.85,
      "acc_stderr": 0.0358870281282637
    },
    "leaderboard|mmlu:virology|5": {
      "acc": 0.5301204819277109,
      "acc_stderr": 0.03885425420866767
    },
    "leaderboard|mmlu:world_religions|5": {
      "acc": 0.8187134502923976,
      "acc_stderr": 0.029547741687640038
    },
    "leaderboard|mmlu:_average|5": {
      "acc": 0.6786171153904837,
      "acc_stderr": 0.03287747628215248
    },
    "all": {
      "acc": 0.6745021144778927,
      "acc_stderr": 0.03209352695880009,
      "acc_norm": 0.6570508811795919,
      "acc_norm_stderr": 0.009400239931863341
    }
  },
  "versions": {
    "leaderboard|arc:challenge|0": 0,
    "leaderboard|hellaswag|0": 0,
    "leaderboard|mmlu:abstract_algebra|5": 0,
    "leaderboard|mmlu:anatomy|5": 0,
    "leaderboard|mmlu:astronomy|5": 0,
    "leaderboard|mmlu:business_ethics|5": 0,
    "leaderboard|mmlu:clinical_knowledge|5": 0,
    "leaderboard|mmlu:college_biology|5": 0,
    "leaderboard|mmlu:college_chemistry|5": 0,
    "leaderboard|mmlu:college_computer_science|5": 0,
    "leaderboard|mmlu:college_mathematics|5": 0,
    "leaderboard|mmlu:college_medicine|5": 0,
    "leaderboard|mmlu:college_physics|5": 0,
    "leaderboard|mmlu:computer_security|5": 0,
    "leaderboard|mmlu:conceptual_physics|5": 0,
    "leaderboard|mmlu:econometrics|5": 0,
    "leaderboard|mmlu:electrical_engineering|5": 0,
    "leaderboard|mmlu:elementary_mathematics|5": 0,
    "leaderboard|mmlu:formal_logic|5": 0,
    "leaderboard|mmlu:global_facts|5": 0,
    "leaderboard|mmlu:high_school_biology|5": 0,
    "leaderboard|mmlu:high_school_chemistry|5": 0,
    "leaderboard|mmlu:high_school_computer_science|5": 0,
    "leaderboard|mmlu:high_school_european_history|5": 0,
    "leaderboard|mmlu:high_school_geography|5": 0,
    "leaderboard|mmlu:high_school_government_and_politics|5": 0,
    "leaderboard|mmlu:high_school_macroeconomics|5": 0,
    "leaderboard|mmlu:high_school_mathematics|5": 0,
    "leaderboard|mmlu:high_school_microeconomics|5": 0,
    "leaderboard|mmlu:high_school_physics|5": 0,
    "leaderboard|mmlu:high_school_psychology|5": 0,
    "leaderboard|mmlu:high_school_statistics|5": 0,
    "leaderboard|mmlu:high_school_us_history|5": 0,
    "leaderboard|mmlu:high_school_world_history|5": 0,
    "leaderboard|mmlu:human_aging|5": 0,
    "leaderboard|mmlu:human_sexuality|5": 0,
    "leaderboard|mmlu:international_law|5": 0,
    "leaderboard|mmlu:jurisprudence|5": 0,
    "leaderboard|mmlu:logical_fallacies|5": 0,
    "leaderboard|mmlu:machine_learning|5": 0,
    "leaderboard|mmlu:management|5": 0,
    "leaderboard|mmlu:marketing|5": 0,
    "leaderboard|mmlu:medical_genetics|5": 0,
    "leaderboard|mmlu:miscellaneous|5": 0,
    "leaderboard|mmlu:moral_disputes|5": 0,
    "leaderboard|mmlu:moral_scenarios|5": 0,
    "leaderboard|mmlu:nutrition|5": 0,
    "leaderboard|mmlu:philosophy|5": 0,
    "leaderboard|mmlu:prehistory|5": 0,
    "leaderboard|mmlu:professional_accounting|5": 0,
    "leaderboard|mmlu:professional_law|5": 0,
    "leaderboard|mmlu:professional_medicine|5": 0,
    "leaderboard|mmlu:professional_psychology|5": 0,
    "leaderboard|mmlu:public_relations|5": 0,
    "leaderboard|mmlu:security_studies|5": 0,
    "leaderboard|mmlu:sociology|5": 0,
    "leaderboard|mmlu:us_foreign_policy|5": 0,
    "leaderboard|mmlu:virology|5": 0,
    "leaderboard|mmlu:world_religions|5": 0
  },
  "config_tasks": {
    "leaderboard|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "arc"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_harness",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "train",
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:anatomy": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:astronomy": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_biology": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_physics": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:computer_security": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:econometrics": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:global_facts": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_microeconomics": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_aging": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:international_law": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:management": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:marketing": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:nutrition": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:philosophy": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:prehistory": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_law": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:public_relations": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:security_studies": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:sociology": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:virology": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:world_religions": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|arc:challenge|0": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "17b0cae357c0259e",
        "hash_input_tokens": "c8968de53ca8a354",
        "hash_cont_tokens": "0824eeb61d0d5f83"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 4687,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|hellaswag|0": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "31985c805c3a737e",
        "hash_input_tokens": "bb5ca407f7450848",
        "hash_cont_tokens": "3fa65080885b9527"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 39802,
      "non_padded": 366,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "2f776a367d23aea2",
        "hash_input_tokens": "a5875a8854b05035",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "516f74bef25df620",
        "hash_input_tokens": "3f7c6dce45271bb0",
        "hash_cont_tokens": "a14b5b1906dc16a3"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "faf4e80f65de93ca",
        "hash_input_tokens": "e1eb2a287c9695ce",
        "hash_cont_tokens": "235273fd0bc50bcd"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "e42ceba53e0137b9",
        "hash_input_tokens": "24028fa0f50e0c17",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "49654f71d94b65c3",
        "hash_input_tokens": "2aba65d1a5d22239",
        "hash_cont_tokens": "c27aff2906fc75aa"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "2b460b75f1fdfefd",
        "hash_input_tokens": "7bce623fe37bbe79",
        "hash_cont_tokens": "28f68b5aab4efb1c"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "242c9be6da583e95",
        "hash_input_tokens": "18be9b1f4feb7b78",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 388,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "ed2bdb4e87c4b371",
        "hash_input_tokens": "ac64e4573c33ed10",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "770bc4281c973190",
        "hash_input_tokens": "39fb2ab63aa767c8",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "1407310156cc0307",
        "hash_input_tokens": "884d1bf0436afd5b",
        "hash_cont_tokens": "a7bc5e74098b6e5f"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "833a0d7b55aed500",
        "hash_input_tokens": "53861b5c6cbacca4",
        "hash_cont_tokens": "e50fa3937d31d8fb"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "94034c97e85d8f46",
        "hash_input_tokens": "b37f4b68d596c733",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "e40d15a34640d6fa",
        "hash_input_tokens": "de3fe192dcf8c8d3",
        "hash_cont_tokens": "a9551e5af217ca25"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 928,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "612f340fae41338d",
        "hash_input_tokens": "4ba058c77674c279",
        "hash_cont_tokens": "1616cbbcc0299188"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "10275b312d812ae6",
        "hash_input_tokens": "b38b94acf36df152",
        "hash_cont_tokens": "13d52dc7c10431df"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "5ec274c6c82aca23",
        "hash_input_tokens": "49f6a75dbc2828b8",
        "hash_cont_tokens": "f7e8022519425282"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1484,
      "non_padded": 28,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "b25faa42c9b5e8e8",
        "hash_input_tokens": "2f18550adf2a9fd2",
        "hash_cont_tokens": "bec51e4e496b5986"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 492,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "332fdee50a1921b4",
        "hash_input_tokens": "3bd87f57f2512980",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "e624e26ede922561",
        "hash_input_tokens": "2d28db63852549c4",
        "hash_cont_tokens": "7c5f05353074320e"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1236,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "0e3e5f5d9246482a",
        "hash_input_tokens": "41a0897ea93da7e1",
        "hash_cont_tokens": "a062b42dc4e451a1"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 808,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "c00487e67c1813cc",
        "hash_input_tokens": "8c90019471a06970",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "318f4513c537c6bf",
        "hash_input_tokens": "ee2a1299e0e5aa3a",
        "hash_cont_tokens": "b7342549497ce598"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 656,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "ee5789fcc1a81b1e",
        "hash_input_tokens": "714dcd701844d531",
        "hash_cont_tokens": "ba635a50235d17d6"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "ac42d888e1ce1155",
        "hash_input_tokens": "91db04691e129ad4",
        "hash_cont_tokens": "861078cb569a9a2d"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "ad2120a4e003a6c8",
        "hash_input_tokens": "0c622a6303171b57",
        "hash_cont_tokens": "1bd5d8a9878df20b"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "5d88f41fc2d643a8",
        "hash_input_tokens": "3997f860eafd5c6d",
        "hash_cont_tokens": "d641c253ea3fb50b"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1060,
      "non_padded": 20,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "bfc393381298609e",
        "hash_input_tokens": "5122a8629176e1ac",
        "hash_cont_tokens": "ba80bf94e62b9d1d"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "fc78b4997e436734",
        "hash_input_tokens": "0929645e46dfd554",
        "hash_cont_tokens": "38f92c2d4b51791c"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 588,
      "non_padded": 16,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "d5c76aa40b9dbc43",
        "hash_input_tokens": "70d9d9e2cd0ed97e",
        "hash_cont_tokens": "c73b94409db7bea8"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2176,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "f258a8882370bd09",
        "hash_input_tokens": "af4280a476ff7a3e",
        "hash_cont_tokens": "550de2236ddcd3d7"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "5d5ca4840131ba21",
        "hash_input_tokens": "9cad06ab227234b9",
        "hash_cont_tokens": "fa0ad891ef2b914f"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "11845057459afd72",
        "hash_input_tokens": "88a3b8a042e4827f",
        "hash_cont_tokens": "a762b3a2973ca3b3"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "756b9096b8eaf892",
        "hash_input_tokens": "6c8f8f825449854e",
        "hash_cont_tokens": "cc785052ada0f4d2"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 856,
      "non_padded": 36,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "7bbdd5ae4d4e3819",
        "hash_input_tokens": "ac0064befba2a85d",
        "hash_cont_tokens": "ba1fca3d357e2778"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 524,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "db2aefbff5eec996",
        "hash_input_tokens": "88420f4afc09dfdb",
        "hash_cont_tokens": "cc18c6558eedc4bc"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "0f89ee3fe03d6a21",
        "hash_input_tokens": "7abcf8460f7c8305",
        "hash_cont_tokens": "8931513df4f32f4a"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 432,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "98a04b1f8f841069",
        "hash_input_tokens": "cd065846c43732e6",
        "hash_cont_tokens": "1cdf879b3cebe91e"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "97dfd8213b3dcdaa",
        "hash_input_tokens": "96d91a388f7eb72e",
        "hash_cont_tokens": "7545fb7f81f641be"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:management|5": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "f51611f514b265b0",
        "hash_input_tokens": "989227e2525480e2",
        "hash_cont_tokens": "dac3108173edd07e"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "9f78a5f533812e32",
        "hash_input_tokens": "4d8a7e9649653b28",
        "hash_cont_tokens": "86873731b8b2342d"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 936,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "40449dc81d7567c7",
        "hash_input_tokens": "0e8f3f9f3e9de4cd",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "bffec9fc237bcf93",
        "hash_input_tokens": "91fddc95debcb5b5",
        "hash_cont_tokens": "ff17a87c03e638c1"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3128,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "83c5d8d486a769b5",
        "hash_input_tokens": "8d33218535cf4ccf",
        "hash_cont_tokens": "1d40b5bbe8afbaed"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "36e9398d79da8dca",
        "hash_input_tokens": "8aa9000923535193",
        "hash_cont_tokens": "1d48b7d571b76d89"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "e909c006f7654f23",
        "hash_input_tokens": "176bc739e1341e2a",
        "hash_cont_tokens": "664d16d1431ecbc7"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1224,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "a1b5b6f7f76861c2",
        "hash_input_tokens": "19bd2d4026d062d2",
        "hash_cont_tokens": "92ca5851410cb91d"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1196,
      "non_padded": 48,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "152187949bcd0921",
        "hash_input_tokens": "b244bc6bebf0f96f",
        "hash_cont_tokens": "bba4bbb234487df6"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1296,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "0eb7345d6144ee0d",
        "hash_input_tokens": "bc1023bf375b8c82",
        "hash_cont_tokens": "f4a54bb8d07b6cf9"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1128,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "36ac764272bfb182",
        "hash_input_tokens": "4c895b875a93d2fe",
        "hash_cont_tokens": "f5012b40482f1956"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6136,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "7b8d69ea2acaf2f7",
        "hash_input_tokens": "2d144cd24bdb4f97",
        "hash_cont_tokens": "2a0af8cca646c87c"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "fe8937e9ffc99771",
        "hash_input_tokens": "111189334a31482f",
        "hash_cont_tokens": "1be95eae5e663495"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2392,
      "non_padded": 56,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "d9f57f1d86c072b5",
        "hash_input_tokens": "fe273e5dcd3b2a1c",
        "hash_cont_tokens": "d885165284a3d1dc"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 440,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "869c9c3ae196b7c3",
        "hash_input_tokens": "b22b171701819c6f",
        "hash_cont_tokens": "4b188bcf8e4c63dc"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "9c616231ea800efa",
        "hash_input_tokens": "f447e1fe1b3863de",
        "hash_cont_tokens": "25ae64adfded17db"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 804,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "0c7a7081c71c07b6",
        "hash_input_tokens": "738fa68d2167f9f2",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:virology|5": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "01e95325d8b738e4",
        "hash_input_tokens": "5024277bd42cf665",
        "hash_cont_tokens": "b9a3303d5aa72742"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 660,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "be6544f84d16d74b",
        "hash_input_tokens": "3c461de9cdee38ec",
        "hash_cont_tokens": "bbd486c0f082eb01"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "2c5a6e28e29721bb",
      "hash_full_prompts": "7af4bd6e95aa344a",
      "hash_input_tokens": "750706832236ec7e",
      "hash_cont_tokens": "3b0ca4ec28a829ca"
    },
    "truncated": 0,
    "non_truncated": 25256,
    "padded": 100389,
    "non_padded": 634,
    "num_truncated_few_shots": 0
  }
}