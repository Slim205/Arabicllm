{
  "config_general": {
    "lighteval_sha": "1bcde543f10ace8ab221df11e8989b527674a774",
    "num_fewshot_seeds": 1,
    "override_batch_size": 8,
    "max_samples": null,
    "job_id": "",
    "start_time": 164928.155101669,
    "end_time": 165930.786538522,
    "total_evaluation_time_secondes": "1002.6314368530002",
    "model_name": "_gpfs_workdir_barkallasl_Evaluation_ift_full1_Meta-Llama-3-8B-Instruct",
    "model_sha": "",
    "model_dtype": "torch.float16",
    "model_size": "14.96 GB",
    "config": null
  },
  "results": {
    "leaderboard|arc:challenge|0": {
      "acc": 0.5281569965870307,
      "acc_stderr": 0.014588204105102203,
      "acc_norm": 0.5656996587030717,
      "acc_norm_stderr": 0.01448470304885736
    },
    "leaderboard|hellaswag|0": {
      "acc": 0.5833499302927704,
      "acc_stderr": 0.004919962822208312,
      "acc_norm": 0.7697669786895041,
      "acc_norm_stderr": 0.004201215520808244
    },
    "leaderboard|mmlu:abstract_algebra|0": {
      "acc": 0.26,
      "acc_stderr": 0.044084400227680794
    },
    "leaderboard|mmlu:anatomy|0": {
      "acc": 0.7111111111111111,
      "acc_stderr": 0.0391545063041425
    },
    "leaderboard|mmlu:astronomy|0": {
      "acc": 0.743421052631579,
      "acc_stderr": 0.0355418036802569
    },
    "leaderboard|mmlu:business_ethics|0": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001975
    },
    "leaderboard|mmlu:clinical_knowledge|0": {
      "acc": 0.7547169811320755,
      "acc_stderr": 0.0264803571798957
    },
    "leaderboard|mmlu:college_biology|0": {
      "acc": 0.7916666666666666,
      "acc_stderr": 0.03396116205845334
    },
    "leaderboard|mmlu:college_chemistry|0": {
      "acc": 0.44,
      "acc_stderr": 0.04988876515698589
    },
    "leaderboard|mmlu:college_computer_science|0": {
      "acc": 0.45,
      "acc_stderr": 0.05
    },
    "leaderboard|mmlu:college_mathematics|0": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276
    },
    "leaderboard|mmlu:college_medicine|0": {
      "acc": 0.630057803468208,
      "acc_stderr": 0.0368122963339432
    },
    "leaderboard|mmlu:college_physics|0": {
      "acc": 0.47058823529411764,
      "acc_stderr": 0.04966570903978529
    },
    "leaderboard|mmlu:computer_security|0": {
      "acc": 0.74,
      "acc_stderr": 0.04408440022768078
    },
    "leaderboard|mmlu:conceptual_physics|0": {
      "acc": 0.548936170212766,
      "acc_stderr": 0.03252909619613197
    },
    "leaderboard|mmlu:econometrics|0": {
      "acc": 0.5701754385964912,
      "acc_stderr": 0.04657047260594964
    },
    "leaderboard|mmlu:electrical_engineering|0": {
      "acc": 0.6137931034482759,
      "acc_stderr": 0.04057324734419035
    },
    "leaderboard|mmlu:elementary_mathematics|0": {
      "acc": 0.4497354497354497,
      "acc_stderr": 0.02562085704293665
    },
    "leaderboard|mmlu:formal_logic|0": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.044444444444444495
    },
    "leaderboard|mmlu:global_facts|0": {
      "acc": 0.43,
      "acc_stderr": 0.04975698519562428
    },
    "leaderboard|mmlu:high_school_biology|0": {
      "acc": 0.7709677419354839,
      "acc_stderr": 0.023904914311782658
    },
    "leaderboard|mmlu:high_school_chemistry|0": {
      "acc": 0.5615763546798029,
      "acc_stderr": 0.03491207857486519
    },
    "leaderboard|mmlu:high_school_computer_science|0": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316
    },
    "leaderboard|mmlu:high_school_european_history|0": {
      "acc": 0.7454545454545455,
      "acc_stderr": 0.03401506715249039
    },
    "leaderboard|mmlu:high_school_geography|0": {
      "acc": 0.7878787878787878,
      "acc_stderr": 0.029126522834586808
    },
    "leaderboard|mmlu:high_school_government_and_politics|0": {
      "acc": 0.8704663212435233,
      "acc_stderr": 0.024233532297758723
    },
    "leaderboard|mmlu:high_school_macroeconomics|0": {
      "acc": 0.6307692307692307,
      "acc_stderr": 0.02446861524147892
    },
    "leaderboard|mmlu:high_school_mathematics|0": {
      "acc": 0.3814814814814815,
      "acc_stderr": 0.029616718927497596
    },
    "leaderboard|mmlu:high_school_microeconomics|0": {
      "acc": 0.7226890756302521,
      "acc_stderr": 0.02907937453948001
    },
    "leaderboard|mmlu:high_school_physics|0": {
      "acc": 0.4304635761589404,
      "acc_stderr": 0.04042809961395634
    },
    "leaderboard|mmlu:high_school_psychology|0": {
      "acc": 0.8348623853211009,
      "acc_stderr": 0.015919557829976037
    },
    "leaderboard|mmlu:high_school_statistics|0": {
      "acc": 0.49074074074074076,
      "acc_stderr": 0.03409386946992699
    },
    "leaderboard|mmlu:high_school_us_history|0": {
      "acc": 0.8431372549019608,
      "acc_stderr": 0.025524722324553346
    },
    "leaderboard|mmlu:high_school_world_history|0": {
      "acc": 0.8396624472573839,
      "acc_stderr": 0.02388438092596567
    },
    "leaderboard|mmlu:human_aging|0": {
      "acc": 0.6816143497757847,
      "acc_stderr": 0.03126580522513713
    },
    "leaderboard|mmlu:human_sexuality|0": {
      "acc": 0.7633587786259542,
      "acc_stderr": 0.03727673575596913
    },
    "leaderboard|mmlu:international_law|0": {
      "acc": 0.768595041322314,
      "acc_stderr": 0.03849856098794088
    },
    "leaderboard|mmlu:jurisprudence|0": {
      "acc": 0.7870370370370371,
      "acc_stderr": 0.0395783547198098
    },
    "leaderboard|mmlu:logical_fallacies|0": {
      "acc": 0.7975460122699386,
      "acc_stderr": 0.03157065078911901
    },
    "leaderboard|mmlu:machine_learning|0": {
      "acc": 0.48214285714285715,
      "acc_stderr": 0.047427623612430116
    },
    "leaderboard|mmlu:management|0": {
      "acc": 0.8640776699029126,
      "acc_stderr": 0.03393295729761013
    },
    "leaderboard|mmlu:marketing|0": {
      "acc": 0.8846153846153846,
      "acc_stderr": 0.02093019318517933
    },
    "leaderboard|mmlu:medical_genetics|0": {
      "acc": 0.84,
      "acc_stderr": 0.0368452949177471
    },
    "leaderboard|mmlu:miscellaneous|0": {
      "acc": 0.8378033205619413,
      "acc_stderr": 0.013182222616720885
    },
    "leaderboard|mmlu:moral_disputes|0": {
      "acc": 0.708092485549133,
      "acc_stderr": 0.024476994076247337
    },
    "leaderboard|mmlu:moral_scenarios|0": {
      "acc": 0.25251396648044694,
      "acc_stderr": 0.01453033020146863
    },
    "leaderboard|mmlu:nutrition|0": {
      "acc": 0.738562091503268,
      "acc_stderr": 0.025160998214292456
    },
    "leaderboard|mmlu:philosophy|0": {
      "acc": 0.7106109324758842,
      "acc_stderr": 0.025755865922632952
    },
    "leaderboard|mmlu:prehistory|0": {
      "acc": 0.7438271604938271,
      "acc_stderr": 0.024288533637726095
    },
    "leaderboard|mmlu:professional_accounting|0": {
      "acc": 0.5390070921985816,
      "acc_stderr": 0.02973659252642444
    },
    "leaderboard|mmlu:professional_law|0": {
      "acc": 0.48435462842242505,
      "acc_stderr": 0.012763982838120951
    },
    "leaderboard|mmlu:professional_medicine|0": {
      "acc": 0.7426470588235294,
      "acc_stderr": 0.026556519470041506
    },
    "leaderboard|mmlu:professional_psychology|0": {
      "acc": 0.6944444444444444,
      "acc_stderr": 0.018635594034423976
    },
    "leaderboard|mmlu:public_relations|0": {
      "acc": 0.6909090909090909,
      "acc_stderr": 0.044262946482000985
    },
    "leaderboard|mmlu:security_studies|0": {
      "acc": 0.7183673469387755,
      "acc_stderr": 0.028795185574291296
    },
    "leaderboard|mmlu:sociology|0": {
      "acc": 0.8507462686567164,
      "acc_stderr": 0.025196929874827075
    },
    "leaderboard|mmlu:us_foreign_policy|0": {
      "acc": 0.87,
      "acc_stderr": 0.033799766898963086
    },
    "leaderboard|mmlu:virology|0": {
      "acc": 0.5240963855421686,
      "acc_stderr": 0.03887971849597264
    },
    "leaderboard|mmlu:world_religions|0": {
      "acc": 0.8070175438596491,
      "acc_stderr": 0.030267457554898458
    },
    "leaderboard|mmlu:_average|0": {
      "acc": 0.6547505850481838,
      "acc_stderr": 0.03320387527805316
    },
    "all": {
      "acc": 0.6513947504173946,
      "acc_stderr": 0.03240896708095492,
      "acc_norm": 0.6677333186962879,
      "acc_norm_stderr": 0.009342959284832803
    }
  },
  "versions": {
    "leaderboard|arc:challenge|0": 0,
    "leaderboard|hellaswag|0": 0,
    "leaderboard|mmlu:abstract_algebra|0": 0,
    "leaderboard|mmlu:anatomy|0": 0,
    "leaderboard|mmlu:astronomy|0": 0,
    "leaderboard|mmlu:business_ethics|0": 0,
    "leaderboard|mmlu:clinical_knowledge|0": 0,
    "leaderboard|mmlu:college_biology|0": 0,
    "leaderboard|mmlu:college_chemistry|0": 0,
    "leaderboard|mmlu:college_computer_science|0": 0,
    "leaderboard|mmlu:college_mathematics|0": 0,
    "leaderboard|mmlu:college_medicine|0": 0,
    "leaderboard|mmlu:college_physics|0": 0,
    "leaderboard|mmlu:computer_security|0": 0,
    "leaderboard|mmlu:conceptual_physics|0": 0,
    "leaderboard|mmlu:econometrics|0": 0,
    "leaderboard|mmlu:electrical_engineering|0": 0,
    "leaderboard|mmlu:elementary_mathematics|0": 0,
    "leaderboard|mmlu:formal_logic|0": 0,
    "leaderboard|mmlu:global_facts|0": 0,
    "leaderboard|mmlu:high_school_biology|0": 0,
    "leaderboard|mmlu:high_school_chemistry|0": 0,
    "leaderboard|mmlu:high_school_computer_science|0": 0,
    "leaderboard|mmlu:high_school_european_history|0": 0,
    "leaderboard|mmlu:high_school_geography|0": 0,
    "leaderboard|mmlu:high_school_government_and_politics|0": 0,
    "leaderboard|mmlu:high_school_macroeconomics|0": 0,
    "leaderboard|mmlu:high_school_mathematics|0": 0,
    "leaderboard|mmlu:high_school_microeconomics|0": 0,
    "leaderboard|mmlu:high_school_physics|0": 0,
    "leaderboard|mmlu:high_school_psychology|0": 0,
    "leaderboard|mmlu:high_school_statistics|0": 0,
    "leaderboard|mmlu:high_school_us_history|0": 0,
    "leaderboard|mmlu:high_school_world_history|0": 0,
    "leaderboard|mmlu:human_aging|0": 0,
    "leaderboard|mmlu:human_sexuality|0": 0,
    "leaderboard|mmlu:international_law|0": 0,
    "leaderboard|mmlu:jurisprudence|0": 0,
    "leaderboard|mmlu:logical_fallacies|0": 0,
    "leaderboard|mmlu:machine_learning|0": 0,
    "leaderboard|mmlu:management|0": 0,
    "leaderboard|mmlu:marketing|0": 0,
    "leaderboard|mmlu:medical_genetics|0": 0,
    "leaderboard|mmlu:miscellaneous|0": 0,
    "leaderboard|mmlu:moral_disputes|0": 0,
    "leaderboard|mmlu:moral_scenarios|0": 0,
    "leaderboard|mmlu:nutrition|0": 0,
    "leaderboard|mmlu:philosophy|0": 0,
    "leaderboard|mmlu:prehistory|0": 0,
    "leaderboard|mmlu:professional_accounting|0": 0,
    "leaderboard|mmlu:professional_law|0": 0,
    "leaderboard|mmlu:professional_medicine|0": 0,
    "leaderboard|mmlu:professional_psychology|0": 0,
    "leaderboard|mmlu:public_relations|0": 0,
    "leaderboard|mmlu:security_studies|0": 0,
    "leaderboard|mmlu:sociology|0": 0,
    "leaderboard|mmlu:us_foreign_policy|0": 0,
    "leaderboard|mmlu:virology|0": 0,
    "leaderboard|mmlu:world_religions|0": 0
  },
  "config_tasks": {
    "leaderboard|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "arc"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_harness",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "train",
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:anatomy": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:astronomy": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_biology": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_physics": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:computer_security": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:econometrics": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:global_facts": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_microeconomics": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_aging": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:international_law": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:management": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:marketing": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:nutrition": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:philosophy": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:prehistory": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_law": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:public_relations": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:security_studies": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:sociology": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:virology": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:world_religions": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|arc:challenge|0": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "17b0cae357c0259e",
        "hash_input_tokens": "49f914e509706a0e",
        "hash_cont_tokens": "81d3b894ea813550"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 4687,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|hellaswag|0": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "31985c805c3a737e",
        "hash_input_tokens": "dad539ec35e92192",
        "hash_cont_tokens": "62f86fec09eb9253"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 39713,
      "non_padded": 455,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "4c76229e00c9c0e9",
        "hash_input_tokens": "0c5c6fd48803bf98",
        "hash_cont_tokens": "5204561e2386b3ca"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:anatomy|0": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "6a1f8104dccbd33b",
        "hash_input_tokens": "aab5f3c39dc676fc",
        "hash_cont_tokens": "fafbb7e36165912f"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 536,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:astronomy|0": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "1302effa3a76ce4c",
        "hash_input_tokens": "cb91b77604ccfd19",
        "hash_cont_tokens": "0d7867c4537fe287"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 588,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:business_ethics|0": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "03cb8bce5336419a",
        "hash_input_tokens": "bbd9bd0236de7298",
        "hash_cont_tokens": "e3a4413073693776"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "ffbb9c7b2be257f9",
        "hash_input_tokens": "7925525d291ca6e3",
        "hash_cont_tokens": "bbda97be923c2577"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1056,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_biology|0": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "3ee77f176f38eb8e",
        "hash_input_tokens": "404e8983bb7986de",
        "hash_cont_tokens": "4130848417737217"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 568,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_chemistry|0": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "ce61a69c46d47aeb",
        "hash_input_tokens": "667e4a6f46087710",
        "hash_cont_tokens": "6805d439a2a48821"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_computer_science|0": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "32805b52d7d5daab",
        "hash_input_tokens": "e50a7e286e1cded3",
        "hash_cont_tokens": "293b865d31f0befd"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_mathematics|0": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "55da1a0a0bd33722",
        "hash_input_tokens": "90eea6e19dd2a9ce",
        "hash_cont_tokens": "97fb25216bd4e902"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_medicine|0": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "c33e143163049176",
        "hash_input_tokens": "8514317d703f6a1a",
        "hash_cont_tokens": "a7bc5e74098b6e5f"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 684,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_physics|0": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "ebdab1cdb7e555df",
        "hash_input_tokens": "9fb31f1f0775f5ed",
        "hash_cont_tokens": "cbd88795d965a7b3"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 400,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:computer_security|0": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "a24fd7d08a560921",
        "hash_input_tokens": "a2bca5c9527eb928",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "8300977a79386993",
        "hash_input_tokens": "d8895fafe87ce9c0",
        "hash_cont_tokens": "168496fdb8097890"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:econometrics|0": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "ddde36788a04a46f",
        "hash_input_tokens": "67836aed7996cc19",
        "hash_cont_tokens": "1616cbbcc0299188"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 440,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "acbc5def98c19b3f",
        "hash_input_tokens": "01f5de310a9f3101",
        "hash_cont_tokens": "13d52dc7c10431df"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 576,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "146e61d07497a9bd",
        "hash_input_tokens": "53d6e2dd24314795",
        "hash_cont_tokens": "f7e8022519425282"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1496,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:formal_logic|0": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "8635216e1909a03f",
        "hash_input_tokens": "02e688c8f2ffdc65",
        "hash_cont_tokens": "a748d3fdfd3f44d1"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 500,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:global_facts|0": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "30b315aa6353ee47",
        "hash_input_tokens": "b58d015dc9622500",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_biology|0": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "c9136373af2180de",
        "hash_input_tokens": "2785f316da5df926",
        "hash_cont_tokens": "7c5f05353074320e"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1232,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "b0661bfa1add6404",
        "hash_input_tokens": "98b6a9f4ee27cd51",
        "hash_cont_tokens": "a062b42dc4e451a1"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 796,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "80fc1d623a3d665f",
        "hash_input_tokens": "11aa4c41f7965f5c",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 392,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "854da6e5af0fe1a1",
        "hash_input_tokens": "59bb3b30684a9f27",
        "hash_cont_tokens": "b7342549497ce598"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_geography|0": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "7dc963c7acd19ad8",
        "hash_input_tokens": "e10a62eedca1add8",
        "hash_cont_tokens": "ba635a50235d17d6"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 788,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "1f675dcdebc9758f",
        "hash_input_tokens": "7aa7d15ddcab62c4",
        "hash_cont_tokens": "861078cb569a9a2d"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 752,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "2fb32cf2d80f0b35",
        "hash_input_tokens": "62dc0b3119684608",
        "hash_cont_tokens": "1bd5d8a9878df20b"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1513,
      "non_padded": 47,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "fd6646fdb5d58a1f",
        "hash_input_tokens": "f3b68a2a6e2d3ec6",
        "hash_cont_tokens": "d641c253ea3fb50b"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1048,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "2118f21f71d87d84",
        "hash_input_tokens": "9f8d60b905e103c9",
        "hash_cont_tokens": "ba80bf94e62b9d1d"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 924,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_physics|0": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "dc3ce06378548565",
        "hash_input_tokens": "083a0c860a503a87",
        "hash_cont_tokens": "38f92c2d4b51791c"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 592,
      "non_padded": 12,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "c8d1d98a40e11f2f",
        "hash_input_tokens": "1364c9fb24768357",
        "hash_cont_tokens": "7cebb0d84386e0b2"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2084,
      "non_padded": 96,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "666c8759b98ee4ff",
        "hash_input_tokens": "93ebc88422c48114",
        "hash_cont_tokens": "550de2236ddcd3d7"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 856,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "95fef1c4b7d3f81e",
        "hash_input_tokens": "71f8de38c99061c5",
        "hash_cont_tokens": "fa0ad891ef2b914f"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "7e5085b6184b0322",
        "hash_input_tokens": "c496869ae6a34fbf",
        "hash_cont_tokens": "a762b3a2973ca3b3"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_aging|0": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "c17333e7c7c10797",
        "hash_input_tokens": "dff43c81aceb28c4",
        "hash_cont_tokens": "cc785052ada0f4d2"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 864,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_sexuality|0": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "4edd1e9045df5e3d",
        "hash_input_tokens": "62d5c7fb61a121e7",
        "hash_cont_tokens": "ba1fca3d357e2778"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 512,
      "non_padded": 12,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:international_law|0": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "db2fa00d771a062a",
        "hash_input_tokens": "736bdf6a89b00960",
        "hash_cont_tokens": "cc18c6558eedc4bc"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 468,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:jurisprudence|0": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "e956f86b124076fe",
        "hash_input_tokens": "3af6e2f39a99998e",
        "hash_cont_tokens": "8931513df4f32f4a"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 404,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "956e0e6365ab79f1",
        "hash_input_tokens": "25ebdb034ce47b70",
        "hash_cont_tokens": "1cdf879b3cebe91e"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 632,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:machine_learning|0": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "397997cc6f4d581e",
        "hash_input_tokens": "df5b066d729be5b4",
        "hash_cont_tokens": "7545fb7f81f641be"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 436,
      "non_padded": 12,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:management|0": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "2bcbe6f6ca63d740",
        "hash_input_tokens": "3ce922e4ab2ee817",
        "hash_cont_tokens": "dac3108173edd07e"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 376,
      "non_padded": 36,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:marketing|0": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "8ddb20d964a1b065",
        "hash_input_tokens": "70fdb043308aba62",
        "hash_cont_tokens": "86873731b8b2342d"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 876,
      "non_padded": 60,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:medical_genetics|0": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "182a71f4763d2cea",
        "hash_input_tokens": "3fb8237f5876baba",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 380,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:miscellaneous|0": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "4c404fdbb4ca57fc",
        "hash_input_tokens": "1abbafc227e9f30a",
        "hash_cont_tokens": "64a29ceb379966af"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3060,
      "non_padded": 72,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_disputes|0": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "60cbd2baa3fea5c9",
        "hash_input_tokens": "64e679bb436d862f",
        "hash_cont_tokens": "1d40b5bbe8afbaed"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1340,
      "non_padded": 44,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "fd8b0431fbdd75ef",
        "hash_input_tokens": "4bebbb6bbebb748f",
        "hash_cont_tokens": "1d48b7d571b76d89"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3480,
      "non_padded": 100,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:nutrition|0": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "71e55e2b829b6528",
        "hash_input_tokens": "a763bb2cf0a1933a",
        "hash_cont_tokens": "664d16d1431ecbc7"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1188,
      "non_padded": 36,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:philosophy|0": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "a6d489a8d208fa4b",
        "hash_input_tokens": "32ae633da989c31a",
        "hash_cont_tokens": "92ca5851410cb91d"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1212,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:prehistory|0": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "6cc50f032a19acaa",
        "hash_input_tokens": "d1789f3990c6439c",
        "hash_cont_tokens": "6f1822c5df3f3104"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1240,
      "non_padded": 56,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_accounting|0": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "50f57ab32f5f6cea",
        "hash_input_tokens": "d518e7c7aa7200e6",
        "hash_cont_tokens": "f4a54bb8d07b6cf9"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1104,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_law|0": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "a8fdc85c64f4b215",
        "hash_input_tokens": "791343a469b13538",
        "hash_cont_tokens": "f5012b40482f1956"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6120,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_medicine|0": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "c373a28a3050a73a",
        "hash_input_tokens": "f3d4d441f3a4d266",
        "hash_cont_tokens": "55b0fa3e0bf5b008"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1084,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_psychology|0": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "bf5254fe818356af",
        "hash_input_tokens": "23b5846c8eb7fca0",
        "hash_cont_tokens": "d76b9ce1d4e961f0"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2376,
      "non_padded": 72,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:public_relations|0": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "b66d52e28e7d14e0",
        "hash_input_tokens": "c78e36419ffa42cb",
        "hash_cont_tokens": "f20fceb13b71bfd0"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 400,
      "non_padded": 40,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:security_studies|0": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "514c14feaf000ad9",
        "hash_input_tokens": "032cc5eb564edaa5",
        "hash_cont_tokens": "53342b812f928d69"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 960,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:sociology|0": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "f6c9bc9d18c80870",
        "hash_input_tokens": "f9aec117e8f5226d",
        "hash_cont_tokens": "525f2982befbdc64"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 764,
      "non_padded": 40,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "ed7b78629db6678f",
        "hash_input_tokens": "481ad4b7c9525fdf",
        "hash_cont_tokens": "ca3db01df633c5ca"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 380,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:virology|0": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "bc52ffdc3f9b994a",
        "hash_input_tokens": "e408c5de9d937c67",
        "hash_cont_tokens": "e60d839e34684fbf"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 644,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:world_religions|0": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "ecdb4a4f94f62930",
        "hash_input_tokens": "f7001d2ed21d4c9c",
        "hash_cont_tokens": "d5a0e0988e0b983d"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 660,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "2c5a6e28e29721bb",
      "hash_full_prompts": "2c5a6e28e29721bb",
      "hash_input_tokens": "a46d5ab15cad39e0",
      "hash_cont_tokens": "6cac054c631fbb94"
    },
    "truncated": 0,
    "non_truncated": 25256,
    "padded": 99333,
    "non_padded": 1690,
    "num_truncated_few_shots": 0
  }
}