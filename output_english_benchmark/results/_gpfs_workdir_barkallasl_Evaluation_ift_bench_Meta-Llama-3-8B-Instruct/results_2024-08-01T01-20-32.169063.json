{
  "config_general": {
    "lighteval_sha": "1bcde543f10ace8ab221df11e8989b527674a774",
    "num_fewshot_seeds": 1,
    "override_batch_size": 8,
    "max_samples": null,
    "job_id": "",
    "start_time": 169785.802224532,
    "end_time": 170835.601444662,
    "total_evaluation_time_secondes": "1049.799220129993",
    "model_name": "_gpfs_workdir_barkallasl_Evaluation_ift_bench_Meta-Llama-3-8B-Instruct",
    "model_sha": "",
    "model_dtype": "torch.float16",
    "model_size": "14.96 GB",
    "config": null
  },
  "results": {
    "leaderboard|arc:challenge|0": {
      "acc": 0.5460750853242321,
      "acc_stderr": 0.014549221105171869,
      "acc_norm": 0.5691126279863481,
      "acc_norm_stderr": 0.014471133392642468
    },
    "leaderboard|hellaswag|0": {
      "acc": 0.5856403106950807,
      "acc_stderr": 0.004916043838455667,
      "acc_norm": 0.7616012746464847,
      "acc_norm_stderr": 0.004252333443827121
    },
    "leaderboard|mmlu:abstract_algebra|0": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814
    },
    "leaderboard|mmlu:anatomy|0": {
      "acc": 0.6888888888888889,
      "acc_stderr": 0.039992628766177214
    },
    "leaderboard|mmlu:astronomy|0": {
      "acc": 0.7236842105263158,
      "acc_stderr": 0.03639057569952929
    },
    "leaderboard|mmlu:business_ethics|0": {
      "acc": 0.63,
      "acc_stderr": 0.04852365870939098
    },
    "leaderboard|mmlu:clinical_knowledge|0": {
      "acc": 0.7849056603773585,
      "acc_stderr": 0.02528839450289137
    },
    "leaderboard|mmlu:college_biology|0": {
      "acc": 0.7986111111111112,
      "acc_stderr": 0.03353647469713839
    },
    "leaderboard|mmlu:college_chemistry|0": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620333
    },
    "leaderboard|mmlu:college_computer_science|0": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956912
    },
    "leaderboard|mmlu:college_mathematics|0": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316
    },
    "leaderboard|mmlu:college_medicine|0": {
      "acc": 0.6705202312138728,
      "acc_stderr": 0.03583901754736412
    },
    "leaderboard|mmlu:college_physics|0": {
      "acc": 0.49019607843137253,
      "acc_stderr": 0.04974229460422817
    },
    "leaderboard|mmlu:computer_security|0": {
      "acc": 0.78,
      "acc_stderr": 0.04163331998932261
    },
    "leaderboard|mmlu:conceptual_physics|0": {
      "acc": 0.5702127659574469,
      "acc_stderr": 0.03236214467715564
    },
    "leaderboard|mmlu:econometrics|0": {
      "acc": 0.5789473684210527,
      "acc_stderr": 0.046446020912223177
    },
    "leaderboard|mmlu:electrical_engineering|0": {
      "acc": 0.6413793103448275,
      "acc_stderr": 0.039966295748767186
    },
    "leaderboard|mmlu:elementary_mathematics|0": {
      "acc": 0.47619047619047616,
      "acc_stderr": 0.02572209706438853
    },
    "leaderboard|mmlu:formal_logic|0": {
      "acc": 0.47619047619047616,
      "acc_stderr": 0.04467062628403273
    },
    "leaderboard|mmlu:global_facts|0": {
      "acc": 0.41,
      "acc_stderr": 0.04943110704237102
    },
    "leaderboard|mmlu:high_school_biology|0": {
      "acc": 0.7935483870967742,
      "acc_stderr": 0.023025899617188695
    },
    "leaderboard|mmlu:high_school_chemistry|0": {
      "acc": 0.5172413793103449,
      "acc_stderr": 0.035158955511656986
    },
    "leaderboard|mmlu:high_school_computer_science|0": {
      "acc": 0.7,
      "acc_stderr": 0.046056618647183814
    },
    "leaderboard|mmlu:high_school_european_history|0": {
      "acc": 0.7333333333333333,
      "acc_stderr": 0.03453131801885417
    },
    "leaderboard|mmlu:high_school_geography|0": {
      "acc": 0.8232323232323232,
      "acc_stderr": 0.027178752639044915
    },
    "leaderboard|mmlu:high_school_government_and_politics|0": {
      "acc": 0.8860103626943006,
      "acc_stderr": 0.022935144053919436
    },
    "leaderboard|mmlu:high_school_macroeconomics|0": {
      "acc": 0.6461538461538462,
      "acc_stderr": 0.024243783994062167
    },
    "leaderboard|mmlu:high_school_mathematics|0": {
      "acc": 0.3925925925925926,
      "acc_stderr": 0.029773847012532967
    },
    "leaderboard|mmlu:high_school_microeconomics|0": {
      "acc": 0.7478991596638656,
      "acc_stderr": 0.028205545033277726
    },
    "leaderboard|mmlu:high_school_physics|0": {
      "acc": 0.48344370860927155,
      "acc_stderr": 0.04080244185628972
    },
    "leaderboard|mmlu:high_school_psychology|0": {
      "acc": 0.8440366972477065,
      "acc_stderr": 0.01555580271359017
    },
    "leaderboard|mmlu:high_school_statistics|0": {
      "acc": 0.5324074074074074,
      "acc_stderr": 0.03402801581358966
    },
    "leaderboard|mmlu:high_school_us_history|0": {
      "acc": 0.8382352941176471,
      "acc_stderr": 0.02584501798692692
    },
    "leaderboard|mmlu:high_school_world_history|0": {
      "acc": 0.8523206751054853,
      "acc_stderr": 0.0230943295825957
    },
    "leaderboard|mmlu:human_aging|0": {
      "acc": 0.7085201793721974,
      "acc_stderr": 0.030500283176545857
    },
    "leaderboard|mmlu:human_sexuality|0": {
      "acc": 0.7938931297709924,
      "acc_stderr": 0.03547771004159464
    },
    "leaderboard|mmlu:international_law|0": {
      "acc": 0.768595041322314,
      "acc_stderr": 0.03849856098794088
    },
    "leaderboard|mmlu:jurisprudence|0": {
      "acc": 0.7870370370370371,
      "acc_stderr": 0.039578354719809784
    },
    "leaderboard|mmlu:logical_fallacies|0": {
      "acc": 0.8159509202453987,
      "acc_stderr": 0.030446777687971716
    },
    "leaderboard|mmlu:machine_learning|0": {
      "acc": 0.5446428571428571,
      "acc_stderr": 0.04726835553719098
    },
    "leaderboard|mmlu:management|0": {
      "acc": 0.8155339805825242,
      "acc_stderr": 0.03840423627288276
    },
    "leaderboard|mmlu:marketing|0": {
      "acc": 0.905982905982906,
      "acc_stderr": 0.019119892798924978
    },
    "leaderboard|mmlu:medical_genetics|0": {
      "acc": 0.85,
      "acc_stderr": 0.03588702812826369
    },
    "leaderboard|mmlu:miscellaneous|0": {
      "acc": 0.8301404853128991,
      "acc_stderr": 0.013428186370608315
    },
    "leaderboard|mmlu:moral_disputes|0": {
      "acc": 0.7254335260115607,
      "acc_stderr": 0.024027745155265026
    },
    "leaderboard|mmlu:moral_scenarios|0": {
      "acc": 0.28156424581005585,
      "acc_stderr": 0.015042290171866103
    },
    "leaderboard|mmlu:nutrition|0": {
      "acc": 0.7712418300653595,
      "acc_stderr": 0.02405102973991225
    },
    "leaderboard|mmlu:philosophy|0": {
      "acc": 0.7009646302250804,
      "acc_stderr": 0.026003301117885142
    },
    "leaderboard|mmlu:prehistory|0": {
      "acc": 0.7469135802469136,
      "acc_stderr": 0.024191808600713
    },
    "leaderboard|mmlu:professional_accounting|0": {
      "acc": 0.549645390070922,
      "acc_stderr": 0.029680105565029043
    },
    "leaderboard|mmlu:professional_law|0": {
      "acc": 0.49478487614080835,
      "acc_stderr": 0.012769541449652547
    },
    "leaderboard|mmlu:professional_medicine|0": {
      "acc": 0.7352941176470589,
      "acc_stderr": 0.026799562024887657
    },
    "leaderboard|mmlu:professional_psychology|0": {
      "acc": 0.696078431372549,
      "acc_stderr": 0.018607552131279834
    },
    "leaderboard|mmlu:public_relations|0": {
      "acc": 0.6909090909090909,
      "acc_stderr": 0.044262946482000985
    },
    "leaderboard|mmlu:security_studies|0": {
      "acc": 0.7387755102040816,
      "acc_stderr": 0.02812342933514278
    },
    "leaderboard|mmlu:sociology|0": {
      "acc": 0.8557213930348259,
      "acc_stderr": 0.02484575321230604
    },
    "leaderboard|mmlu:us_foreign_policy|0": {
      "acc": 0.88,
      "acc_stderr": 0.03265986323710906
    },
    "leaderboard|mmlu:virology|0": {
      "acc": 0.5180722891566265,
      "acc_stderr": 0.03889951252827216
    },
    "leaderboard|mmlu:world_religions|0": {
      "acc": 0.8128654970760234,
      "acc_stderr": 0.029913127232368032
    },
    "leaderboard|mmlu:_average|0": {
      "acc": 0.6685744331396172,
      "acc_stderr": 0.03300594187588152
    },
    "all": {
      "acc": 0.6650925099148727,
      "acc_stderr": 0.03221701613337074,
      "acc_norm": 0.6653569513164164,
      "acc_norm_stderr": 0.009361733418234795
    }
  },
  "versions": {
    "leaderboard|arc:challenge|0": 0,
    "leaderboard|hellaswag|0": 0,
    "leaderboard|mmlu:abstract_algebra|0": 0,
    "leaderboard|mmlu:anatomy|0": 0,
    "leaderboard|mmlu:astronomy|0": 0,
    "leaderboard|mmlu:business_ethics|0": 0,
    "leaderboard|mmlu:clinical_knowledge|0": 0,
    "leaderboard|mmlu:college_biology|0": 0,
    "leaderboard|mmlu:college_chemistry|0": 0,
    "leaderboard|mmlu:college_computer_science|0": 0,
    "leaderboard|mmlu:college_mathematics|0": 0,
    "leaderboard|mmlu:college_medicine|0": 0,
    "leaderboard|mmlu:college_physics|0": 0,
    "leaderboard|mmlu:computer_security|0": 0,
    "leaderboard|mmlu:conceptual_physics|0": 0,
    "leaderboard|mmlu:econometrics|0": 0,
    "leaderboard|mmlu:electrical_engineering|0": 0,
    "leaderboard|mmlu:elementary_mathematics|0": 0,
    "leaderboard|mmlu:formal_logic|0": 0,
    "leaderboard|mmlu:global_facts|0": 0,
    "leaderboard|mmlu:high_school_biology|0": 0,
    "leaderboard|mmlu:high_school_chemistry|0": 0,
    "leaderboard|mmlu:high_school_computer_science|0": 0,
    "leaderboard|mmlu:high_school_european_history|0": 0,
    "leaderboard|mmlu:high_school_geography|0": 0,
    "leaderboard|mmlu:high_school_government_and_politics|0": 0,
    "leaderboard|mmlu:high_school_macroeconomics|0": 0,
    "leaderboard|mmlu:high_school_mathematics|0": 0,
    "leaderboard|mmlu:high_school_microeconomics|0": 0,
    "leaderboard|mmlu:high_school_physics|0": 0,
    "leaderboard|mmlu:high_school_psychology|0": 0,
    "leaderboard|mmlu:high_school_statistics|0": 0,
    "leaderboard|mmlu:high_school_us_history|0": 0,
    "leaderboard|mmlu:high_school_world_history|0": 0,
    "leaderboard|mmlu:human_aging|0": 0,
    "leaderboard|mmlu:human_sexuality|0": 0,
    "leaderboard|mmlu:international_law|0": 0,
    "leaderboard|mmlu:jurisprudence|0": 0,
    "leaderboard|mmlu:logical_fallacies|0": 0,
    "leaderboard|mmlu:machine_learning|0": 0,
    "leaderboard|mmlu:management|0": 0,
    "leaderboard|mmlu:marketing|0": 0,
    "leaderboard|mmlu:medical_genetics|0": 0,
    "leaderboard|mmlu:miscellaneous|0": 0,
    "leaderboard|mmlu:moral_disputes|0": 0,
    "leaderboard|mmlu:moral_scenarios|0": 0,
    "leaderboard|mmlu:nutrition|0": 0,
    "leaderboard|mmlu:philosophy|0": 0,
    "leaderboard|mmlu:prehistory|0": 0,
    "leaderboard|mmlu:professional_accounting|0": 0,
    "leaderboard|mmlu:professional_law|0": 0,
    "leaderboard|mmlu:professional_medicine|0": 0,
    "leaderboard|mmlu:professional_psychology|0": 0,
    "leaderboard|mmlu:public_relations|0": 0,
    "leaderboard|mmlu:security_studies|0": 0,
    "leaderboard|mmlu:sociology|0": 0,
    "leaderboard|mmlu:us_foreign_policy|0": 0,
    "leaderboard|mmlu:virology|0": 0,
    "leaderboard|mmlu:world_religions|0": 0
  },
  "config_tasks": {
    "leaderboard|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "arc"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_harness",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "train",
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:anatomy": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:astronomy": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_biology": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_physics": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:computer_security": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:econometrics": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:global_facts": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_microeconomics": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_aging": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:international_law": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:management": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:marketing": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:nutrition": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:philosophy": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:prehistory": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_law": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:public_relations": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:security_studies": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:sociology": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:virology": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:world_religions": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|arc:challenge|0": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "17b0cae357c0259e",
        "hash_input_tokens": "49f914e509706a0e",
        "hash_cont_tokens": "81d3b894ea813550"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 4687,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|hellaswag|0": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "31985c805c3a737e",
        "hash_input_tokens": "dad539ec35e92192",
        "hash_cont_tokens": "62f86fec09eb9253"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 39713,
      "non_padded": 455,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "4c76229e00c9c0e9",
        "hash_input_tokens": "0c5c6fd48803bf98",
        "hash_cont_tokens": "5204561e2386b3ca"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:anatomy|0": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "6a1f8104dccbd33b",
        "hash_input_tokens": "aab5f3c39dc676fc",
        "hash_cont_tokens": "fafbb7e36165912f"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 536,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:astronomy|0": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "1302effa3a76ce4c",
        "hash_input_tokens": "cb91b77604ccfd19",
        "hash_cont_tokens": "0d7867c4537fe287"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 588,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:business_ethics|0": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "03cb8bce5336419a",
        "hash_input_tokens": "bbd9bd0236de7298",
        "hash_cont_tokens": "e3a4413073693776"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "ffbb9c7b2be257f9",
        "hash_input_tokens": "7925525d291ca6e3",
        "hash_cont_tokens": "bbda97be923c2577"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1056,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_biology|0": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "3ee77f176f38eb8e",
        "hash_input_tokens": "404e8983bb7986de",
        "hash_cont_tokens": "4130848417737217"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 568,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_chemistry|0": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "ce61a69c46d47aeb",
        "hash_input_tokens": "667e4a6f46087710",
        "hash_cont_tokens": "6805d439a2a48821"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_computer_science|0": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "32805b52d7d5daab",
        "hash_input_tokens": "e50a7e286e1cded3",
        "hash_cont_tokens": "293b865d31f0befd"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_mathematics|0": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "55da1a0a0bd33722",
        "hash_input_tokens": "90eea6e19dd2a9ce",
        "hash_cont_tokens": "97fb25216bd4e902"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_medicine|0": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "c33e143163049176",
        "hash_input_tokens": "8514317d703f6a1a",
        "hash_cont_tokens": "a7bc5e74098b6e5f"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 684,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_physics|0": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "ebdab1cdb7e555df",
        "hash_input_tokens": "9fb31f1f0775f5ed",
        "hash_cont_tokens": "cbd88795d965a7b3"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 400,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:computer_security|0": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "a24fd7d08a560921",
        "hash_input_tokens": "a2bca5c9527eb928",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "8300977a79386993",
        "hash_input_tokens": "d8895fafe87ce9c0",
        "hash_cont_tokens": "168496fdb8097890"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:econometrics|0": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "ddde36788a04a46f",
        "hash_input_tokens": "67836aed7996cc19",
        "hash_cont_tokens": "1616cbbcc0299188"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 440,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "acbc5def98c19b3f",
        "hash_input_tokens": "01f5de310a9f3101",
        "hash_cont_tokens": "13d52dc7c10431df"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 576,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "146e61d07497a9bd",
        "hash_input_tokens": "53d6e2dd24314795",
        "hash_cont_tokens": "f7e8022519425282"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1496,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:formal_logic|0": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "8635216e1909a03f",
        "hash_input_tokens": "02e688c8f2ffdc65",
        "hash_cont_tokens": "a748d3fdfd3f44d1"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 500,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:global_facts|0": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "30b315aa6353ee47",
        "hash_input_tokens": "b58d015dc9622500",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_biology|0": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "c9136373af2180de",
        "hash_input_tokens": "2785f316da5df926",
        "hash_cont_tokens": "7c5f05353074320e"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1232,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "b0661bfa1add6404",
        "hash_input_tokens": "98b6a9f4ee27cd51",
        "hash_cont_tokens": "a062b42dc4e451a1"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 796,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "80fc1d623a3d665f",
        "hash_input_tokens": "11aa4c41f7965f5c",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 392,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "854da6e5af0fe1a1",
        "hash_input_tokens": "59bb3b30684a9f27",
        "hash_cont_tokens": "b7342549497ce598"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_geography|0": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "7dc963c7acd19ad8",
        "hash_input_tokens": "e10a62eedca1add8",
        "hash_cont_tokens": "ba635a50235d17d6"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 788,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "1f675dcdebc9758f",
        "hash_input_tokens": "7aa7d15ddcab62c4",
        "hash_cont_tokens": "861078cb569a9a2d"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 752,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "2fb32cf2d80f0b35",
        "hash_input_tokens": "62dc0b3119684608",
        "hash_cont_tokens": "1bd5d8a9878df20b"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1513,
      "non_padded": 47,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "fd6646fdb5d58a1f",
        "hash_input_tokens": "f3b68a2a6e2d3ec6",
        "hash_cont_tokens": "d641c253ea3fb50b"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1048,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "2118f21f71d87d84",
        "hash_input_tokens": "9f8d60b905e103c9",
        "hash_cont_tokens": "ba80bf94e62b9d1d"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 924,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_physics|0": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "dc3ce06378548565",
        "hash_input_tokens": "083a0c860a503a87",
        "hash_cont_tokens": "38f92c2d4b51791c"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 592,
      "non_padded": 12,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "c8d1d98a40e11f2f",
        "hash_input_tokens": "1364c9fb24768357",
        "hash_cont_tokens": "7cebb0d84386e0b2"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2084,
      "non_padded": 96,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "666c8759b98ee4ff",
        "hash_input_tokens": "93ebc88422c48114",
        "hash_cont_tokens": "550de2236ddcd3d7"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 856,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "95fef1c4b7d3f81e",
        "hash_input_tokens": "71f8de38c99061c5",
        "hash_cont_tokens": "fa0ad891ef2b914f"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "7e5085b6184b0322",
        "hash_input_tokens": "c496869ae6a34fbf",
        "hash_cont_tokens": "a762b3a2973ca3b3"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_aging|0": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "c17333e7c7c10797",
        "hash_input_tokens": "dff43c81aceb28c4",
        "hash_cont_tokens": "cc785052ada0f4d2"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 864,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_sexuality|0": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "4edd1e9045df5e3d",
        "hash_input_tokens": "62d5c7fb61a121e7",
        "hash_cont_tokens": "ba1fca3d357e2778"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 512,
      "non_padded": 12,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:international_law|0": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "db2fa00d771a062a",
        "hash_input_tokens": "736bdf6a89b00960",
        "hash_cont_tokens": "cc18c6558eedc4bc"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 468,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:jurisprudence|0": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "e956f86b124076fe",
        "hash_input_tokens": "3af6e2f39a99998e",
        "hash_cont_tokens": "8931513df4f32f4a"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 404,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "956e0e6365ab79f1",
        "hash_input_tokens": "25ebdb034ce47b70",
        "hash_cont_tokens": "1cdf879b3cebe91e"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 632,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:machine_learning|0": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "397997cc6f4d581e",
        "hash_input_tokens": "df5b066d729be5b4",
        "hash_cont_tokens": "7545fb7f81f641be"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 436,
      "non_padded": 12,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:management|0": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "2bcbe6f6ca63d740",
        "hash_input_tokens": "3ce922e4ab2ee817",
        "hash_cont_tokens": "dac3108173edd07e"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 376,
      "non_padded": 36,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:marketing|0": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "8ddb20d964a1b065",
        "hash_input_tokens": "70fdb043308aba62",
        "hash_cont_tokens": "86873731b8b2342d"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 876,
      "non_padded": 60,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:medical_genetics|0": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "182a71f4763d2cea",
        "hash_input_tokens": "3fb8237f5876baba",
        "hash_cont_tokens": "b1e74e2fab182909"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 380,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:miscellaneous|0": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "4c404fdbb4ca57fc",
        "hash_input_tokens": "1abbafc227e9f30a",
        "hash_cont_tokens": "64a29ceb379966af"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3060,
      "non_padded": 72,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_disputes|0": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "60cbd2baa3fea5c9",
        "hash_input_tokens": "64e679bb436d862f",
        "hash_cont_tokens": "1d40b5bbe8afbaed"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1340,
      "non_padded": 44,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "fd8b0431fbdd75ef",
        "hash_input_tokens": "4bebbb6bbebb748f",
        "hash_cont_tokens": "1d48b7d571b76d89"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3480,
      "non_padded": 100,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:nutrition|0": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "71e55e2b829b6528",
        "hash_input_tokens": "a763bb2cf0a1933a",
        "hash_cont_tokens": "664d16d1431ecbc7"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1188,
      "non_padded": 36,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:philosophy|0": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "a6d489a8d208fa4b",
        "hash_input_tokens": "32ae633da989c31a",
        "hash_cont_tokens": "92ca5851410cb91d"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1212,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:prehistory|0": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "6cc50f032a19acaa",
        "hash_input_tokens": "d1789f3990c6439c",
        "hash_cont_tokens": "6f1822c5df3f3104"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1240,
      "non_padded": 56,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_accounting|0": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "50f57ab32f5f6cea",
        "hash_input_tokens": "d518e7c7aa7200e6",
        "hash_cont_tokens": "f4a54bb8d07b6cf9"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1104,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_law|0": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "a8fdc85c64f4b215",
        "hash_input_tokens": "791343a469b13538",
        "hash_cont_tokens": "f5012b40482f1956"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6120,
      "non_padded": 16,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_medicine|0": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "c373a28a3050a73a",
        "hash_input_tokens": "f3d4d441f3a4d266",
        "hash_cont_tokens": "55b0fa3e0bf5b008"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1084,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_psychology|0": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "bf5254fe818356af",
        "hash_input_tokens": "23b5846c8eb7fca0",
        "hash_cont_tokens": "d76b9ce1d4e961f0"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2376,
      "non_padded": 72,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:public_relations|0": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "b66d52e28e7d14e0",
        "hash_input_tokens": "c78e36419ffa42cb",
        "hash_cont_tokens": "f20fceb13b71bfd0"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 400,
      "non_padded": 40,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:security_studies|0": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "514c14feaf000ad9",
        "hash_input_tokens": "032cc5eb564edaa5",
        "hash_cont_tokens": "53342b812f928d69"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 960,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:sociology|0": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "f6c9bc9d18c80870",
        "hash_input_tokens": "f9aec117e8f5226d",
        "hash_cont_tokens": "525f2982befbdc64"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 764,
      "non_padded": 40,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "ed7b78629db6678f",
        "hash_input_tokens": "481ad4b7c9525fdf",
        "hash_cont_tokens": "ca3db01df633c5ca"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 380,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:virology|0": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "bc52ffdc3f9b994a",
        "hash_input_tokens": "e408c5de9d937c67",
        "hash_cont_tokens": "e60d839e34684fbf"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 644,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:world_religions|0": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "ecdb4a4f94f62930",
        "hash_input_tokens": "f7001d2ed21d4c9c",
        "hash_cont_tokens": "d5a0e0988e0b983d"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 660,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "2c5a6e28e29721bb",
      "hash_full_prompts": "2c5a6e28e29721bb",
      "hash_input_tokens": "a46d5ab15cad39e0",
      "hash_cont_tokens": "6cac054c631fbb94"
    },
    "truncated": 0,
    "non_truncated": 25256,
    "padded": 99333,
    "non_padded": 1690,
    "num_truncated_few_shots": 0
  }
}