{
  "config_general": {
    "lighteval_sha": "2c6fcd34da05fa752b6107ecd4c20a0de41a02ba",
    "num_fewshot_seeds": 1,
    "override_batch_size": 32,
    "max_samples": null,
    "job_id": "",
    "start_time": 99696.53925356,
    "end_time": 100543.498271914,
    "total_evaluation_time_secondes": "846.9590183539985",
    "model_name": "EleutherAI/pythia-70m",
    "model_sha": "a39f36b100fe8a5377810d56c3f4789b9c53ac42",
    "model_dtype": "torch.float16",
    "model_size": "138.83 MB",
    "config": null
  },
  "results": {
    "leaderboard|arc:challenge|25": {
      "acc": 0.1825938566552901,
      "acc_stderr": 0.011289730684564986,
      "acc_norm": 0.21245733788395904,
      "acc_norm_stderr": 0.011953482906582949
    },
    "leaderboard|hellaswag|10": {
      "acc": 0.26618203545110536,
      "acc_stderr": 0.004410573431837628,
      "acc_norm": 0.27016530571599284,
      "acc_norm_stderr": 0.0044313755499113695
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256
    },
    "leaderboard|mmlu:anatomy|5": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.039154506304142495
    },
    "leaderboard|mmlu:astronomy|5": {
      "acc": 0.24342105263157895,
      "acc_stderr": 0.034923496688842384
    },
    "leaderboard|mmlu:business_ethics|5": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "acc": 0.21509433962264152,
      "acc_stderr": 0.025288394502891373
    },
    "leaderboard|mmlu:college_biology|5": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.03476590104304134
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "acc": 0.41,
      "acc_stderr": 0.049431107042371025
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394
    },
    "leaderboard|mmlu:college_medicine|5": {
      "acc": 0.35260115606936415,
      "acc_stderr": 0.03643037168958549
    },
    "leaderboard|mmlu:college_physics|5": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.04533838195929775
    },
    "leaderboard|mmlu:computer_security|5": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036843
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "acc": 0.2723404255319149,
      "acc_stderr": 0.029101290698386705
    },
    "leaderboard|mmlu:econometrics|5": {
      "acc": 0.17543859649122806,
      "acc_stderr": 0.03577954813948369
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "acc": 0.2482758620689655,
      "acc_stderr": 0.036001056927277696
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "acc": 0.2566137566137566,
      "acc_stderr": 0.022494510767503154
    },
    "leaderboard|mmlu:formal_logic|5": {
      "acc": 0.1984126984126984,
      "acc_stderr": 0.035670166752768614
    },
    "leaderboard|mmlu:global_facts|5": {
      "acc": 0.14,
      "acc_stderr": 0.034873508801977704
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "acc": 0.26129032258064516,
      "acc_stderr": 0.024993053397764815
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "acc": 0.30049261083743845,
      "acc_stderr": 0.03225799476233484
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "acc": 0.2606060606060606,
      "acc_stderr": 0.03427743175816524
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.032742879140268674
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "acc": 0.3626943005181347,
      "acc_stderr": 0.034697137917043715
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "acc": 0.2948717948717949,
      "acc_stderr": 0.023119362758232283
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "acc": 0.2814814814814815,
      "acc_stderr": 0.027420019350945277
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "acc": 0.24369747899159663,
      "acc_stderr": 0.02788682807838057
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "acc": 0.2847682119205298,
      "acc_stderr": 0.03684881521389023
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "acc": 0.27706422018348625,
      "acc_stderr": 0.019188482590169535
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.03398110890294636
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.03132179803083292
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "acc": 0.26582278481012656,
      "acc_stderr": 0.02875679962965834
    },
    "leaderboard|mmlu:human_aging|5": {
      "acc": 0.23766816143497757,
      "acc_stderr": 0.028568079464714267
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "acc": 0.26717557251908397,
      "acc_stderr": 0.038808483010823944
    },
    "leaderboard|mmlu:international_law|5": {
      "acc": 0.2396694214876033,
      "acc_stderr": 0.03896878985070417
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.041331194402438376
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "acc": 0.3067484662576687,
      "acc_stderr": 0.036230899157241495
    },
    "leaderboard|mmlu:machine_learning|5": {
      "acc": 0.23214285714285715,
      "acc_stderr": 0.04007341809755806
    },
    "leaderboard|mmlu:management|5": {
      "acc": 0.18446601941747573,
      "acc_stderr": 0.03840423627288276
    },
    "leaderboard|mmlu:marketing|5": {
      "acc": 0.23504273504273504,
      "acc_stderr": 0.027778835904935427
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542129
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "acc": 0.20178799489144317,
      "acc_stderr": 0.014351702181636863
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "acc": 0.2514450867052023,
      "acc_stderr": 0.023357365785874037
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "acc": 0.23910614525139665,
      "acc_stderr": 0.014265554192331158
    },
    "leaderboard|mmlu:nutrition|5": {
      "acc": 0.25163398692810457,
      "acc_stderr": 0.0248480182638752
    },
    "leaderboard|mmlu:philosophy|5": {
      "acc": 0.19292604501607716,
      "acc_stderr": 0.02241151678091136
    },
    "leaderboard|mmlu:prehistory|5": {
      "acc": 0.21296296296296297,
      "acc_stderr": 0.0227797190887334
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "acc": 0.21631205673758866,
      "acc_stderr": 0.024561720560562807
    },
    "leaderboard|mmlu:professional_law|5": {
      "acc": 0.23859191655801826,
      "acc_stderr": 0.010885929742002217
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "acc": 0.30514705882352944,
      "acc_stderr": 0.027971541370170598
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.016639319350313264
    },
    "leaderboard|mmlu:public_relations|5": {
      "acc": 0.2636363636363636,
      "acc_stderr": 0.04220224692971987
    },
    "leaderboard|mmlu:security_studies|5": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.028920583220675585
    },
    "leaderboard|mmlu:sociology|5": {
      "acc": 0.26865671641791045,
      "acc_stderr": 0.031343283582089536
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505
    },
    "leaderboard|mmlu:virology|5": {
      "acc": 0.2289156626506024,
      "acc_stderr": 0.03270745277352477
    },
    "leaderboard|mmlu:world_religions|5": {
      "acc": 0.23391812865497075,
      "acc_stderr": 0.03246721765117827
    },
    "leaderboard|truthfulqa:mc|0": {
      "truthfulqa_mc1": 0.23990208078335373,
      "truthfulqa_mc1_stderr": 0.014948812679062133,
      "truthfulqa_mc2": 0.46829392492078264,
      "truthfulqa_mc2_stderr": 0.015473767105050822
    },
    "leaderboard|winogrande|5": {
      "acc": 0.5074980268350434,
      "acc_stderr": 0.014050905521228577
    },
    "leaderboard|gsm8k|5": {
      "qem": 0.0,
      "qem_stderr": 0.0
    },
    "leaderboard|mmlu:_average|5": {
      "acc": 0.2585295440736839,
      "acc_stderr": 0.03251879168271407
    },
    "all": {
      "acc": 0.26154096551902367,
      "acc_stderr": 0.03138870559253888,
      "acc_norm": 0.24131132179997594,
      "acc_norm_stderr": 0.008192429228247158,
      "truthfulqa_mc1": 0.23990208078335373,
      "truthfulqa_mc1_stderr": 0.014948812679062133,
      "truthfulqa_mc2": 0.46829392492078264,
      "truthfulqa_mc2_stderr": 0.015473767105050822,
      "qem": 0.0,
      "qem_stderr": 0.0
    }
  },
  "versions": {
    "leaderboard|arc:challenge|25": 0,
    "leaderboard|gsm8k|5": 0,
    "leaderboard|hellaswag|10": 0,
    "leaderboard|mmlu:abstract_algebra|5": 0,
    "leaderboard|mmlu:anatomy|5": 0,
    "leaderboard|mmlu:astronomy|5": 0,
    "leaderboard|mmlu:business_ethics|5": 0,
    "leaderboard|mmlu:clinical_knowledge|5": 0,
    "leaderboard|mmlu:college_biology|5": 0,
    "leaderboard|mmlu:college_chemistry|5": 0,
    "leaderboard|mmlu:college_computer_science|5": 0,
    "leaderboard|mmlu:college_mathematics|5": 0,
    "leaderboard|mmlu:college_medicine|5": 0,
    "leaderboard|mmlu:college_physics|5": 0,
    "leaderboard|mmlu:computer_security|5": 0,
    "leaderboard|mmlu:conceptual_physics|5": 0,
    "leaderboard|mmlu:econometrics|5": 0,
    "leaderboard|mmlu:electrical_engineering|5": 0,
    "leaderboard|mmlu:elementary_mathematics|5": 0,
    "leaderboard|mmlu:formal_logic|5": 0,
    "leaderboard|mmlu:global_facts|5": 0,
    "leaderboard|mmlu:high_school_biology|5": 0,
    "leaderboard|mmlu:high_school_chemistry|5": 0,
    "leaderboard|mmlu:high_school_computer_science|5": 0,
    "leaderboard|mmlu:high_school_european_history|5": 0,
    "leaderboard|mmlu:high_school_geography|5": 0,
    "leaderboard|mmlu:high_school_government_and_politics|5": 0,
    "leaderboard|mmlu:high_school_macroeconomics|5": 0,
    "leaderboard|mmlu:high_school_mathematics|5": 0,
    "leaderboard|mmlu:high_school_microeconomics|5": 0,
    "leaderboard|mmlu:high_school_physics|5": 0,
    "leaderboard|mmlu:high_school_psychology|5": 0,
    "leaderboard|mmlu:high_school_statistics|5": 0,
    "leaderboard|mmlu:high_school_us_history|5": 0,
    "leaderboard|mmlu:high_school_world_history|5": 0,
    "leaderboard|mmlu:human_aging|5": 0,
    "leaderboard|mmlu:human_sexuality|5": 0,
    "leaderboard|mmlu:international_law|5": 0,
    "leaderboard|mmlu:jurisprudence|5": 0,
    "leaderboard|mmlu:logical_fallacies|5": 0,
    "leaderboard|mmlu:machine_learning|5": 0,
    "leaderboard|mmlu:management|5": 0,
    "leaderboard|mmlu:marketing|5": 0,
    "leaderboard|mmlu:medical_genetics|5": 0,
    "leaderboard|mmlu:miscellaneous|5": 0,
    "leaderboard|mmlu:moral_disputes|5": 0,
    "leaderboard|mmlu:moral_scenarios|5": 0,
    "leaderboard|mmlu:nutrition|5": 0,
    "leaderboard|mmlu:philosophy|5": 0,
    "leaderboard|mmlu:prehistory|5": 0,
    "leaderboard|mmlu:professional_accounting|5": 0,
    "leaderboard|mmlu:professional_law|5": 0,
    "leaderboard|mmlu:professional_medicine|5": 0,
    "leaderboard|mmlu:professional_psychology|5": 0,
    "leaderboard|mmlu:public_relations|5": 0,
    "leaderboard|mmlu:security_studies|5": 0,
    "leaderboard|mmlu:sociology|5": 0,
    "leaderboard|mmlu:us_foreign_policy|5": 0,
    "leaderboard|mmlu:virology|5": 0,
    "leaderboard|mmlu:world_religions|5": 0,
    "leaderboard|truthfulqa:mc|0": 0,
    "leaderboard|winogrande|5": 0
  },
  "config_tasks": {
    "leaderboard|arc:challenge": {
      "name": "arc:challenge",
      "prompt_function": "arc",
      "hf_repo": "ai2_arc",
      "hf_subset": "ARC-Challenge",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm_nospace"
      ],
      "hf_avail_splits": [
        "train",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "arc"
      ],
      "original_num_docs": 1172,
      "effective_num_docs": 1172,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|gsm8k": {
      "name": "gsm8k",
      "prompt_function": "gsm8k",
      "hf_repo": "gsm8k",
      "hf_subset": "main",
      "metric": [
        "quasi_exact_match_gsm8k"
      ],
      "hf_avail_splits": [
        "train",
        "test"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": 256,
      "stop_sequence": [
        "Question:",
        "Question",
        ":"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 1319,
      "effective_num_docs": 1319,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|hellaswag": {
      "name": "hellaswag",
      "prompt_function": "hellaswag_harness",
      "hf_repo": "hellaswag",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc",
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "train",
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling_from_train",
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 10042,
      "effective_num_docs": 10042,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:abstract_algebra": {
      "name": "mmlu:abstract_algebra",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:anatomy": {
      "name": "mmlu:anatomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:astronomy": {
      "name": "mmlu:astronomy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:business_ethics": {
      "name": "mmlu:business_ethics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:clinical_knowledge": {
      "name": "mmlu:clinical_knowledge",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_biology": {
      "name": "mmlu:college_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_chemistry": {
      "name": "mmlu:college_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_computer_science": {
      "name": "mmlu:college_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_mathematics": {
      "name": "mmlu:college_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_medicine": {
      "name": "mmlu:college_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:college_physics": {
      "name": "mmlu:college_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:computer_security": {
      "name": "mmlu:computer_security",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:conceptual_physics": {
      "name": "mmlu:conceptual_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:econometrics": {
      "name": "mmlu:econometrics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:electrical_engineering": {
      "name": "mmlu:electrical_engineering",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:elementary_mathematics": {
      "name": "mmlu:elementary_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:formal_logic": {
      "name": "mmlu:formal_logic",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:global_facts": {
      "name": "mmlu:global_facts",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_biology": {
      "name": "mmlu:high_school_biology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_chemistry": {
      "name": "mmlu:high_school_chemistry",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_computer_science": {
      "name": "mmlu:high_school_computer_science",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_european_history": {
      "name": "mmlu:high_school_european_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_geography": {
      "name": "mmlu:high_school_geography",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics": {
      "name": "mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics": {
      "name": "mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_mathematics": {
      "name": "mmlu:high_school_mathematics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_microeconomics": {
      "name": "mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_physics": {
      "name": "mmlu:high_school_physics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_psychology": {
      "name": "mmlu:high_school_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_statistics": {
      "name": "mmlu:high_school_statistics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_us_history": {
      "name": "mmlu:high_school_us_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:high_school_world_history": {
      "name": "mmlu:high_school_world_history",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_aging": {
      "name": "mmlu:human_aging",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:human_sexuality": {
      "name": "mmlu:human_sexuality",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:international_law": {
      "name": "mmlu:international_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:jurisprudence": {
      "name": "mmlu:jurisprudence",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:logical_fallacies": {
      "name": "mmlu:logical_fallacies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:machine_learning": {
      "name": "mmlu:machine_learning",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:management": {
      "name": "mmlu:management",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:marketing": {
      "name": "mmlu:marketing",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:medical_genetics": {
      "name": "mmlu:medical_genetics",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:miscellaneous": {
      "name": "mmlu:miscellaneous",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_disputes": {
      "name": "mmlu:moral_disputes",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:moral_scenarios": {
      "name": "mmlu:moral_scenarios",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:nutrition": {
      "name": "mmlu:nutrition",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:philosophy": {
      "name": "mmlu:philosophy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:prehistory": {
      "name": "mmlu:prehistory",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_accounting": {
      "name": "mmlu:professional_accounting",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_law": {
      "name": "mmlu:professional_law",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_medicine": {
      "name": "mmlu:professional_medicine",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:professional_psychology": {
      "name": "mmlu:professional_psychology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:public_relations": {
      "name": "mmlu:public_relations",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:security_studies": {
      "name": "mmlu:security_studies",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:sociology": {
      "name": "mmlu:sociology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:us_foreign_policy": {
      "name": "mmlu:us_foreign_policy",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:virology": {
      "name": "mmlu:virology",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|mmlu:world_religions": {
      "name": "mmlu:world_religions",
      "prompt_function": "mmlu_harness",
      "hf_repo": "lighteval/mmlu",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "auxiliary_train",
        "test",
        "validation",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": 1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard",
        "mmlu"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|truthfulqa:mc": {
      "name": "truthfulqa:mc",
      "prompt_function": "truthful_qa_multiple_choice",
      "hf_repo": "truthful_qa",
      "hf_subset": "multiple_choice",
      "metric": [
        "truthfulqa_mc_metrics"
      ],
      "hf_avail_splits": [
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 817,
      "effective_num_docs": 817,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "leaderboard|winogrande": {
      "name": "winogrande",
      "prompt_function": "winogrande",
      "hf_repo": "winogrande",
      "hf_subset": "winogrande_xl",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "train",
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "validation"
      ],
      "few_shots_split": null,
      "few_shots_select": "random_sampling",
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "leaderboard"
      ],
      "original_num_docs": 1267,
      "effective_num_docs": 1267,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "leaderboard|arc:challenge|25": {
      "hashes": {
        "hash_examples": "17b0cae357c0259e",
        "hash_full_prompts": "045cbb916e5145c6",
        "hash_input_tokens": "46f0b3b783032db3",
        "hash_cont_tokens": "dd9d83cc407f3752"
      },
      "truncated": 0,
      "non_truncated": 1172,
      "padded": 4687,
      "non_padded": 0,
      "effective_few_shots": 25.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|hellaswag|10": {
      "hashes": {
        "hash_examples": "31985c805c3a737e",
        "hash_full_prompts": "fb29e0ca629fe078",
        "hash_input_tokens": "fbc1f01ae1b1274a",
        "hash_cont_tokens": "a577daa6fb3acd98"
      },
      "truncated": 0,
      "non_truncated": 10042,
      "padded": 40076,
      "non_padded": 92,
      "effective_few_shots": 10.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:abstract_algebra|5": {
      "hashes": {
        "hash_examples": "4c76229e00c9c0e9",
        "hash_full_prompts": "2f776a367d23aea2",
        "hash_input_tokens": "651eb02a3e250f23",
        "hash_cont_tokens": "74c639e56bb475af"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:anatomy|5": {
      "hashes": {
        "hash_examples": "6a1f8104dccbd33b",
        "hash_full_prompts": "516f74bef25df620",
        "hash_input_tokens": "7e8c5680094f695a",
        "hash_cont_tokens": "ec7e2288ab5f1ce9"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:astronomy|5": {
      "hashes": {
        "hash_examples": "1302effa3a76ce4c",
        "hash_full_prompts": "faf4e80f65de93ca",
        "hash_input_tokens": "77477bd0c52d45e3",
        "hash_cont_tokens": "42b5a6e48d5c7319"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:business_ethics|5": {
      "hashes": {
        "hash_examples": "03cb8bce5336419a",
        "hash_full_prompts": "e42ceba53e0137b9",
        "hash_input_tokens": "6b5ceb137f712c8a",
        "hash_cont_tokens": "f46eda7e7da54bf8"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:clinical_knowledge|5": {
      "hashes": {
        "hash_examples": "ffbb9c7b2be257f9",
        "hash_full_prompts": "49654f71d94b65c3",
        "hash_input_tokens": "3d5bb6ba9c4ca681",
        "hash_cont_tokens": "bc82b3cc5072f164"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_biology|5": {
      "hashes": {
        "hash_examples": "3ee77f176f38eb8e",
        "hash_full_prompts": "2b460b75f1fdfefd",
        "hash_input_tokens": "ba7ac66cf5b5c9a3",
        "hash_cont_tokens": "7d9d259d1ab54c0a"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_chemistry|5": {
      "hashes": {
        "hash_examples": "ce61a69c46d47aeb",
        "hash_full_prompts": "242c9be6da583e95",
        "hash_input_tokens": "a2fbd45193285610",
        "hash_cont_tokens": "4f3687cd9e44f540"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_computer_science|5": {
      "hashes": {
        "hash_examples": "32805b52d7d5daab",
        "hash_full_prompts": "ed2bdb4e87c4b371",
        "hash_input_tokens": "ce4853a41d0cd54f",
        "hash_cont_tokens": "fad8c7e0d8c335c7"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_mathematics|5": {
      "hashes": {
        "hash_examples": "55da1a0a0bd33722",
        "hash_full_prompts": "770bc4281c973190",
        "hash_input_tokens": "c7ff9bb692e15ec2",
        "hash_cont_tokens": "64cbad4dfebfa2c4"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_medicine|5": {
      "hashes": {
        "hash_examples": "c33e143163049176",
        "hash_full_prompts": "1407310156cc0307",
        "hash_input_tokens": "60083c2819a4076a",
        "hash_cont_tokens": "500f6a276b9c8960"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:college_physics|5": {
      "hashes": {
        "hash_examples": "ebdab1cdb7e555df",
        "hash_full_prompts": "833a0d7b55aed500",
        "hash_input_tokens": "187a83723231715f",
        "hash_cont_tokens": "c013a70f5b052723"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:computer_security|5": {
      "hashes": {
        "hash_examples": "a24fd7d08a560921",
        "hash_full_prompts": "94034c97e85d8f46",
        "hash_input_tokens": "2f78ec05a5d22700",
        "hash_cont_tokens": "74c639e56bb475af"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:conceptual_physics|5": {
      "hashes": {
        "hash_examples": "8300977a79386993",
        "hash_full_prompts": "e40d15a34640d6fa",
        "hash_input_tokens": "74f8c05c84a32dec",
        "hash_cont_tokens": "43818b3dc0c7496f"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:econometrics|5": {
      "hashes": {
        "hash_examples": "ddde36788a04a46f",
        "hash_full_prompts": "612f340fae41338d",
        "hash_input_tokens": "131f662c1f175a3e",
        "hash_cont_tokens": "497e966c4a2f7760"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:electrical_engineering|5": {
      "hashes": {
        "hash_examples": "acbc5def98c19b3f",
        "hash_full_prompts": "10275b312d812ae6",
        "hash_input_tokens": "f5fc3b3406272d9b",
        "hash_cont_tokens": "7e14ccd1e2688bb8"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:elementary_mathematics|5": {
      "hashes": {
        "hash_examples": "146e61d07497a9bd",
        "hash_full_prompts": "5ec274c6c82aca23",
        "hash_input_tokens": "22c100bb6e286131",
        "hash_cont_tokens": "36f4284d11515df5"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:formal_logic|5": {
      "hashes": {
        "hash_examples": "8635216e1909a03f",
        "hash_full_prompts": "b25faa42c9b5e8e8",
        "hash_input_tokens": "a22ed98baa88d48e",
        "hash_cont_tokens": "20034c13549d89d3"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:global_facts|5": {
      "hashes": {
        "hash_examples": "30b315aa6353ee47",
        "hash_full_prompts": "332fdee50a1921b4",
        "hash_input_tokens": "22b9700a3b810df8",
        "hash_cont_tokens": "74c639e56bb475af"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_biology|5": {
      "hashes": {
        "hash_examples": "c9136373af2180de",
        "hash_full_prompts": "e624e26ede922561",
        "hash_input_tokens": "cfdaa31bdc17666b",
        "hash_cont_tokens": "ce8477815b16bf0d"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_chemistry|5": {
      "hashes": {
        "hash_examples": "b0661bfa1add6404",
        "hash_full_prompts": "0e3e5f5d9246482a",
        "hash_input_tokens": "fcbf90e620cf6536",
        "hash_cont_tokens": "6db048339b8b13ee"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_computer_science|5": {
      "hashes": {
        "hash_examples": "80fc1d623a3d665f",
        "hash_full_prompts": "c00487e67c1813cc",
        "hash_input_tokens": "074396f738a653a3",
        "hash_cont_tokens": "62e5a109ec2c691a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_european_history|5": {
      "hashes": {
        "hash_examples": "854da6e5af0fe1a1",
        "hash_full_prompts": "318f4513c537c6bf",
        "hash_input_tokens": "6e47d696116edd01",
        "hash_cont_tokens": "47a5e5973f50fe17"
      },
      "truncated": 660,
      "non_truncated": -495,
      "padded": 0,
      "non_padded": 660,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_geography|5": {
      "hashes": {
        "hash_examples": "7dc963c7acd19ad8",
        "hash_full_prompts": "ee5789fcc1a81b1e",
        "hash_input_tokens": "632a01a732aa777a",
        "hash_cont_tokens": "cfed4e4c2adfbf35"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_government_and_politics|5": {
      "hashes": {
        "hash_examples": "1f675dcdebc9758f",
        "hash_full_prompts": "ac42d888e1ce1155",
        "hash_input_tokens": "77a4715398a66462",
        "hash_cont_tokens": "5d4317e7acbf10e5"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_macroeconomics|5": {
      "hashes": {
        "hash_examples": "2fb32cf2d80f0b35",
        "hash_full_prompts": "ad2120a4e003a6c8",
        "hash_input_tokens": "33fce82da9a8f7b8",
        "hash_cont_tokens": "8d468d84a686647d"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_mathematics|5": {
      "hashes": {
        "hash_examples": "fd6646fdb5d58a1f",
        "hash_full_prompts": "5d88f41fc2d643a8",
        "hash_input_tokens": "982bcb0861d7c1e7",
        "hash_cont_tokens": "f495df2541d0ea70"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_microeconomics|5": {
      "hashes": {
        "hash_examples": "2118f21f71d87d84",
        "hash_full_prompts": "bfc393381298609e",
        "hash_input_tokens": "52aa679d61fc1dc7",
        "hash_cont_tokens": "4c32e38c066727bc"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 944,
      "non_padded": 8,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_physics|5": {
      "hashes": {
        "hash_examples": "dc3ce06378548565",
        "hash_full_prompts": "fc78b4997e436734",
        "hash_input_tokens": "c3aca140a476dd7e",
        "hash_cont_tokens": "2de92598e2a5542b"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_psychology|5": {
      "hashes": {
        "hash_examples": "c8d1d98a40e11f2f",
        "hash_full_prompts": "d5c76aa40b9dbc43",
        "hash_input_tokens": "2d917c8672a37c2e",
        "hash_cont_tokens": "066f3ad305a16dbf"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_statistics|5": {
      "hashes": {
        "hash_examples": "666c8759b98ee4ff",
        "hash_full_prompts": "f258a8882370bd09",
        "hash_input_tokens": "714b6290f90cad19",
        "hash_cont_tokens": "b82a9cd2530d17d8"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_us_history|5": {
      "hashes": {
        "hash_examples": "95fef1c4b7d3f81e",
        "hash_full_prompts": "5d5ca4840131ba21",
        "hash_input_tokens": "b8f58f05dc082011",
        "hash_cont_tokens": "e5ab34a54e3f5b7c"
      },
      "truncated": 816,
      "non_truncated": -612,
      "padded": 0,
      "non_padded": 816,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:high_school_world_history|5": {
      "hashes": {
        "hash_examples": "7e5085b6184b0322",
        "hash_full_prompts": "11845057459afd72",
        "hash_input_tokens": "3af911bf93093a85",
        "hash_cont_tokens": "e49bc61edf535886"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_aging|5": {
      "hashes": {
        "hash_examples": "c17333e7c7c10797",
        "hash_full_prompts": "756b9096b8eaf892",
        "hash_input_tokens": "c122ff639fc9d56a",
        "hash_cont_tokens": "7982edf99219e1b0"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:human_sexuality|5": {
      "hashes": {
        "hash_examples": "4edd1e9045df5e3d",
        "hash_full_prompts": "7bbdd5ae4d4e3819",
        "hash_input_tokens": "f97564430cdb3e22",
        "hash_cont_tokens": "ed73d516c5552dd0"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 524,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:international_law|5": {
      "hashes": {
        "hash_examples": "db2fa00d771a062a",
        "hash_full_prompts": "db2aefbff5eec996",
        "hash_input_tokens": "2117ff32fc3e462b",
        "hash_cont_tokens": "9a47b656409e1f92"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:jurisprudence|5": {
      "hashes": {
        "hash_examples": "e956f86b124076fe",
        "hash_full_prompts": "0f89ee3fe03d6a21",
        "hash_input_tokens": "8b0302f3c835c54d",
        "hash_cont_tokens": "f124b796e0b5dfbb"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 428,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:logical_fallacies|5": {
      "hashes": {
        "hash_examples": "956e0e6365ab79f1",
        "hash_full_prompts": "98a04b1f8f841069",
        "hash_input_tokens": "c411d79f0e8759ab",
        "hash_cont_tokens": "eb791fcbee9e0682"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 644,
      "non_padded": 8,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:machine_learning|5": {
      "hashes": {
        "hash_examples": "397997cc6f4d581e",
        "hash_full_prompts": "97dfd8213b3dcdaa",
        "hash_input_tokens": "4cbf920724908138",
        "hash_cont_tokens": "d495c0c6c31a6059"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:management|5": {
      "hashes": {
        "hash_examples": "2bcbe6f6ca63d740",
        "hash_full_prompts": "f51611f514b265b0",
        "hash_input_tokens": "5402c11228372a45",
        "hash_cont_tokens": "27795e9c98bdeda8"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:marketing|5": {
      "hashes": {
        "hash_examples": "8ddb20d964a1b065",
        "hash_full_prompts": "9f78a5f533812e32",
        "hash_input_tokens": "292e54a4097a1b65",
        "hash_cont_tokens": "874c5b0b496cbe8a"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 904,
      "non_padded": 32,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:medical_genetics|5": {
      "hashes": {
        "hash_examples": "182a71f4763d2cea",
        "hash_full_prompts": "40449dc81d7567c7",
        "hash_input_tokens": "6a3f15fa314fdc6c",
        "hash_cont_tokens": "74c639e56bb475af"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:miscellaneous|5": {
      "hashes": {
        "hash_examples": "4c404fdbb4ca57fc",
        "hash_full_prompts": "bffec9fc237bcf93",
        "hash_input_tokens": "10baca04d746effe",
        "hash_cont_tokens": "313ee361fbdbab3c"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3132,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_disputes|5": {
      "hashes": {
        "hash_examples": "60cbd2baa3fea5c9",
        "hash_full_prompts": "83c5d8d486a769b5",
        "hash_input_tokens": "4dfda7b794bea10d",
        "hash_cont_tokens": "5628a608617254fe"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:moral_scenarios|5": {
      "hashes": {
        "hash_examples": "fd8b0431fbdd75ef",
        "hash_full_prompts": "36e9398d79da8dca",
        "hash_input_tokens": "d034da4fe5af0e97",
        "hash_cont_tokens": "37763d363cbcba89"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3576,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:nutrition|5": {
      "hashes": {
        "hash_examples": "71e55e2b829b6528",
        "hash_full_prompts": "e909c006f7654f23",
        "hash_input_tokens": "fef37d04d622df68",
        "hash_cont_tokens": "6d307d5b31de6862"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1224,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:philosophy|5": {
      "hashes": {
        "hash_examples": "a6d489a8d208fa4b",
        "hash_full_prompts": "a1b5b6f7f76861c2",
        "hash_input_tokens": "0575c0b420866966",
        "hash_cont_tokens": "06eafa950d795e8e"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1244,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:prehistory|5": {
      "hashes": {
        "hash_examples": "6cc50f032a19acaa",
        "hash_full_prompts": "152187949bcd0921",
        "hash_input_tokens": "a32bfc3a4984c9fc",
        "hash_cont_tokens": "b88dc5457a06af4d"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1296,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_accounting|5": {
      "hashes": {
        "hash_examples": "50f57ab32f5f6cea",
        "hash_full_prompts": "0eb7345d6144ee0d",
        "hash_input_tokens": "3e668d3ce9b6593b",
        "hash_cont_tokens": "3a172ef83f67dce5"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1120,
      "non_padded": 8,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_law|5": {
      "hashes": {
        "hash_examples": "a8fdc85c64f4b215",
        "hash_full_prompts": "36ac764272bfb182",
        "hash_input_tokens": "7f837322b1b62ac1",
        "hash_cont_tokens": "6b31cf265df9b81b"
      },
      "truncated": 16,
      "non_truncated": 1518,
      "padded": 6120,
      "non_padded": 16,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_medicine|5": {
      "hashes": {
        "hash_examples": "c373a28a3050a73a",
        "hash_full_prompts": "7b8d69ea2acaf2f7",
        "hash_input_tokens": "05a8ef0dd10b4bba",
        "hash_cont_tokens": "c8213c5d3ad7e502"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:professional_psychology|5": {
      "hashes": {
        "hash_examples": "bf5254fe818356af",
        "hash_full_prompts": "fe8937e9ffc99771",
        "hash_input_tokens": "62b75e5af561a6ea",
        "hash_cont_tokens": "5f698bd276f5db85"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2444,
      "non_padded": 4,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:public_relations|5": {
      "hashes": {
        "hash_examples": "b66d52e28e7d14e0",
        "hash_full_prompts": "d9f57f1d86c072b5",
        "hash_input_tokens": "cb9ec4357f30b41f",
        "hash_cont_tokens": "ca79966b90cda0ea"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 432,
      "non_padded": 8,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:security_studies|5": {
      "hashes": {
        "hash_examples": "514c14feaf000ad9",
        "hash_full_prompts": "869c9c3ae196b7c3",
        "hash_input_tokens": "3bad229573ed6a9c",
        "hash_cont_tokens": "3f72075ccbad1d9d"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:sociology|5": {
      "hashes": {
        "hash_examples": "f6c9bc9d18c80870",
        "hash_full_prompts": "9c616231ea800efa",
        "hash_input_tokens": "c3fd3e60d4040293",
        "hash_cont_tokens": "3e8b847ab6273dfe"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 784,
      "non_padded": 20,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:us_foreign_policy|5": {
      "hashes": {
        "hash_examples": "ed7b78629db6678f",
        "hash_full_prompts": "0c7a7081c71c07b6",
        "hash_input_tokens": "9be8b08435db8e88",
        "hash_cont_tokens": "74c639e56bb475af"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 388,
      "non_padded": 12,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:virology|5": {
      "hashes": {
        "hash_examples": "bc52ffdc3f9b994a",
        "hash_full_prompts": "01e95325d8b738e4",
        "hash_input_tokens": "31406452e9730bde",
        "hash_cont_tokens": "0065c4bbe6134c1c"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 664,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|mmlu:world_religions|5": {
      "hashes": {
        "hash_examples": "ecdb4a4f94f62930",
        "hash_full_prompts": "be6544f84d16d74b",
        "hash_input_tokens": "7ed628ee80ce8bc7",
        "hash_cont_tokens": "ffd72b68545cd461"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|truthfulqa:mc|0": {
      "hashes": {
        "hash_examples": "36a6d90e75d92d4a",
        "hash_full_prompts": "36a6d90e75d92d4a",
        "hash_input_tokens": "290dce847259c006",
        "hash_cont_tokens": "b9ef8afa5e3076d9"
      },
      "truncated": 0,
      "non_truncated": 817,
      "padded": 9996,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|winogrande|5": {
      "hashes": {
        "hash_examples": "087d5d1a1afd4c7b",
        "hash_full_prompts": "e0bb3cac43f294b2",
        "hash_input_tokens": "892a7af0525264ba",
        "hash_cont_tokens": "aba48a298be51304"
      },
      "truncated": 0,
      "non_truncated": 1267,
      "padded": 2534,
      "non_padded": 0,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    },
    "leaderboard|gsm8k|5": {
      "hashes": {
        "hash_examples": "0ed016e24e7512fd",
        "hash_full_prompts": "41d55e83abc0e02d",
        "hash_input_tokens": "eef147c820b17188",
        "hash_cont_tokens": "6d01ff43137bb6dc"
      },
      "truncated": 1319,
      "non_truncated": 0,
      "padded": 1233,
      "non_padded": 86,
      "effective_few_shots": 5.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "670666fa3a90ce5d",
      "hash_full_prompts": "0d4e43c08decbb96",
      "hash_input_tokens": "6f991cf46487609d",
      "hash_cont_tokens": "85aa6a1e2a8af4de"
    },
    "truncated": 2811,
    "non_truncated": 25848,
    "padded": 113094,
    "non_padded": 1778,
    "num_truncated_few_shots": 0
  }
}