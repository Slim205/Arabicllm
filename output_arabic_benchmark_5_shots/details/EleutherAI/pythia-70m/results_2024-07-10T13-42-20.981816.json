{
  "config_general": {
    "lighteval_sha": "4d8c620733802aba964d87aafb12739c8f72bc7d",
    "num_fewshot_seeds": 1,
    "override_batch_size": 32,
    "max_samples": null,
    "job_id": "",
    "start_time": 9261.565811148,
    "end_time": 10470.966530665,
    "total_evaluation_time_secondes": "1209.4007195169997",
    "model_name": "EleutherAI/pythia-70m",
    "model_sha": "a39f36b100fe8a5377810d56c3f4789b9c53ac42",
    "model_dtype": "torch.float16",
    "model_size": "138.83 MB",
    "config": null
  },
  "results": {
    "community|acva:Algeria|1": {
      "acc_norm": 0.49743589743589745,
      "acc_norm_stderr": 0.03589743589743589
    },
    "community|acva:Ancient_Egypt|1": {
      "acc_norm": 0.9492063492063492,
      "acc_norm_stderr": 0.01239139518482262
    },
    "community|acva:Arab_Empire|1": {
      "acc_norm": 0.6528301886792452,
      "acc_norm_stderr": 0.02930010170554965
    },
    "community|acva:Arabic_Architecture|1": {
      "acc_norm": 0.4564102564102564,
      "acc_norm_stderr": 0.035761230969912135
    },
    "community|acva:Arabic_Art|1": {
      "acc_norm": 0.6358974358974359,
      "acc_norm_stderr": 0.03454653867786389
    },
    "community|acva:Arabic_Astronomy|1": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03581804596782233
    },
    "community|acva:Arabic_Calligraphy|1": {
      "acc_norm": 0.5215686274509804,
      "acc_norm_stderr": 0.031343587064005626
    },
    "community|acva:Arabic_Ceremony|1": {
      "acc_norm": 0.4810810810810811,
      "acc_norm_stderr": 0.036834092970087065
    },
    "community|acva:Arabic_Clothing|1": {
      "acc_norm": 0.49743589743589745,
      "acc_norm_stderr": 0.035897435897435895
    },
    "community|acva:Arabic_Culture|1": {
      "acc_norm": 0.7692307692307693,
      "acc_norm_stderr": 0.0302493752938313
    },
    "community|acva:Arabic_Food|1": {
      "acc_norm": 0.558974358974359,
      "acc_norm_stderr": 0.0356473293185358
    },
    "community|acva:Arabic_Funeral|1": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.050529115263991134
    },
    "community|acva:Arabic_Geography|1": {
      "acc_norm": 0.3931034482758621,
      "acc_norm_stderr": 0.040703290137070705
    },
    "community|acva:Arabic_History|1": {
      "acc_norm": 0.6923076923076923,
      "acc_norm_stderr": 0.03313653039774173
    },
    "community|acva:Arabic_Language_Origin|1": {
      "acc_norm": 0.45263157894736844,
      "acc_norm_stderr": 0.05133911377354486
    },
    "community|acva:Arabic_Literature|1": {
      "acc_norm": 0.4413793103448276,
      "acc_norm_stderr": 0.04137931034482757
    },
    "community|acva:Arabic_Math|1": {
      "acc_norm": 0.6974358974358974,
      "acc_norm_stderr": 0.03298070870085618
    },
    "community|acva:Arabic_Medicine|1": {
      "acc_norm": 0.46206896551724136,
      "acc_norm_stderr": 0.041546596717075474
    },
    "community|acva:Arabic_Music|1": {
      "acc_norm": 0.762589928057554,
      "acc_norm_stderr": 0.036220593237998276
    },
    "community|acva:Arabic_Ornament|1": {
      "acc_norm": 0.5282051282051282,
      "acc_norm_stderr": 0.035840746749208334
    },
    "community|acva:Arabic_Philosophy|1": {
      "acc_norm": 0.41379310344827586,
      "acc_norm_stderr": 0.04104269211806232
    },
    "community|acva:Arabic_Physics_and_Chemistry|1": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03581804596782232
    },
    "community|acva:Arabic_Wedding|1": {
      "acc_norm": 0.5897435897435898,
      "acc_norm_stderr": 0.03531493712326671
    },
    "community|acva:Bahrain|1": {
      "acc_norm": 0.6888888888888889,
      "acc_norm_stderr": 0.06979205927323111
    },
    "community|acva:Comoros|1": {
      "acc_norm": 0.6222222222222222,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Egypt_modern|1": {
      "acc_norm": 0.6842105263157895,
      "acc_norm_stderr": 0.04794350420740798
    },
    "community|acva:InfluenceFromAncientEgypt|1": {
      "acc_norm": 0.39487179487179486,
      "acc_norm_stderr": 0.035095456022620375
    },
    "community|acva:InfluenceFromByzantium|1": {
      "acc_norm": 0.7172413793103448,
      "acc_norm_stderr": 0.03752833958003337
    },
    "community|acva:InfluenceFromChina|1": {
      "acc_norm": 0.7333333333333333,
      "acc_norm_stderr": 0.0317493043641267
    },
    "community|acva:InfluenceFromGreece|1": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.03549871080367708
    },
    "community|acva:InfluenceFromIslam|1": {
      "acc_norm": 0.7034482758620689,
      "acc_norm_stderr": 0.03806142687309992
    },
    "community|acva:InfluenceFromPersia|1": {
      "acc_norm": 0.6971428571428572,
      "acc_norm_stderr": 0.03483414676585986
    },
    "community|acva:InfluenceFromRome|1": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.03549871080367708
    },
    "community|acva:Iraq|1": {
      "acc_norm": 0.49411764705882355,
      "acc_norm_stderr": 0.05455069703232772
    },
    "community|acva:Islam_Education|1": {
      "acc_norm": 0.4307692307692308,
      "acc_norm_stderr": 0.035552132520587615
    },
    "community|acva:Islam_branches_and_schools|1": {
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.03751612367420646
    },
    "community|acva:Islamic_law_system|1": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.035498710803677086
    },
    "community|acva:Jordan|1": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Kuwait|1": {
      "acc_norm": 0.7333333333333333,
      "acc_norm_stderr": 0.06666666666666668
    },
    "community|acva:Lebanon|1": {
      "acc_norm": 0.8222222222222222,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Libya|1": {
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.07491109582924914
    },
    "community|acva:Mauritania|1": {
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.07446027270295805
    },
    "community|acva:Mesopotamia_civilization|1": {
      "acc_norm": 0.4774193548387097,
      "acc_norm_stderr": 0.0402500394824441
    },
    "community|acva:Morocco|1": {
      "acc_norm": 0.7777777777777778,
      "acc_norm_stderr": 0.06267511942419628
    },
    "community|acva:Oman|1": {
      "acc_norm": 0.8222222222222222,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Palestine|1": {
      "acc_norm": 0.7529411764705882,
      "acc_norm_stderr": 0.047058823529411785
    },
    "community|acva:Qatar|1": {
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.07385489458759964
    },
    "community|acva:Saudi_Arabia|1": {
      "acc_norm": 0.6717948717948717,
      "acc_norm_stderr": 0.03371243782413707
    },
    "community|acva:Somalia|1": {
      "acc_norm": 0.6444444444444445,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Sudan|1": {
      "acc_norm": 0.6444444444444445,
      "acc_norm_stderr": 0.07216392363431012
    },
    "community|acva:Syria|1": {
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Tunisia|1": {
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.06666666666666667
    },
    "community|acva:United_Arab_Emirates|1": {
      "acc_norm": 0.7647058823529411,
      "acc_norm_stderr": 0.04628210543937907
    },
    "community|acva:Yemen|1": {
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.13333333333333333
    },
    "community|acva:communication|1": {
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.025974025974025955
    },
    "community|acva:computer_and_phone|1": {
      "acc_norm": 0.45084745762711864,
      "acc_norm_stderr": 0.02901934773187137
    },
    "community|acva:daily_life|1": {
      "acc_norm": 0.8130563798219584,
      "acc_norm_stderr": 0.021268948348414647
    },
    "community|acva:entertainment|1": {
      "acc_norm": 0.7661016949152543,
      "acc_norm_stderr": 0.024687839412166384
    },
    "community|alghafa:mcq_exams_test_ar|1": {
      "acc_norm": 0.2495511669658887,
      "acc_norm_stderr": 0.018352826122316646
    },
    "community|alghafa:meta_ar_dialects|1": {
      "acc_norm": 0.2515291936978684,
      "acc_norm_stderr": 0.005907805122663002
    },
    "community|alghafa:meta_ar_msa|1": {
      "acc_norm": 0.2569832402234637,
      "acc_norm_stderr": 0.014614465821966356
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|1": {
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.05807730170189531
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|1": {
      "acc_norm": 0.38666666666666666,
      "acc_norm_stderr": 0.039895463700310406
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|1": {
      "acc_norm": 0.24666666666666667,
      "acc_norm_stderr": 0.03531471376356937
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|1": {
      "acc_norm": 0.5063164477798624,
      "acc_norm_stderr": 0.0055918211844666645
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|1": {
      "acc_norm": 0.3357798165137615,
      "acc_norm_stderr": 0.006099931506747819
    },
    "community|alghafa:multiple_choice_sentiment_task|1": {
      "acc_norm": 0.3308139534883721,
      "acc_norm_stderr": 0.011348211199147563
    },
    "community|arabic_exams|1": {
      "acc_norm": 0.23649906890130354,
      "acc_norm_stderr": 0.018354269670319875
    },
    "community|arabic_mmlu:abstract_algebra|1": {
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768081
    },
    "community|arabic_mmlu:anatomy|1": {
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.035914440841969694
    },
    "community|arabic_mmlu:astronomy|1": {
      "acc_norm": 0.3092105263157895,
      "acc_norm_stderr": 0.03761070869867479
    },
    "community|arabic_mmlu:business_ethics|1": {
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "community|arabic_mmlu:clinical_knowledge|1": {
      "acc_norm": 0.21509433962264152,
      "acc_norm_stderr": 0.02528839450289137
    },
    "community|arabic_mmlu:college_biology|1": {
      "acc_norm": 0.20833333333333334,
      "acc_norm_stderr": 0.033961162058453336
    },
    "community|arabic_mmlu:college_chemistry|1": {
      "acc_norm": 0.41,
      "acc_norm_stderr": 0.049431107042371025
    },
    "community|arabic_mmlu:college_computer_science|1": {
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "community|arabic_mmlu:college_mathematics|1": {
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "community|arabic_mmlu:college_medicine|1": {
      "acc_norm": 0.20809248554913296,
      "acc_norm_stderr": 0.030952890217749874
    },
    "community|arabic_mmlu:college_physics|1": {
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237654
    },
    "community|arabic_mmlu:computer_security|1": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "community|arabic_mmlu:conceptual_physics|1": {
      "acc_norm": 0.26382978723404255,
      "acc_norm_stderr": 0.028809989854102973
    },
    "community|arabic_mmlu:econometrics|1": {
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.039994238792813365
    },
    "community|arabic_mmlu:electrical_engineering|1": {
      "acc_norm": 0.23448275862068965,
      "acc_norm_stderr": 0.035306258743465914
    },
    "community|arabic_mmlu:elementary_mathematics|1": {
      "acc_norm": 0.20899470899470898,
      "acc_norm_stderr": 0.02094048156533486
    },
    "community|arabic_mmlu:formal_logic|1": {
      "acc_norm": 0.20634920634920634,
      "acc_norm_stderr": 0.03619604524124248
    },
    "community|arabic_mmlu:global_facts|1": {
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "community|arabic_mmlu:high_school_biology|1": {
      "acc_norm": 0.1774193548387097,
      "acc_norm_stderr": 0.02173254068932927
    },
    "community|arabic_mmlu:high_school_chemistry|1": {
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.0316185633535861
    },
    "community|arabic_mmlu:high_school_computer_science|1": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "community|arabic_mmlu:high_school_european_history|1": {
      "acc_norm": 0.23030303030303031,
      "acc_norm_stderr": 0.03287666758603489
    },
    "community|arabic_mmlu:high_school_geography|1": {
      "acc_norm": 0.17676767676767677,
      "acc_norm_stderr": 0.027178752639044915
    },
    "community|arabic_mmlu:high_school_government_and_politics|1": {
      "acc_norm": 0.19689119170984457,
      "acc_norm_stderr": 0.028697873971860664
    },
    "community|arabic_mmlu:high_school_macroeconomics|1": {
      "acc_norm": 0.19743589743589743,
      "acc_norm_stderr": 0.020182646968674844
    },
    "community|arabic_mmlu:high_school_mathematics|1": {
      "acc_norm": 0.27037037037037037,
      "acc_norm_stderr": 0.027080372815145668
    },
    "community|arabic_mmlu:high_school_microeconomics|1": {
      "acc_norm": 0.2184873949579832,
      "acc_norm_stderr": 0.026841514322958924
    },
    "community|arabic_mmlu:high_school_physics|1": {
      "acc_norm": 0.33112582781456956,
      "acc_norm_stderr": 0.038425817186598696
    },
    "community|arabic_mmlu:high_school_psychology|1": {
      "acc_norm": 0.344954128440367,
      "acc_norm_stderr": 0.02038060540506697
    },
    "community|arabic_mmlu:high_school_statistics|1": {
      "acc_norm": 0.46296296296296297,
      "acc_norm_stderr": 0.03400603625538272
    },
    "community|arabic_mmlu:high_school_us_history|1": {
      "acc_norm": 0.22058823529411764,
      "acc_norm_stderr": 0.02910225438967408
    },
    "community|arabic_mmlu:high_school_world_history|1": {
      "acc_norm": 0.270042194092827,
      "acc_norm_stderr": 0.028900721906293426
    },
    "community|arabic_mmlu:human_aging|1": {
      "acc_norm": 0.23318385650224216,
      "acc_norm_stderr": 0.028380391147094727
    },
    "community|arabic_mmlu:human_sexuality|1": {
      "acc_norm": 0.2595419847328244,
      "acc_norm_stderr": 0.03844876139785271
    },
    "community|arabic_mmlu:international_law|1": {
      "acc_norm": 0.2396694214876033,
      "acc_norm_stderr": 0.03896878985070417
    },
    "community|arabic_mmlu:jurisprudence|1": {
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.042365112580946336
    },
    "community|arabic_mmlu:logical_fallacies|1": {
      "acc_norm": 0.2331288343558282,
      "acc_norm_stderr": 0.033220157957767414
    },
    "community|arabic_mmlu:machine_learning|1": {
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.042878587513404544
    },
    "community|arabic_mmlu:management|1": {
      "acc_norm": 0.3786407766990291,
      "acc_norm_stderr": 0.04802694698258972
    },
    "community|arabic_mmlu:marketing|1": {
      "acc_norm": 0.2905982905982906,
      "acc_norm_stderr": 0.02974504857267404
    },
    "community|arabic_mmlu:medical_genetics|1": {
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909283
    },
    "community|arabic_mmlu:miscellaneous|1": {
      "acc_norm": 0.28735632183908044,
      "acc_norm_stderr": 0.0161824107306827
    },
    "community|arabic_mmlu:moral_disputes|1": {
      "acc_norm": 0.24855491329479767,
      "acc_norm_stderr": 0.023267528432100174
    },
    "community|arabic_mmlu:moral_scenarios|1": {
      "acc_norm": 0.24022346368715083,
      "acc_norm_stderr": 0.01428834380392529
    },
    "community|arabic_mmlu:nutrition|1": {
      "acc_norm": 0.26143790849673204,
      "acc_norm_stderr": 0.025160998214292456
    },
    "community|arabic_mmlu:philosophy|1": {
      "acc_norm": 0.2315112540192926,
      "acc_norm_stderr": 0.023956532766639133
    },
    "community|arabic_mmlu:prehistory|1": {
      "acc_norm": 0.22839506172839505,
      "acc_norm_stderr": 0.023358211840626267
    },
    "community|arabic_mmlu:professional_accounting|1": {
      "acc_norm": 0.24822695035460993,
      "acc_norm_stderr": 0.025770015644290392
    },
    "community|arabic_mmlu:professional_law|1": {
      "acc_norm": 0.242503259452412,
      "acc_norm_stderr": 0.010946570966348797
    },
    "community|arabic_mmlu:professional_medicine|1": {
      "acc_norm": 0.4522058823529412,
      "acc_norm_stderr": 0.030233758551596452
    },
    "community|arabic_mmlu:professional_psychology|1": {
      "acc_norm": 0.20261437908496732,
      "acc_norm_stderr": 0.01626105528374613
    },
    "community|arabic_mmlu:public_relations|1": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.03831305140884601
    },
    "community|arabic_mmlu:security_studies|1": {
      "acc_norm": 0.17551020408163265,
      "acc_norm_stderr": 0.024352800722970015
    },
    "community|arabic_mmlu:sociology|1": {
      "acc_norm": 0.23880597014925373,
      "acc_norm_stderr": 0.030147775935409217
    },
    "community|arabic_mmlu:us_foreign_policy|1": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "community|arabic_mmlu:virology|1": {
      "acc_norm": 0.28313253012048195,
      "acc_norm_stderr": 0.03507295431370518
    },
    "community|arabic_mmlu:world_religions|1": {
      "acc_norm": 0.2046783625730994,
      "acc_norm_stderr": 0.030944459778533214
    },
    "community|arc_challenge_okapi_ar|1": {
      "acc_norm": 0.26120689655172413,
      "acc_norm_stderr": 0.012903633459892306
    },
    "community|arc_easy_ar|1": {
      "acc_norm": 0.2533840947546531,
      "acc_norm_stderr": 0.00894759736674221
    },
    "community|boolq_ar|1": {
      "acc_norm": 0.39355828220858896,
      "acc_norm_stderr": 0.0085576965481235
    },
    "community|copa_ext_ar|1": {
      "acc_norm": 0.4111111111111111,
      "acc_norm_stderr": 0.05215564061107555
    },
    "community|hellaswag_okapi_ar|1": {
      "acc_norm": 0.24272162250572457,
      "acc_norm_stderr": 0.004477105658675385
    },
    "community|mmlu_okapi_ar|1": {
      "acc_norm": 0.2627872784957053,
      "acc_norm_stderr": 0.0038719853969829774
    },
    "community|openbook_qa_ext_ar|1": {
      "acc_norm": 0.31313131313131315,
      "acc_norm_stderr": 0.02086587657877696
    },
    "community|piqa_ar|1": {
      "acc_norm": 0.5002727768685216,
      "acc_norm_stderr": 0.011681731099489208
    },
    "community|race_ar|1": {
      "acc_norm": 0.28058429701765064,
      "acc_norm_stderr": 0.006400093046781666
    },
    "community|sciq_ar|1": {
      "acc_norm": 0.3316582914572864,
      "acc_norm_stderr": 0.01493315316824283
    },
    "community|toxigen_ar|1": {
      "acc_norm": 0.4320855614973262,
      "acc_norm_stderr": 0.01620887578524445
    },
    "lighteval|xstory_cloze:ar|0": {
      "acc": 0.46790205162144277,
      "acc_stderr": 0.01284058450398202
    },
    "community|acva:_average|1": {
      "acc_norm": 0.5920880465921851,
      "acc_norm_stderr": 0.0457639218017572
    },
    "community|alghafa:_average|1": {
      "acc_norm": 0.3382563502225056,
      "acc_norm_stderr": 0.021689171124787014
    },
    "community|arabic_mmlu:_average|1": {
      "acc_norm": 0.2600380238759158,
      "acc_norm_stderr": 0.03264666166619226
    },
    "all": {
      "acc_norm": 0.41269545448365713,
      "acc_norm_stderr": 0.03595387777932578,
      "acc": 0.46790205162144277,
      "acc_stderr": 0.01284058450398202
    }
  },
  "versions": {
    "community|acva:Algeria|1": 0,
    "community|acva:Ancient_Egypt|1": 0,
    "community|acva:Arab_Empire|1": 0,
    "community|acva:Arabic_Architecture|1": 0,
    "community|acva:Arabic_Art|1": 0,
    "community|acva:Arabic_Astronomy|1": 0,
    "community|acva:Arabic_Calligraphy|1": 0,
    "community|acva:Arabic_Ceremony|1": 0,
    "community|acva:Arabic_Clothing|1": 0,
    "community|acva:Arabic_Culture|1": 0,
    "community|acva:Arabic_Food|1": 0,
    "community|acva:Arabic_Funeral|1": 0,
    "community|acva:Arabic_Geography|1": 0,
    "community|acva:Arabic_History|1": 0,
    "community|acva:Arabic_Language_Origin|1": 0,
    "community|acva:Arabic_Literature|1": 0,
    "community|acva:Arabic_Math|1": 0,
    "community|acva:Arabic_Medicine|1": 0,
    "community|acva:Arabic_Music|1": 0,
    "community|acva:Arabic_Ornament|1": 0,
    "community|acva:Arabic_Philosophy|1": 0,
    "community|acva:Arabic_Physics_and_Chemistry|1": 0,
    "community|acva:Arabic_Wedding|1": 0,
    "community|acva:Bahrain|1": 0,
    "community|acva:Comoros|1": 0,
    "community|acva:Egypt_modern|1": 0,
    "community|acva:InfluenceFromAncientEgypt|1": 0,
    "community|acva:InfluenceFromByzantium|1": 0,
    "community|acva:InfluenceFromChina|1": 0,
    "community|acva:InfluenceFromGreece|1": 0,
    "community|acva:InfluenceFromIslam|1": 0,
    "community|acva:InfluenceFromPersia|1": 0,
    "community|acva:InfluenceFromRome|1": 0,
    "community|acva:Iraq|1": 0,
    "community|acva:Islam_Education|1": 0,
    "community|acva:Islam_branches_and_schools|1": 0,
    "community|acva:Islamic_law_system|1": 0,
    "community|acva:Jordan|1": 0,
    "community|acva:Kuwait|1": 0,
    "community|acva:Lebanon|1": 0,
    "community|acva:Libya|1": 0,
    "community|acva:Mauritania|1": 0,
    "community|acva:Mesopotamia_civilization|1": 0,
    "community|acva:Morocco|1": 0,
    "community|acva:Oman|1": 0,
    "community|acva:Palestine|1": 0,
    "community|acva:Qatar|1": 0,
    "community|acva:Saudi_Arabia|1": 0,
    "community|acva:Somalia|1": 0,
    "community|acva:Sudan|1": 0,
    "community|acva:Syria|1": 0,
    "community|acva:Tunisia|1": 0,
    "community|acva:United_Arab_Emirates|1": 0,
    "community|acva:Yemen|1": 0,
    "community|acva:communication|1": 0,
    "community|acva:computer_and_phone|1": 0,
    "community|acva:daily_life|1": 0,
    "community|acva:entertainment|1": 0,
    "community|alghafa:mcq_exams_test_ar|1": 0,
    "community|alghafa:meta_ar_dialects|1": 0,
    "community|alghafa:meta_ar_msa|1": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|1": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|1": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|1": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|1": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|1": 0,
    "community|alghafa:multiple_choice_sentiment_task|1": 0,
    "community|arabic_exams|1": 0,
    "community|arabic_mmlu:abstract_algebra|1": 0,
    "community|arabic_mmlu:anatomy|1": 0,
    "community|arabic_mmlu:astronomy|1": 0,
    "community|arabic_mmlu:business_ethics|1": 0,
    "community|arabic_mmlu:clinical_knowledge|1": 0,
    "community|arabic_mmlu:college_biology|1": 0,
    "community|arabic_mmlu:college_chemistry|1": 0,
    "community|arabic_mmlu:college_computer_science|1": 0,
    "community|arabic_mmlu:college_mathematics|1": 0,
    "community|arabic_mmlu:college_medicine|1": 0,
    "community|arabic_mmlu:college_physics|1": 0,
    "community|arabic_mmlu:computer_security|1": 0,
    "community|arabic_mmlu:conceptual_physics|1": 0,
    "community|arabic_mmlu:econometrics|1": 0,
    "community|arabic_mmlu:electrical_engineering|1": 0,
    "community|arabic_mmlu:elementary_mathematics|1": 0,
    "community|arabic_mmlu:formal_logic|1": 0,
    "community|arabic_mmlu:global_facts|1": 0,
    "community|arabic_mmlu:high_school_biology|1": 0,
    "community|arabic_mmlu:high_school_chemistry|1": 0,
    "community|arabic_mmlu:high_school_computer_science|1": 0,
    "community|arabic_mmlu:high_school_european_history|1": 0,
    "community|arabic_mmlu:high_school_geography|1": 0,
    "community|arabic_mmlu:high_school_government_and_politics|1": 0,
    "community|arabic_mmlu:high_school_macroeconomics|1": 0,
    "community|arabic_mmlu:high_school_mathematics|1": 0,
    "community|arabic_mmlu:high_school_microeconomics|1": 0,
    "community|arabic_mmlu:high_school_physics|1": 0,
    "community|arabic_mmlu:high_school_psychology|1": 0,
    "community|arabic_mmlu:high_school_statistics|1": 0,
    "community|arabic_mmlu:high_school_us_history|1": 0,
    "community|arabic_mmlu:high_school_world_history|1": 0,
    "community|arabic_mmlu:human_aging|1": 0,
    "community|arabic_mmlu:human_sexuality|1": 0,
    "community|arabic_mmlu:international_law|1": 0,
    "community|arabic_mmlu:jurisprudence|1": 0,
    "community|arabic_mmlu:logical_fallacies|1": 0,
    "community|arabic_mmlu:machine_learning|1": 0,
    "community|arabic_mmlu:management|1": 0,
    "community|arabic_mmlu:marketing|1": 0,
    "community|arabic_mmlu:medical_genetics|1": 0,
    "community|arabic_mmlu:miscellaneous|1": 0,
    "community|arabic_mmlu:moral_disputes|1": 0,
    "community|arabic_mmlu:moral_scenarios|1": 0,
    "community|arabic_mmlu:nutrition|1": 0,
    "community|arabic_mmlu:philosophy|1": 0,
    "community|arabic_mmlu:prehistory|1": 0,
    "community|arabic_mmlu:professional_accounting|1": 0,
    "community|arabic_mmlu:professional_law|1": 0,
    "community|arabic_mmlu:professional_medicine|1": 0,
    "community|arabic_mmlu:professional_psychology|1": 0,
    "community|arabic_mmlu:public_relations|1": 0,
    "community|arabic_mmlu:security_studies|1": 0,
    "community|arabic_mmlu:sociology|1": 0,
    "community|arabic_mmlu:us_foreign_policy|1": 0,
    "community|arabic_mmlu:virology|1": 0,
    "community|arabic_mmlu:world_religions|1": 0,
    "community|arc_challenge_okapi_ar|1": 0,
    "community|arc_easy_ar|1": 0,
    "community|boolq_ar|1": 0,
    "community|copa_ext_ar|1": 0,
    "community|hellaswag_okapi_ar|1": 0,
    "community|mmlu_okapi_ar|1": 0,
    "community|openbook_qa_ext_ar|1": 0,
    "community|piqa_ar|1": 0,
    "community|race_ar|1": 0,
    "community|sciq_ar|1": 0,
    "community|toxigen_ar|1": 0,
    "lighteval|xstory_cloze:ar|0": 0
  },
  "config_tasks": {
    "community|acva:Algeria": {
      "name": "acva:Algeria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Algeria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Ancient_Egypt": {
      "name": "acva:Ancient_Egypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Ancient_Egypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 315,
      "effective_num_docs": 315,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arab_Empire": {
      "name": "acva:Arab_Empire",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arab_Empire",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Architecture": {
      "name": "acva:Arabic_Architecture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Architecture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Art": {
      "name": "acva:Arabic_Art",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Art",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Astronomy": {
      "name": "acva:Arabic_Astronomy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Calligraphy": {
      "name": "acva:Arabic_Calligraphy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Calligraphy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 255,
      "effective_num_docs": 255,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ceremony": {
      "name": "acva:Arabic_Ceremony",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ceremony",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 185,
      "effective_num_docs": 185,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Clothing": {
      "name": "acva:Arabic_Clothing",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Clothing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Culture": {
      "name": "acva:Arabic_Culture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Culture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Food": {
      "name": "acva:Arabic_Food",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Food",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Funeral": {
      "name": "acva:Arabic_Funeral",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Funeral",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Geography": {
      "name": "acva:Arabic_Geography",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_History": {
      "name": "acva:Arabic_History",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_History",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Language_Origin": {
      "name": "acva:Arabic_Language_Origin",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Language_Origin",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Literature": {
      "name": "acva:Arabic_Literature",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Literature",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Math": {
      "name": "acva:Arabic_Math",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Math",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Medicine": {
      "name": "acva:Arabic_Medicine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Music": {
      "name": "acva:Arabic_Music",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Music",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 139,
      "effective_num_docs": 139,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ornament": {
      "name": "acva:Arabic_Ornament",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ornament",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Philosophy": {
      "name": "acva:Arabic_Philosophy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry": {
      "name": "acva:Arabic_Physics_and_Chemistry",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Physics_and_Chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Wedding": {
      "name": "acva:Arabic_Wedding",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Wedding",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Bahrain": {
      "name": "acva:Bahrain",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Bahrain",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Comoros": {
      "name": "acva:Comoros",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Comoros",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Egypt_modern": {
      "name": "acva:Egypt_modern",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Egypt_modern",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromAncientEgypt": {
      "name": "acva:InfluenceFromAncientEgypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromAncientEgypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromByzantium": {
      "name": "acva:InfluenceFromByzantium",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromByzantium",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromChina": {
      "name": "acva:InfluenceFromChina",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromChina",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromGreece": {
      "name": "acva:InfluenceFromGreece",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromGreece",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromIslam": {
      "name": "acva:InfluenceFromIslam",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromIslam",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromPersia": {
      "name": "acva:InfluenceFromPersia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromPersia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromRome": {
      "name": "acva:InfluenceFromRome",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromRome",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Iraq": {
      "name": "acva:Iraq",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Iraq",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_Education": {
      "name": "acva:Islam_Education",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_Education",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_branches_and_schools": {
      "name": "acva:Islam_branches_and_schools",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_branches_and_schools",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islamic_law_system": {
      "name": "acva:Islamic_law_system",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islamic_law_system",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Jordan": {
      "name": "acva:Jordan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Jordan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Kuwait": {
      "name": "acva:Kuwait",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Kuwait",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Lebanon": {
      "name": "acva:Lebanon",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Lebanon",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Libya": {
      "name": "acva:Libya",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Libya",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mauritania": {
      "name": "acva:Mauritania",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mauritania",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mesopotamia_civilization": {
      "name": "acva:Mesopotamia_civilization",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mesopotamia_civilization",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 155,
      "effective_num_docs": 155,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Morocco": {
      "name": "acva:Morocco",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Morocco",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Oman": {
      "name": "acva:Oman",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Oman",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Palestine": {
      "name": "acva:Palestine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Palestine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Qatar": {
      "name": "acva:Qatar",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Qatar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Saudi_Arabia": {
      "name": "acva:Saudi_Arabia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Saudi_Arabia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Somalia": {
      "name": "acva:Somalia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Somalia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Sudan": {
      "name": "acva:Sudan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Sudan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Syria": {
      "name": "acva:Syria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Syria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Tunisia": {
      "name": "acva:Tunisia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Tunisia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:United_Arab_Emirates": {
      "name": "acva:United_Arab_Emirates",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "United_Arab_Emirates",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Yemen": {
      "name": "acva:Yemen",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Yemen",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 10,
      "effective_num_docs": 10,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:communication": {
      "name": "acva:communication",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "communication",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 364,
      "effective_num_docs": 364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:computer_and_phone": {
      "name": "acva:computer_and_phone",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "computer_and_phone",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:daily_life": {
      "name": "acva:daily_life",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "daily_life",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 337,
      "effective_num_docs": 337,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:entertainment": {
      "name": "acva:entertainment",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "entertainment",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_exams": {
      "name": "arabic_exams",
      "prompt_function": "arabic_exams",
      "hf_repo": "OALL/Arabic_EXAMS",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 537,
      "effective_num_docs": 537,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:abstract_algebra": {
      "name": "arabic_mmlu:abstract_algebra",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:anatomy": {
      "name": "arabic_mmlu:anatomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:astronomy": {
      "name": "arabic_mmlu:astronomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:business_ethics": {
      "name": "arabic_mmlu:business_ethics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:clinical_knowledge": {
      "name": "arabic_mmlu:clinical_knowledge",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_biology": {
      "name": "arabic_mmlu:college_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_chemistry": {
      "name": "arabic_mmlu:college_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_computer_science": {
      "name": "arabic_mmlu:college_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_mathematics": {
      "name": "arabic_mmlu:college_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_medicine": {
      "name": "arabic_mmlu:college_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_physics": {
      "name": "arabic_mmlu:college_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:computer_security": {
      "name": "arabic_mmlu:computer_security",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:conceptual_physics": {
      "name": "arabic_mmlu:conceptual_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:econometrics": {
      "name": "arabic_mmlu:econometrics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:electrical_engineering": {
      "name": "arabic_mmlu:electrical_engineering",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:elementary_mathematics": {
      "name": "arabic_mmlu:elementary_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:formal_logic": {
      "name": "arabic_mmlu:formal_logic",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:global_facts": {
      "name": "arabic_mmlu:global_facts",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_biology": {
      "name": "arabic_mmlu:high_school_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_chemistry": {
      "name": "arabic_mmlu:high_school_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_computer_science": {
      "name": "arabic_mmlu:high_school_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_european_history": {
      "name": "arabic_mmlu:high_school_european_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_geography": {
      "name": "arabic_mmlu:high_school_geography",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics": {
      "name": "arabic_mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics": {
      "name": "arabic_mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_mathematics": {
      "name": "arabic_mmlu:high_school_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_microeconomics": {
      "name": "arabic_mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_physics": {
      "name": "arabic_mmlu:high_school_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_psychology": {
      "name": "arabic_mmlu:high_school_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_statistics": {
      "name": "arabic_mmlu:high_school_statistics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_us_history": {
      "name": "arabic_mmlu:high_school_us_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_world_history": {
      "name": "arabic_mmlu:high_school_world_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_aging": {
      "name": "arabic_mmlu:human_aging",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_sexuality": {
      "name": "arabic_mmlu:human_sexuality",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:international_law": {
      "name": "arabic_mmlu:international_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:jurisprudence": {
      "name": "arabic_mmlu:jurisprudence",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:logical_fallacies": {
      "name": "arabic_mmlu:logical_fallacies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:machine_learning": {
      "name": "arabic_mmlu:machine_learning",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:management": {
      "name": "arabic_mmlu:management",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:marketing": {
      "name": "arabic_mmlu:marketing",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:medical_genetics": {
      "name": "arabic_mmlu:medical_genetics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:miscellaneous": {
      "name": "arabic_mmlu:miscellaneous",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_disputes": {
      "name": "arabic_mmlu:moral_disputes",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_scenarios": {
      "name": "arabic_mmlu:moral_scenarios",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:nutrition": {
      "name": "arabic_mmlu:nutrition",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:philosophy": {
      "name": "arabic_mmlu:philosophy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:prehistory": {
      "name": "arabic_mmlu:prehistory",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_accounting": {
      "name": "arabic_mmlu:professional_accounting",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_law": {
      "name": "arabic_mmlu:professional_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_medicine": {
      "name": "arabic_mmlu:professional_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_psychology": {
      "name": "arabic_mmlu:professional_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:public_relations": {
      "name": "arabic_mmlu:public_relations",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:security_studies": {
      "name": "arabic_mmlu:security_studies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:sociology": {
      "name": "arabic_mmlu:sociology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:us_foreign_policy": {
      "name": "arabic_mmlu:us_foreign_policy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:virology": {
      "name": "arabic_mmlu:virology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:world_religions": {
      "name": "arabic_mmlu:world_religions",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_challenge_okapi_ar": {
      "name": "arc_challenge_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_challenge_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1160,
      "effective_num_docs": 1160,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_easy_ar": {
      "name": "arc_easy_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_easy_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 2364,
      "effective_num_docs": 2364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|copa_ext_ar": {
      "name": "copa_ext_ar",
      "prompt_function": "copa_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "copa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 90,
      "effective_num_docs": 90,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|mmlu_okapi_ar": {
      "name": "mmlu_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "mmlu_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 12923,
      "effective_num_docs": 12923,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|openbook_qa_ext_ar": {
      "name": "openbook_qa_ext_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "openbook_qa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 495,
      "effective_num_docs": 495,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|piqa_ar": {
      "name": "piqa_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "piqa_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1833,
      "effective_num_docs": 1833,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|sciq_ar": {
      "name": "sciq_ar",
      "prompt_function": "sciq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "sciq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 995,
      "effective_num_docs": 995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|toxigen_ar": {
      "name": "toxigen_ar",
      "prompt_function": "toxigen_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "toxigen_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 935,
      "effective_num_docs": 935,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "lighteval|xstory_cloze:ar": {
      "name": "xstory_cloze:ar",
      "prompt_function": "storycloze",
      "hf_repo": "juletxara/xstory_cloze",
      "hf_subset": "ar",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "training",
        "eval"
      ],
      "evaluation_splits": [
        "eval"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "lighteval"
      ],
      "original_num_docs": 1511,
      "effective_num_docs": 1511,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|acva:Algeria|1": {
      "hashes": {
        "hash_examples": "da5a3003cd46f6f9",
        "hash_full_prompts": "08450acdc5841823",
        "hash_input_tokens": "f542618c3555efdb",
        "hash_cont_tokens": "7d961eddf71b40d7"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Ancient_Egypt|1": {
      "hashes": {
        "hash_examples": "52d6f767fede195b",
        "hash_full_prompts": "bc077cf0b564e324",
        "hash_input_tokens": "779a8efe178bfa33",
        "hash_cont_tokens": "7b74e6317c1738b2"
      },
      "truncated": 0,
      "non_truncated": 315,
      "padded": 630,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arab_Empire|1": {
      "hashes": {
        "hash_examples": "8dacff6a79804a75",
        "hash_full_prompts": "88d81cacfeccdcc9",
        "hash_input_tokens": "4fc52642ddfb3c2f",
        "hash_cont_tokens": "c38a01ee63764ec9"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 530,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Architecture|1": {
      "hashes": {
        "hash_examples": "df286cd862d9f6bb",
        "hash_full_prompts": "f0b18f51b7336861",
        "hash_input_tokens": "24051d034c9a8f43",
        "hash_cont_tokens": "107307c7c6058283"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Art|1": {
      "hashes": {
        "hash_examples": "112883d764118a49",
        "hash_full_prompts": "bcdef7e94a709476",
        "hash_input_tokens": "2a976ab7cd067da3",
        "hash_cont_tokens": "dae1164dca03ed89"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Astronomy|1": {
      "hashes": {
        "hash_examples": "20dcdf2454bf8671",
        "hash_full_prompts": "c3cca77143784e93",
        "hash_input_tokens": "8a8d188b48681637",
        "hash_cont_tokens": "6d32ff4bc7a8111e"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Calligraphy|1": {
      "hashes": {
        "hash_examples": "3a9f9d1ebe868a15",
        "hash_full_prompts": "6b96112dc339e94c",
        "hash_input_tokens": "97ab5976027d27e4",
        "hash_cont_tokens": "43733169225ac13f"
      },
      "truncated": 0,
      "non_truncated": 255,
      "padded": 510,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ceremony|1": {
      "hashes": {
        "hash_examples": "c927630f8d2f44da",
        "hash_full_prompts": "924572f13c908843",
        "hash_input_tokens": "2cab5bd5cf76aa9e",
        "hash_cont_tokens": "a66d4f3f9224ff06"
      },
      "truncated": 0,
      "non_truncated": 185,
      "padded": 370,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Clothing|1": {
      "hashes": {
        "hash_examples": "6ad0740c2ac6ac92",
        "hash_full_prompts": "87de2bfc01e66007",
        "hash_input_tokens": "c29beab801057887",
        "hash_cont_tokens": "7ddb375a72e0b801"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Culture|1": {
      "hashes": {
        "hash_examples": "2177bd857ad872ae",
        "hash_full_prompts": "aabe9c561a7ad8e9",
        "hash_input_tokens": "211bef2e63d855d8",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Food|1": {
      "hashes": {
        "hash_examples": "a6ada65b71d7c9c5",
        "hash_full_prompts": "04e6db147eef3521",
        "hash_input_tokens": "19f6976c155d6617",
        "hash_cont_tokens": "e6978584df694ee0"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Funeral|1": {
      "hashes": {
        "hash_examples": "fcee39dc29eaae91",
        "hash_full_prompts": "5d0ccd0f8ff46a7a",
        "hash_input_tokens": "b20905c4eb21e1c9",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Geography|1": {
      "hashes": {
        "hash_examples": "d36eda7c89231c02",
        "hash_full_prompts": "2e0f3aac2b8eafc0",
        "hash_input_tokens": "4938fbd54abc19f2",
        "hash_cont_tokens": "7c7d83776a2e8a54"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_History|1": {
      "hashes": {
        "hash_examples": "6354ac0d6db6a5fc",
        "hash_full_prompts": "bca9a521a42b0e9f",
        "hash_input_tokens": "56820db27076e155",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Language_Origin|1": {
      "hashes": {
        "hash_examples": "ddc967c8aca34402",
        "hash_full_prompts": "e19160e79b81d098",
        "hash_input_tokens": "60f1719055fc6fe7",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Literature|1": {
      "hashes": {
        "hash_examples": "4305379fd46be5d8",
        "hash_full_prompts": "2c24096b5043f7c1",
        "hash_input_tokens": "4a7f4a4c89d69e88",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Math|1": {
      "hashes": {
        "hash_examples": "dec621144f4d28be",
        "hash_full_prompts": "196b1a12d0b75597",
        "hash_input_tokens": "861169edea34fa8e",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Medicine|1": {
      "hashes": {
        "hash_examples": "2b344cdae9495ff2",
        "hash_full_prompts": "2b7180e41f078e37",
        "hash_input_tokens": "03d107e15fd90f2d",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Music|1": {
      "hashes": {
        "hash_examples": "0c54624d881944ce",
        "hash_full_prompts": "1fdc61c72bf434ce",
        "hash_input_tokens": "402b6a11343d4847",
        "hash_cont_tokens": "5aa58462a622ebdd"
      },
      "truncated": 0,
      "non_truncated": 139,
      "padded": 278,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ornament|1": {
      "hashes": {
        "hash_examples": "251a4a84289d8bc1",
        "hash_full_prompts": "388d99de656df132",
        "hash_input_tokens": "08f81dadb6dd8cf0",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Philosophy|1": {
      "hashes": {
        "hash_examples": "3f86fb9c94c13d22",
        "hash_full_prompts": "6e3e324a61f21dcd",
        "hash_input_tokens": "a91a260294fc9ece",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry|1": {
      "hashes": {
        "hash_examples": "8fec65af3695b62a",
        "hash_full_prompts": "6b57779714254862",
        "hash_input_tokens": "aff5a64e2c6d266c",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Wedding|1": {
      "hashes": {
        "hash_examples": "9cc3477184d7a4b8",
        "hash_full_prompts": "3587ebc8a10d6b80",
        "hash_input_tokens": "1a58cbfce5ee372d",
        "hash_cont_tokens": "80970b3f0d64d4ad"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Bahrain|1": {
      "hashes": {
        "hash_examples": "c92e803a0fa8b9e2",
        "hash_full_prompts": "5cfc9b7aa371a11b",
        "hash_input_tokens": "76012b32e6909cda",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Comoros|1": {
      "hashes": {
        "hash_examples": "06e5d4bba8e54cae",
        "hash_full_prompts": "86ea16646c164959",
        "hash_input_tokens": "80707f64c3c9fe51",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Egypt_modern|1": {
      "hashes": {
        "hash_examples": "c6ec369164f93446",
        "hash_full_prompts": "a1694e3165eed385",
        "hash_input_tokens": "6f21375b200157a8",
        "hash_cont_tokens": "e56cf73e5245c88f"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromAncientEgypt|1": {
      "hashes": {
        "hash_examples": "b9d56d74818b9bd4",
        "hash_full_prompts": "891730fed5532065",
        "hash_input_tokens": "878b014975c8788f",
        "hash_cont_tokens": "f8eb731d255e856b"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromByzantium|1": {
      "hashes": {
        "hash_examples": "5316c9624e7e59b8",
        "hash_full_prompts": "e6a52f9607b44739",
        "hash_input_tokens": "2875790843b5e4e3",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromChina|1": {
      "hashes": {
        "hash_examples": "87894bce95a56411",
        "hash_full_prompts": "e7f8757aa6716328",
        "hash_input_tokens": "2d52fb567e3458b9",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromGreece|1": {
      "hashes": {
        "hash_examples": "0baa78a27e469312",
        "hash_full_prompts": "b13f064df257d4e6",
        "hash_input_tokens": "ab9a29bdd6ef18cb",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromIslam|1": {
      "hashes": {
        "hash_examples": "0c2532cde6541ff2",
        "hash_full_prompts": "30575e122523edc1",
        "hash_input_tokens": "b44eeec1b52f2264",
        "hash_cont_tokens": "9c5c207bee75308c"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromPersia|1": {
      "hashes": {
        "hash_examples": "efcd8112dc53c6e5",
        "hash_full_prompts": "11df2acd893ef7aa",
        "hash_input_tokens": "39132b3c7645b56e",
        "hash_cont_tokens": "62d62b3e9e6183e3"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromRome|1": {
      "hashes": {
        "hash_examples": "9db61480e2e85fd3",
        "hash_full_prompts": "67dc3f4a1c72a546",
        "hash_input_tokens": "aa4f4bb674a2d97e",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Iraq|1": {
      "hashes": {
        "hash_examples": "96dac3dfa8d2f41f",
        "hash_full_prompts": "acc8c53686c90951",
        "hash_input_tokens": "e05a4fecb04fb9ab",
        "hash_cont_tokens": "7e819f6d1ffda764"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_Education|1": {
      "hashes": {
        "hash_examples": "0d80355f6a4cb51b",
        "hash_full_prompts": "4c4de8372b4096b7",
        "hash_input_tokens": "df077237b98d5b6e",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_branches_and_schools|1": {
      "hashes": {
        "hash_examples": "5cedce1be2c3ad50",
        "hash_full_prompts": "6634aaeb7e0f8a21",
        "hash_input_tokens": "255b819eeea9e394",
        "hash_cont_tokens": "62d62b3e9e6183e3"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islamic_law_system|1": {
      "hashes": {
        "hash_examples": "c0e6db8bc84e105e",
        "hash_full_prompts": "670bad94a69bdfef",
        "hash_input_tokens": "6a8860cf70cf9046",
        "hash_cont_tokens": "f61a6e8fc4c366e5"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Jordan|1": {
      "hashes": {
        "hash_examples": "33deb5b4e5ddd6a1",
        "hash_full_prompts": "65b80dcaf71dd6bd",
        "hash_input_tokens": "8e56f8f859028d5c",
        "hash_cont_tokens": "ea990c1f365a0290"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Kuwait|1": {
      "hashes": {
        "hash_examples": "eb41773346d7c46c",
        "hash_full_prompts": "f093387dfb078957",
        "hash_input_tokens": "5d24d6ec1fcf49a1",
        "hash_cont_tokens": "ffb5342882ce748a"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Lebanon|1": {
      "hashes": {
        "hash_examples": "25932dbf4c13d34f",
        "hash_full_prompts": "7e3103bc9394e34e",
        "hash_input_tokens": "f1efd62e11a19e7a",
        "hash_cont_tokens": "038f73dee7aeb9e5"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Libya|1": {
      "hashes": {
        "hash_examples": "f2c4db63cd402926",
        "hash_full_prompts": "8b398908d3538e23",
        "hash_input_tokens": "b2e5c1bf8cf50e7b",
        "hash_cont_tokens": "d5c1fb53ebb02d4b"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mauritania|1": {
      "hashes": {
        "hash_examples": "8723ab5fdf286b54",
        "hash_full_prompts": "9e477a9903545d76",
        "hash_input_tokens": "b056b5cc97d79789",
        "hash_cont_tokens": "5450695d48507c85"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mesopotamia_civilization|1": {
      "hashes": {
        "hash_examples": "c33f5502a6130ca9",
        "hash_full_prompts": "2ee6ce0f65d7fe17",
        "hash_input_tokens": "f10373babbc1201a",
        "hash_cont_tokens": "383238edf5f51834"
      },
      "truncated": 0,
      "non_truncated": 155,
      "padded": 310,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Morocco|1": {
      "hashes": {
        "hash_examples": "588a5ed27904b1ae",
        "hash_full_prompts": "1175c5a2196c9767",
        "hash_input_tokens": "5deb198895054053",
        "hash_cont_tokens": "40e374174af85e23"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Oman|1": {
      "hashes": {
        "hash_examples": "d447c52b94248b69",
        "hash_full_prompts": "81795b29937debdc",
        "hash_input_tokens": "95d40adee2307ae4",
        "hash_cont_tokens": "65e8c58473172baf"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Palestine|1": {
      "hashes": {
        "hash_examples": "19197e076ad14ff5",
        "hash_full_prompts": "7dbddde3fcab446f",
        "hash_input_tokens": "7bb3878acb386401",
        "hash_cont_tokens": "11809798330c290e"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Qatar|1": {
      "hashes": {
        "hash_examples": "cf0736fa185b28f6",
        "hash_full_prompts": "96ad87838abdad3d",
        "hash_input_tokens": "177d6cf11e6d2494",
        "hash_cont_tokens": "3c8823cafa43ec19"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Saudi_Arabia|1": {
      "hashes": {
        "hash_examples": "69beda6e1b85a08d",
        "hash_full_prompts": "5c29247144aef66e",
        "hash_input_tokens": "b70b8f1916057519",
        "hash_cont_tokens": "21fb936d1a40e4e9"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Somalia|1": {
      "hashes": {
        "hash_examples": "b387940c65784fbf",
        "hash_full_prompts": "5d73ff2092d70a24",
        "hash_input_tokens": "e54da7381163edf2",
        "hash_cont_tokens": "490f9d16e0937e65"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Sudan|1": {
      "hashes": {
        "hash_examples": "e02c32b9d2dd0c3f",
        "hash_full_prompts": "8880cfd56c3715a1",
        "hash_input_tokens": "c2b86256ce63dd22",
        "hash_cont_tokens": "f17e1337bd897204"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Syria|1": {
      "hashes": {
        "hash_examples": "60a6f8fe73bda4bb",
        "hash_full_prompts": "a26b6e2caf0254bd",
        "hash_input_tokens": "d613658cd423c5e9",
        "hash_cont_tokens": "87c22f5ca5c0bb7f"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Tunisia|1": {
      "hashes": {
        "hash_examples": "34bb15d3830c5649",
        "hash_full_prompts": "5f6e5f5ec9eff8c2",
        "hash_input_tokens": "1693926875e1c68a",
        "hash_cont_tokens": "4c23081c9f577b44"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:United_Arab_Emirates|1": {
      "hashes": {
        "hash_examples": "98a0ba78172718ce",
        "hash_full_prompts": "cc09e39016efd2e0",
        "hash_input_tokens": "38131e8eac4fb592",
        "hash_cont_tokens": "92e8c3426277030e"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Yemen|1": {
      "hashes": {
        "hash_examples": "18e9bcccbb4ced7a",
        "hash_full_prompts": "bf7f7bcc47eb9d26",
        "hash_input_tokens": "67cb743199a7b91c",
        "hash_cont_tokens": "d023fe669a6468db"
      },
      "truncated": 0,
      "non_truncated": 10,
      "padded": 20,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:communication|1": {
      "hashes": {
        "hash_examples": "9ff28ab5eab5c97b",
        "hash_full_prompts": "1bb25a08a1800163",
        "hash_input_tokens": "9694f778005b116e",
        "hash_cont_tokens": "396b6e232d004221"
      },
      "truncated": 0,
      "non_truncated": 364,
      "padded": 728,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:computer_and_phone|1": {
      "hashes": {
        "hash_examples": "37bac2f086aaf6c2",
        "hash_full_prompts": "e3fe8bf62f11bffb",
        "hash_input_tokens": "2a93d9006bd55164",
        "hash_cont_tokens": "2ab016304f0b1bf3"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:daily_life|1": {
      "hashes": {
        "hash_examples": "bf07363c1c252e2f",
        "hash_full_prompts": "2c19e63a10100b3a",
        "hash_input_tokens": "b88f0f14944a6bc1",
        "hash_cont_tokens": "374371630123530d"
      },
      "truncated": 0,
      "non_truncated": 337,
      "padded": 674,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:entertainment|1": {
      "hashes": {
        "hash_examples": "37077bc00f0ac56a",
        "hash_full_prompts": "3ff9d7809edab3b7",
        "hash_input_tokens": "b952301fd5683b72",
        "hash_cont_tokens": "f6e7f114f81be966"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:mcq_exams_test_ar|1": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "59d73e35257a3717",
        "hash_input_tokens": "9c8a246fe1e3b3cc",
        "hash_cont_tokens": "a485d00ec726cd1c"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2228,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|1": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "337fe8af3a258863",
        "hash_input_tokens": "2ea8c17488013f07",
        "hash_cont_tokens": "4bf30245eaeeec8a"
      },
      "truncated": 0,
      "non_truncated": 5395,
      "padded": 21580,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_msa|1": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "aeee0fc46a35b919",
        "hash_input_tokens": "70e6df0f8595b6d5",
        "hash_cont_tokens": "e411d9cd3e154972"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|1": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "66a61f57cae5c3b6",
        "hash_input_tokens": "95c797b5e858efbc",
        "hash_cont_tokens": "9b5200174677484d"
      },
      "truncated": 0,
      "non_truncated": 75,
      "padded": 150,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|1": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "053a7c1d73c3424f",
        "hash_input_tokens": "e0caaa0d127ff741",
        "hash_cont_tokens": "7c233e7fc47b8216"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|1": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "92cf1c08cf87cf84",
        "hash_input_tokens": "0efa564c5daf8f50",
        "hash_cont_tokens": "d8e1e6d6cdbdd437"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|1": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "53ce2ee287edfe82",
        "hash_input_tokens": "24599f4f8a5adb9b",
        "hash_cont_tokens": "218ef75172427273"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|1": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "13c600b97e48884c",
        "hash_input_tokens": "eb8f2d4ddebf8726",
        "hash_cont_tokens": "d13aa1b14ff088f5"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17985,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|1": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "a66d3446949784af",
        "hash_input_tokens": "3d4ea9821a8809b1",
        "hash_cont_tokens": "395b1430c86fd68d"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5135,
      "non_padded": 25,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_exams|1": {
      "hashes": {
        "hash_examples": "6d721df351722656",
        "hash_full_prompts": "0bc438c037d2e250",
        "hash_input_tokens": "a597504298508d9a",
        "hash_cont_tokens": "7a63c945981c16b4"
      },
      "truncated": 0,
      "non_truncated": 537,
      "padded": 2120,
      "non_padded": 28,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:abstract_algebra|1": {
      "hashes": {
        "hash_examples": "f2ddca8f45c0a511",
        "hash_full_prompts": "1e3c8af2c9f8986c",
        "hash_input_tokens": "63e54464d18b1954",
        "hash_cont_tokens": "9df6c4cae45c0176"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 392,
      "non_padded": 8,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:anatomy|1": {
      "hashes": {
        "hash_examples": "dfdbc1b83107668d",
        "hash_full_prompts": "36ae7f64fb80f9a1",
        "hash_input_tokens": "c8ed115794d47769",
        "hash_cont_tokens": "ae040f9553602efe"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:astronomy|1": {
      "hashes": {
        "hash_examples": "9736a606002a848e",
        "hash_full_prompts": "75117e87b0b16e79",
        "hash_input_tokens": "7bd7000853f1bb7d",
        "hash_cont_tokens": "aa22f7eb15efe639"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:business_ethics|1": {
      "hashes": {
        "hash_examples": "735e452fbb6dc63d",
        "hash_full_prompts": "e968124a7d6f3479",
        "hash_input_tokens": "bfa46fa3aff613d0",
        "hash_cont_tokens": "0529aa082f3fef57"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 396,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:clinical_knowledge|1": {
      "hashes": {
        "hash_examples": "6ab0ca4da98aedcf",
        "hash_full_prompts": "d23d9a80992bca1e",
        "hash_input_tokens": "770e8614b45ecffb",
        "hash_cont_tokens": "1c31bb62d1b3ae41"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1056,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_biology|1": {
      "hashes": {
        "hash_examples": "17e4e390848018a4",
        "hash_full_prompts": "6cfb5f04d33ca41c",
        "hash_input_tokens": "32b9478a212e88b3",
        "hash_cont_tokens": "de0055b95ede8119"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_chemistry|1": {
      "hashes": {
        "hash_examples": "4abb169f6dfd234b",
        "hash_full_prompts": "fc942d8d9ee5d152",
        "hash_input_tokens": "bac7c54d2b9da9e5",
        "hash_cont_tokens": "ad954f63c8015ba2"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_computer_science|1": {
      "hashes": {
        "hash_examples": "a369e2e941358a1e",
        "hash_full_prompts": "7228519ad0444e13",
        "hash_input_tokens": "cf88f7c32b463db7",
        "hash_cont_tokens": "2c60fa6410c48205"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_mathematics|1": {
      "hashes": {
        "hash_examples": "d7be03b8b6020bff",
        "hash_full_prompts": "7f747fcb22aafda8",
        "hash_input_tokens": "2d5ff3ff7b54f483",
        "hash_cont_tokens": "59a28a7f7d3c85d2"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_medicine|1": {
      "hashes": {
        "hash_examples": "0518a00f097346bf",
        "hash_full_prompts": "97ac6b18ab4383b3",
        "hash_input_tokens": "6a36aa893ec2ad8c",
        "hash_cont_tokens": "a9195463488c7400"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 684,
      "non_padded": 8,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_physics|1": {
      "hashes": {
        "hash_examples": "5d842cd49bc70e12",
        "hash_full_prompts": "620f425715f816b2",
        "hash_input_tokens": "4df0ed67dcdaf0c5",
        "hash_cont_tokens": "ba0d16f194a1fbfe"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:computer_security|1": {
      "hashes": {
        "hash_examples": "8e85d9f85be9b32f",
        "hash_full_prompts": "97f284e0b4b07b5a",
        "hash_input_tokens": "f7a0243a411af37c",
        "hash_cont_tokens": "d2c0c50afe372db1"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:conceptual_physics|1": {
      "hashes": {
        "hash_examples": "7964b55a0a49502b",
        "hash_full_prompts": "e50f6154b894aba4",
        "hash_input_tokens": "c1d76cb88b94d736",
        "hash_cont_tokens": "1ec68f845885f123"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:econometrics|1": {
      "hashes": {
        "hash_examples": "1e192eae38347257",
        "hash_full_prompts": "80bde2c0c6811766",
        "hash_input_tokens": "526605a7ee5d504c",
        "hash_cont_tokens": "ba5a035eff09abfc"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:electrical_engineering|1": {
      "hashes": {
        "hash_examples": "cf97671d5c441da1",
        "hash_full_prompts": "98d4ebdaea490cd5",
        "hash_input_tokens": "028ce573e7a938cc",
        "hash_cont_tokens": "ba050975795e9020"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 576,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:elementary_mathematics|1": {
      "hashes": {
        "hash_examples": "6f49107ed43c40c5",
        "hash_full_prompts": "37f5c6cedd940ef3",
        "hash_input_tokens": "48c647526d30e7ff",
        "hash_cont_tokens": "1ea1fefb6a4c1ef7"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:formal_logic|1": {
      "hashes": {
        "hash_examples": "7922c376008ba77b",
        "hash_full_prompts": "1781475770b7500e",
        "hash_input_tokens": "28f252add49d5656",
        "hash_cont_tokens": "1acb80e94c977ef1"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:global_facts|1": {
      "hashes": {
        "hash_examples": "11f9813185047d5b",
        "hash_full_prompts": "8a094199c28eadb0",
        "hash_input_tokens": "20cb218c49f54c85",
        "hash_cont_tokens": "dff4f720e288eb41"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_biology|1": {
      "hashes": {
        "hash_examples": "2a804b1d90cbe66e",
        "hash_full_prompts": "fc6e911780654c6b",
        "hash_input_tokens": "a7dba0ef89f9151b",
        "hash_cont_tokens": "20bbf7d21acff832"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_chemistry|1": {
      "hashes": {
        "hash_examples": "0032168adabc53b4",
        "hash_full_prompts": "45700ffa835d08fc",
        "hash_input_tokens": "7197351ffd95cbf5",
        "hash_cont_tokens": "d4a64a2d8457e995"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 808,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_computer_science|1": {
      "hashes": {
        "hash_examples": "f2fb8740f9df980f",
        "hash_full_prompts": "246f4598b5daaa99",
        "hash_input_tokens": "f2c235469b3e10fc",
        "hash_cont_tokens": "df9bf37dac520461"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_european_history|1": {
      "hashes": {
        "hash_examples": "73509021e7e66435",
        "hash_full_prompts": "e0533d8af7e62846",
        "hash_input_tokens": "9b0a4a20bee29203",
        "hash_cont_tokens": "5e1cfc829edd9991"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_geography|1": {
      "hashes": {
        "hash_examples": "9e08d1894940ff42",
        "hash_full_prompts": "170cd4c49a2ade34",
        "hash_input_tokens": "638171d1fc5ca629",
        "hash_cont_tokens": "fa60b89a1a89dd5b"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 788,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics|1": {
      "hashes": {
        "hash_examples": "64b7e97817ca6c76",
        "hash_full_prompts": "22482ab07e708668",
        "hash_input_tokens": "4af309bd0a1a74a0",
        "hash_cont_tokens": "4fe7f8fa24db1bf6"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics|1": {
      "hashes": {
        "hash_examples": "9f582da8534bd2ef",
        "hash_full_prompts": "c680be1558d9f9bb",
        "hash_input_tokens": "1e1150be7c7b3b8c",
        "hash_cont_tokens": "b394928a00af119e"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_mathematics|1": {
      "hashes": {
        "hash_examples": "fd54f1c10d423c51",
        "hash_full_prompts": "66bd2c23db40cea4",
        "hash_input_tokens": "79450629ffbfccc6",
        "hash_cont_tokens": "149f4faa489a848a"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_microeconomics|1": {
      "hashes": {
        "hash_examples": "7037896925aaf42f",
        "hash_full_prompts": "861604b670fe6cb5",
        "hash_input_tokens": "c8a096308d3effe8",
        "hash_cont_tokens": "bdbcdae5cb7b799a"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_physics|1": {
      "hashes": {
        "hash_examples": "60c3776215167dae",
        "hash_full_prompts": "7e2392d536687071",
        "hash_input_tokens": "10800f1e90e774fa",
        "hash_cont_tokens": "626abb8646b370cc"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_psychology|1": {
      "hashes": {
        "hash_examples": "61176bfd5da1298f",
        "hash_full_prompts": "a437c64fb87d2e6f",
        "hash_input_tokens": "05440392ae0961d9",
        "hash_cont_tokens": "31245f38acbcdf42"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_statistics|1": {
      "hashes": {
        "hash_examples": "40dfeebd1ea10f76",
        "hash_full_prompts": "c1528c8524dfb8aa",
        "hash_input_tokens": "656a20b31497d9ab",
        "hash_cont_tokens": "b0e3beb4e48bf36f"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_us_history|1": {
      "hashes": {
        "hash_examples": "03daa510ba917f4d",
        "hash_full_prompts": "a2dd9473b12c827c",
        "hash_input_tokens": "6ebfe9eddcbcf5c7",
        "hash_cont_tokens": "fd957470c0337015"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_world_history|1": {
      "hashes": {
        "hash_examples": "be075ffd579f43c2",
        "hash_full_prompts": "702e81b8f2e745e4",
        "hash_input_tokens": "4cdf8dab6bd960d8",
        "hash_cont_tokens": "1fe43b60320dac2b"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.9957805907172996,
      "num_truncated_few_shots": 1
    },
    "community|arabic_mmlu:human_aging|1": {
      "hashes": {
        "hash_examples": "caa5b69f640bd1ef",
        "hash_full_prompts": "f36c27b9d1aa7d10",
        "hash_input_tokens": "4b2ed7b7f9c0d6f8",
        "hash_cont_tokens": "7c9e8328be3467cc"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_sexuality|1": {
      "hashes": {
        "hash_examples": "5ed2e38fb25a3767",
        "hash_full_prompts": "f879b1cca73b337b",
        "hash_input_tokens": "63fd5311ea202b11",
        "hash_cont_tokens": "08b4d96a2e33d9ca"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 516,
      "non_padded": 8,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:international_law|1": {
      "hashes": {
        "hash_examples": "4e3e9e28d1b96484",
        "hash_full_prompts": "f06220ff68d10c54",
        "hash_input_tokens": "c4679fbf0d68ae14",
        "hash_cont_tokens": "f61077d32cff27e4"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:jurisprudence|1": {
      "hashes": {
        "hash_examples": "e264b755366310b3",
        "hash_full_prompts": "2c8cf317e637a49b",
        "hash_input_tokens": "a14026a44418a906",
        "hash_cont_tokens": "2a87840a3af52122"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 432,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:logical_fallacies|1": {
      "hashes": {
        "hash_examples": "a4ab6965a3e38071",
        "hash_full_prompts": "4ee432714d085c3b",
        "hash_input_tokens": "62a7a661a4c06f64",
        "hash_cont_tokens": "095e4f2d6545e4bb"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 648,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:machine_learning|1": {
      "hashes": {
        "hash_examples": "b92320efa6636b40",
        "hash_full_prompts": "68895d83423b8cd5",
        "hash_input_tokens": "de9a1917bb6365e4",
        "hash_cont_tokens": "c7c8aadc6150bb39"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:management|1": {
      "hashes": {
        "hash_examples": "c9ee4872a850fe20",
        "hash_full_prompts": "9eb88d2fb9f3c9ea",
        "hash_input_tokens": "e711f9712705aa0f",
        "hash_cont_tokens": "29893f3eef89032e"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 408,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:marketing|1": {
      "hashes": {
        "hash_examples": "0c151b70f6a047e3",
        "hash_full_prompts": "5c93f8c15fcf840e",
        "hash_input_tokens": "7f47447c3d7017f6",
        "hash_cont_tokens": "499885eeebe9546a"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 924,
      "non_padded": 12,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:medical_genetics|1": {
      "hashes": {
        "hash_examples": "513f6cb8fca3a24e",
        "hash_full_prompts": "940c264c16a6b9b5",
        "hash_input_tokens": "2fa5989fb3d42586",
        "hash_cont_tokens": "9ea06f24c36722a0"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:miscellaneous|1": {
      "hashes": {
        "hash_examples": "259a190d635331db",
        "hash_full_prompts": "a2ae79156b160fa0",
        "hash_input_tokens": "22e26a771debd15f",
        "hash_cont_tokens": "0dca8000b41b0397"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3132,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_disputes|1": {
      "hashes": {
        "hash_examples": "b85052c48a0b7bc3",
        "hash_full_prompts": "98fc7f4603a8147e",
        "hash_input_tokens": "3d1f9e9e6da14017",
        "hash_cont_tokens": "2f88f2936953b564"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_scenarios|1": {
      "hashes": {
        "hash_examples": "28d0b069ef00dd00",
        "hash_full_prompts": "de374132b9bb9514",
        "hash_input_tokens": "759eb615a1fd5900",
        "hash_cont_tokens": "59be118dd2bf6ffb"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:nutrition|1": {
      "hashes": {
        "hash_examples": "00c9bc5f1d305b2f",
        "hash_full_prompts": "133dad09753a4fda",
        "hash_input_tokens": "5588f89d30eebcd7",
        "hash_cont_tokens": "4b7e8ab120a8d026"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1224,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:philosophy|1": {
      "hashes": {
        "hash_examples": "a458c08454a3fd5f",
        "hash_full_prompts": "9b9d0d2019545693",
        "hash_input_tokens": "8ef44774e79e68d5",
        "hash_cont_tokens": "283a2bc4c48d3022"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1236,
      "non_padded": 8,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:prehistory|1": {
      "hashes": {
        "hash_examples": "d6a0ecbdbb670e9c",
        "hash_full_prompts": "3353bd813e81559c",
        "hash_input_tokens": "85a05f32fd6b333b",
        "hash_cont_tokens": "92940cd49a524a02"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1296,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_accounting|1": {
      "hashes": {
        "hash_examples": "b4a95fe480b6540e",
        "hash_full_prompts": "ee18be6e60689080",
        "hash_input_tokens": "9c13df1d44f85f80",
        "hash_cont_tokens": "78998fa4cc2d5053"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1128,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_law|1": {
      "hashes": {
        "hash_examples": "c2be9651cdbdde3b",
        "hash_full_prompts": "5dd0bb135794b471",
        "hash_input_tokens": "513133c94ef9a40d",
        "hash_cont_tokens": "d48dbadc6f787e8d"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6132,
      "non_padded": 4,
      "effective_few_shots": 0.999348109517601,
      "num_truncated_few_shots": 1
    },
    "community|arabic_mmlu:professional_medicine|1": {
      "hashes": {
        "hash_examples": "26ce92416288f273",
        "hash_full_prompts": "7ac6349caa89a687",
        "hash_input_tokens": "8af2c0e17fb20fbb",
        "hash_cont_tokens": "06d2b5bc2047b8bd"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_psychology|1": {
      "hashes": {
        "hash_examples": "71ea5f182ea9a641",
        "hash_full_prompts": "afa70ad832e94ffa",
        "hash_input_tokens": "4416cf716ebc2a72",
        "hash_cont_tokens": "cb803230fd87d2f9"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2448,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:public_relations|1": {
      "hashes": {
        "hash_examples": "125adc21f91f8d77",
        "hash_full_prompts": "10950521dfb87600",
        "hash_input_tokens": "388f4e22334c641b",
        "hash_cont_tokens": "f767cc2b3bf7121b"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 440,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:security_studies|1": {
      "hashes": {
        "hash_examples": "3c18b216c099fb26",
        "hash_full_prompts": "b4dab8a550ef140a",
        "hash_input_tokens": "a10d4eecf5360ed0",
        "hash_cont_tokens": "e39b3a5525176c0c"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:sociology|1": {
      "hashes": {
        "hash_examples": "3f2a9634cef7417d",
        "hash_full_prompts": "f6a06ea098bfb4dd",
        "hash_input_tokens": "98e11ab0bcac3d80",
        "hash_cont_tokens": "b066530975a56dc5"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 800,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:us_foreign_policy|1": {
      "hashes": {
        "hash_examples": "22249da54056475e",
        "hash_full_prompts": "d09bb8a88bbe44ea",
        "hash_input_tokens": "e5e75f88b518668a",
        "hash_cont_tokens": "f22ca3ecda83a73a"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:virology|1": {
      "hashes": {
        "hash_examples": "9d194b9471dc624e",
        "hash_full_prompts": "d06dd070138a34f5",
        "hash_input_tokens": "6a3c58a611ea1efd",
        "hash_cont_tokens": "d7ac6dcd64550694"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 660,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:world_religions|1": {
      "hashes": {
        "hash_examples": "229e5fe50082b064",
        "hash_full_prompts": "9893b9545417827a",
        "hash_input_tokens": "9578aab44f019bce",
        "hash_cont_tokens": "4f8fc7e99b40392f"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_challenge_okapi_ar|1": {
      "hashes": {
        "hash_examples": "ab893807673bc355",
        "hash_full_prompts": "8e1d27802b755fbf",
        "hash_input_tokens": "95aa98ccaa449b4d",
        "hash_cont_tokens": "71ada1c96933c671"
      },
      "truncated": 0,
      "non_truncated": 1160,
      "padded": 4636,
      "non_padded": 4,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_easy_ar|1": {
      "hashes": {
        "hash_examples": "acb688624acc3d04",
        "hash_full_prompts": "96fdb1e9b2d96d95",
        "hash_input_tokens": "5be5073de7117a8b",
        "hash_cont_tokens": "dddc9f9404177d53"
      },
      "truncated": 0,
      "non_truncated": 2364,
      "padded": 9456,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|boolq_ar|1": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "d0a3d23689298bb8",
        "hash_input_tokens": "bf841d40cdd2dd07",
        "hash_cont_tokens": "f16ea95d31c4fe03"
      },
      "truncated": 2,
      "non_truncated": 3258,
      "padded": 6518,
      "non_padded": 2,
      "effective_few_shots": 0.9984662576687117,
      "num_truncated_few_shots": 4
    },
    "community|copa_ext_ar|1": {
      "hashes": {
        "hash_examples": "9bb83301bb72eecf",
        "hash_full_prompts": "2f62038fb6444ba8",
        "hash_input_tokens": "f74da9c365ce881b",
        "hash_cont_tokens": "d129cfa76d823828"
      },
      "truncated": 0,
      "non_truncated": 90,
      "padded": 180,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|1": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "094dcb9804ac766b",
        "hash_input_tokens": "16a6b85391dd6505",
        "hash_cont_tokens": "0ee80ba35813e8bc"
      },
      "truncated": 4,
      "non_truncated": 9167,
      "padded": 36671,
      "non_padded": 13,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|mmlu_okapi_ar|1": {
      "hashes": {
        "hash_examples": "723b5371ad6fdc31",
        "hash_full_prompts": "e1738a4e7064b038",
        "hash_input_tokens": "b4ad4530eba591d3",
        "hash_cont_tokens": "6ff35964cc3ce11e"
      },
      "truncated": 30,
      "non_truncated": 12893,
      "padded": 51501,
      "non_padded": 191,
      "effective_few_shots": 0.9989166602182156,
      "num_truncated_few_shots": 9
    },
    "community|openbook_qa_ext_ar|1": {
      "hashes": {
        "hash_examples": "923d41eb0aca93eb",
        "hash_full_prompts": "b56a17e5f3600a6f",
        "hash_input_tokens": "b3ce34c97b66b5d0",
        "hash_cont_tokens": "e80ac9f55927561f"
      },
      "truncated": 0,
      "non_truncated": 495,
      "padded": 1980,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|piqa_ar|1": {
      "hashes": {
        "hash_examples": "94bc205a520d3ea0",
        "hash_full_prompts": "42b6a79f5783638b",
        "hash_input_tokens": "1f2a79397bb8637b",
        "hash_cont_tokens": "32261c8227d8283a"
      },
      "truncated": 2,
      "non_truncated": 1831,
      "padded": 3638,
      "non_padded": 28,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "community|race_ar|1": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "ace767ebe290c249",
        "hash_input_tokens": "b31562402321ec75",
        "hash_cont_tokens": "2c8ced0f43d822e4"
      },
      "truncated": 639,
      "non_truncated": 4290,
      "padded": 19060,
      "non_padded": 656,
      "effective_few_shots": 0.28626496246703187,
      "num_truncated_few_shots": 3428
    },
    "community|sciq_ar|1": {
      "hashes": {
        "hash_examples": "0524701b5a0f7b4b",
        "hash_full_prompts": "8ffdb66acc1ce528",
        "hash_input_tokens": "dbcc673c996afde8",
        "hash_cont_tokens": "b936688d47731873"
      },
      "truncated": 0,
      "non_truncated": 995,
      "padded": 3971,
      "non_padded": 9,
      "effective_few_shots": 0.9939698492462311,
      "num_truncated_few_shots": 6
    },
    "community|toxigen_ar|1": {
      "hashes": {
        "hash_examples": "1e139513004a9a2e",
        "hash_full_prompts": "1f111d55b7115d36",
        "hash_input_tokens": "89cea8752ad5f937",
        "hash_cont_tokens": "a8bed015cace2400"
      },
      "truncated": 0,
      "non_truncated": 935,
      "padded": 1870,
      "non_padded": 0,
      "effective_few_shots": 1.0,
      "num_truncated_few_shots": 0
    },
    "lighteval|xstory_cloze:ar|0": {
      "hashes": {
        "hash_examples": "865426a22c787481",
        "hash_full_prompts": "865426a22c787481",
        "hash_input_tokens": "7fcc7236108f901d",
        "hash_cont_tokens": "bb11b8a62b170222"
      },
      "truncated": 0,
      "non_truncated": 1511,
      "padded": 3022,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "2cc032d64ade883f",
      "hash_full_prompts": "5d4aadb212ef288e",
      "hash_input_tokens": "8ca0948be68f255f",
      "hash_cont_tokens": "d09a7c19a4ce5abb"
    },
    "truncated": 677,
    "non_truncated": 85210,
    "padded": 286275,
    "non_padded": 1040,
    "num_truncated_few_shots": 3449
  }
}