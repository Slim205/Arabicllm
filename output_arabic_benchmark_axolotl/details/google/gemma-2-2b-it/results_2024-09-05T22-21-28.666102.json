{
  "config_general": {
    "lighteval_sha": "ba1761f0f7fd6731fba54776aecc6d2ca97e10b3",
    "num_fewshot_seeds": 1,
    "override_batch_size": 4,
    "max_samples": null,
    "job_id": "",
    "start_time": 302119.858106222,
    "end_time": 304702.904950608,
    "total_evaluation_time_secondes": "2583.046844385972",
    "model_name": "google/gemma-2-2b-it",
    "model_sha": "299a8560bedf22ed1c72a8a11e7dce4a7f9f51f8",
    "model_dtype": "torch.bfloat16",
    "model_size": "4.87 GB",
    "config": null
  },
  "results": {
    "community|acva:Algeria|0": {
      "acc_norm": 0.5230769230769231,
      "acc_norm_stderr": 0.0358596530894741
    },
    "community|acva:Ancient_Egypt|0": {
      "acc_norm": 0.050793650793650794,
      "acc_norm_stderr": 0.01239139518482262
    },
    "community|acva:Arab_Empire|0": {
      "acc_norm": 0.3132075471698113,
      "acc_norm_stderr": 0.02854479331905533
    },
    "community|acva:Arabic_Architecture|0": {
      "acc_norm": 0.4564102564102564,
      "acc_norm_stderr": 0.035761230969912135
    },
    "community|acva:Arabic_Art|0": {
      "acc_norm": 0.3435897435897436,
      "acc_norm_stderr": 0.034096273014098566
    },
    "community|acva:Arabic_Astronomy|0": {
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.03581804596782233
    },
    "community|acva:Arabic_Calligraphy|0": {
      "acc_norm": 0.7843137254901961,
      "acc_norm_stderr": 0.025807105892730445
    },
    "community|acva:Arabic_Ceremony|0": {
      "acc_norm": 0.5243243243243243,
      "acc_norm_stderr": 0.0368168445060319
    },
    "community|acva:Arabic_Clothing|0": {
      "acc_norm": 0.5128205128205128,
      "acc_norm_stderr": 0.03588610523192215
    },
    "community|acva:Arabic_Culture|0": {
      "acc_norm": 0.2358974358974359,
      "acc_norm_stderr": 0.030481516761721537
    },
    "community|acva:Arabic_Food|0": {
      "acc_norm": 0.441025641025641,
      "acc_norm_stderr": 0.0356473293185358
    },
    "community|acva:Arabic_Funeral|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.050529115263991134
    },
    "community|acva:Arabic_Geography|0": {
      "acc_norm": 0.6206896551724138,
      "acc_norm_stderr": 0.040434618619167466
    },
    "community|acva:Arabic_History|0": {
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.03298070870085619
    },
    "community|acva:Arabic_Language_Origin|0": {
      "acc_norm": 0.5473684210526316,
      "acc_norm_stderr": 0.051339113773544845
    },
    "community|acva:Arabic_Literature|0": {
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.041618085035015295
    },
    "community|acva:Arabic_Math|0": {
      "acc_norm": 0.30256410256410254,
      "acc_norm_stderr": 0.03298070870085618
    },
    "community|acva:Arabic_Medicine|0": {
      "acc_norm": 0.46206896551724136,
      "acc_norm_stderr": 0.041546596717075474
    },
    "community|acva:Arabic_Music|0": {
      "acc_norm": 0.23741007194244604,
      "acc_norm_stderr": 0.036220593237998276
    },
    "community|acva:Arabic_Ornament|0": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03581804596782232
    },
    "community|acva:Arabic_Philosophy|0": {
      "acc_norm": 0.5793103448275863,
      "acc_norm_stderr": 0.0411391498118926
    },
    "community|acva:Arabic_Physics_and_Chemistry|0": {
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03581804596782232
    },
    "community|acva:Arabic_Wedding|0": {
      "acc_norm": 0.4256410256410256,
      "acc_norm_stderr": 0.03549871080367708
    },
    "community|acva:Bahrain|0": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Comoros|0": {
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Egypt_modern|0": {
      "acc_norm": 0.3263157894736842,
      "acc_norm_stderr": 0.04835966701461423
    },
    "community|acva:InfluenceFromAncientEgypt|0": {
      "acc_norm": 0.6051282051282051,
      "acc_norm_stderr": 0.03509545602262038
    },
    "community|acva:InfluenceFromByzantium|0": {
      "acc_norm": 0.7172413793103448,
      "acc_norm_stderr": 0.03752833958003337
    },
    "community|acva:InfluenceFromChina|0": {
      "acc_norm": 0.2717948717948718,
      "acc_norm_stderr": 0.031940861870257235
    },
    "community|acva:InfluenceFromGreece|0": {
      "acc_norm": 0.6307692307692307,
      "acc_norm_stderr": 0.034648411418637566
    },
    "community|acva:InfluenceFromIslam|0": {
      "acc_norm": 0.296551724137931,
      "acc_norm_stderr": 0.03806142687309993
    },
    "community|acva:InfluenceFromPersia|0": {
      "acc_norm": 0.6971428571428572,
      "acc_norm_stderr": 0.03483414676585986
    },
    "community|acva:InfluenceFromRome|0": {
      "acc_norm": 0.5743589743589743,
      "acc_norm_stderr": 0.03549871080367708
    },
    "community|acva:Iraq|0": {
      "acc_norm": 0.5176470588235295,
      "acc_norm_stderr": 0.05452048340661895
    },
    "community|acva:Islam_Education|0": {
      "acc_norm": 0.4564102564102564,
      "acc_norm_stderr": 0.03576123096991215
    },
    "community|acva:Islam_branches_and_schools|0": {
      "acc_norm": 0.4342857142857143,
      "acc_norm_stderr": 0.037576101528126626
    },
    "community|acva:Islamic_law_system|0": {
      "acc_norm": 0.4256410256410256,
      "acc_norm_stderr": 0.035498710803677086
    },
    "community|acva:Jordan|0": {
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.07106690545187012
    },
    "community|acva:Kuwait|0": {
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.06666666666666667
    },
    "community|acva:Lebanon|0": {
      "acc_norm": 0.17777777777777778,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Libya|0": {
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.07535922203472523
    },
    "community|acva:Mauritania|0": {
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.07446027270295805
    },
    "community|acva:Mesopotamia_civilization|0": {
      "acc_norm": 0.5225806451612903,
      "acc_norm_stderr": 0.0402500394824441
    },
    "community|acva:Morocco|0": {
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.06267511942419628
    },
    "community|acva:Oman|0": {
      "acc_norm": 0.17777777777777778,
      "acc_norm_stderr": 0.05763774795025094
    },
    "community|acva:Palestine|0": {
      "acc_norm": 0.24705882352941178,
      "acc_norm_stderr": 0.047058823529411785
    },
    "community|acva:Qatar|0": {
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.07385489458759964
    },
    "community|acva:Saudi_Arabia|0": {
      "acc_norm": 0.3282051282051282,
      "acc_norm_stderr": 0.03371243782413707
    },
    "community|acva:Somalia|0": {
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.07491109582924914
    },
    "community|acva:Sudan|0": {
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Syria|0": {
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.07309112127323451
    },
    "community|acva:Tunisia|0": {
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.06979205927323111
    },
    "community|acva:United_Arab_Emirates|0": {
      "acc_norm": 0.25882352941176473,
      "acc_norm_stderr": 0.04778846120374094
    },
    "community|acva:Yemen|0": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.13333333333333333
    },
    "community|acva:communication|0": {
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.025974025974025955
    },
    "community|acva:computer_and_phone|0": {
      "acc_norm": 0.45084745762711864,
      "acc_norm_stderr": 0.02901934773187137
    },
    "community|acva:daily_life|0": {
      "acc_norm": 0.18694362017804153,
      "acc_norm_stderr": 0.021268948348414647
    },
    "community|acva:entertainment|0": {
      "acc_norm": 0.23389830508474577,
      "acc_norm_stderr": 0.024687839412166384
    },
    "community|alghafa:mcq_exams_test_ar|0": {
      "acc_norm": 0.2639138240574506,
      "acc_norm_stderr": 0.01869209608329398
    },
    "community|alghafa:meta_ar_dialects|0": {
      "acc_norm": 0.2741427247451344,
      "acc_norm_stderr": 0.006073772326433627
    },
    "community|alghafa:meta_ar_msa|0": {
      "acc_norm": 0.29720670391061454,
      "acc_norm_stderr": 0.0152853133536416
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": {
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.05807730170189531
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": {
      "acc_norm": 0.4666666666666667,
      "acc_norm_stderr": 0.04087046889188088
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": {
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.038807734647314546
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": {
      "acc_norm": 0.8101313320825516,
      "acc_norm_stderr": 0.004386542023140495
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|0": {
      "acc_norm": 0.5354462051709759,
      "acc_norm_stderr": 0.006441953108311428
    },
    "community|alghafa:multiple_choice_sentiment_task|0": {
      "acc_norm": 0.3732558139534884,
      "acc_norm_stderr": 0.011665692088523964
    },
    "community|arabic_exams|0": {
      "acc_norm": 0.23649906890130354,
      "acc_norm_stderr": 0.018354269670319875
    },
    "community|arabic_mmlu:abstract_algebra|0": {
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932268
    },
    "community|arabic_mmlu:anatomy|0": {
      "acc_norm": 0.18518518518518517,
      "acc_norm_stderr": 0.03355677216313142
    },
    "community|arabic_mmlu:astronomy|0": {
      "acc_norm": 0.17763157894736842,
      "acc_norm_stderr": 0.031103182383123398
    },
    "community|arabic_mmlu:business_ethics|0": {
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "community|arabic_mmlu:clinical_knowledge|0": {
      "acc_norm": 0.21509433962264152,
      "acc_norm_stderr": 0.02528839450289137
    },
    "community|arabic_mmlu:college_biology|0": {
      "acc_norm": 0.2569444444444444,
      "acc_norm_stderr": 0.03653946969442099
    },
    "community|arabic_mmlu:college_chemistry|0": {
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "community|arabic_mmlu:college_computer_science|0": {
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "community|arabic_mmlu:college_mathematics|0": {
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "community|arabic_mmlu:college_medicine|0": {
      "acc_norm": 0.20809248554913296,
      "acc_norm_stderr": 0.030952890217749874
    },
    "community|arabic_mmlu:college_physics|0": {
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237654
    },
    "community|arabic_mmlu:computer_security|0": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "community|arabic_mmlu:conceptual_physics|0": {
      "acc_norm": 0.26382978723404255,
      "acc_norm_stderr": 0.028809989854102973
    },
    "community|arabic_mmlu:econometrics|0": {
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.039994238792813365
    },
    "community|arabic_mmlu:electrical_engineering|0": {
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.03565998174135302
    },
    "community|arabic_mmlu:elementary_mathematics|0": {
      "acc_norm": 0.20899470899470898,
      "acc_norm_stderr": 0.02094048156533486
    },
    "community|arabic_mmlu:formal_logic|0": {
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.040061680838488746
    },
    "community|arabic_mmlu:global_facts|0": {
      "acc_norm": 0.18,
      "acc_norm_stderr": 0.038612291966536934
    },
    "community|arabic_mmlu:high_school_biology|0": {
      "acc_norm": 0.1774193548387097,
      "acc_norm_stderr": 0.02173254068932927
    },
    "community|arabic_mmlu:high_school_chemistry|0": {
      "acc_norm": 0.15270935960591134,
      "acc_norm_stderr": 0.02530890453938063
    },
    "community|arabic_mmlu:high_school_computer_science|0": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "community|arabic_mmlu:high_school_european_history|0": {
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03225078108306289
    },
    "community|arabic_mmlu:high_school_geography|0": {
      "acc_norm": 0.17676767676767677,
      "acc_norm_stderr": 0.027178752639044915
    },
    "community|arabic_mmlu:high_school_government_and_politics|0": {
      "acc_norm": 0.19689119170984457,
      "acc_norm_stderr": 0.028697873971860664
    },
    "community|arabic_mmlu:high_school_macroeconomics|0": {
      "acc_norm": 0.20256410256410257,
      "acc_norm_stderr": 0.020377660970371372
    },
    "community|arabic_mmlu:high_school_mathematics|0": {
      "acc_norm": 0.2111111111111111,
      "acc_norm_stderr": 0.024882116857655075
    },
    "community|arabic_mmlu:high_school_microeconomics|0": {
      "acc_norm": 0.21008403361344538,
      "acc_norm_stderr": 0.026461398717471874
    },
    "community|arabic_mmlu:high_school_physics|0": {
      "acc_norm": 0.1986754966887417,
      "acc_norm_stderr": 0.03257847384436776
    },
    "community|arabic_mmlu:high_school_psychology|0": {
      "acc_norm": 0.1926605504587156,
      "acc_norm_stderr": 0.016909276884936094
    },
    "community|arabic_mmlu:high_school_statistics|0": {
      "acc_norm": 0.1527777777777778,
      "acc_norm_stderr": 0.024536326026134224
    },
    "community|arabic_mmlu:high_school_us_history|0": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03039153369274154
    },
    "community|arabic_mmlu:high_school_world_history|0": {
      "acc_norm": 0.270042194092827,
      "acc_norm_stderr": 0.028900721906293426
    },
    "community|arabic_mmlu:human_aging|0": {
      "acc_norm": 0.31390134529147984,
      "acc_norm_stderr": 0.031146796482972465
    },
    "community|arabic_mmlu:human_sexuality|0": {
      "acc_norm": 0.2595419847328244,
      "acc_norm_stderr": 0.03844876139785271
    },
    "community|arabic_mmlu:international_law|0": {
      "acc_norm": 0.2396694214876033,
      "acc_norm_stderr": 0.03896878985070417
    },
    "community|arabic_mmlu:jurisprudence|0": {
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.042365112580946336
    },
    "community|arabic_mmlu:logical_fallacies|0": {
      "acc_norm": 0.22085889570552147,
      "acc_norm_stderr": 0.032591773927421776
    },
    "community|arabic_mmlu:machine_learning|0": {
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.043994650575715215
    },
    "community|arabic_mmlu:management|0": {
      "acc_norm": 0.17475728155339806,
      "acc_norm_stderr": 0.037601780060266224
    },
    "community|arabic_mmlu:marketing|0": {
      "acc_norm": 0.2905982905982906,
      "acc_norm_stderr": 0.02974504857267404
    },
    "community|arabic_mmlu:medical_genetics|0": {
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "community|arabic_mmlu:miscellaneous|0": {
      "acc_norm": 0.2388250319284802,
      "acc_norm_stderr": 0.015246803197398691
    },
    "community|arabic_mmlu:moral_disputes|0": {
      "acc_norm": 0.24855491329479767,
      "acc_norm_stderr": 0.023267528432100174
    },
    "community|arabic_mmlu:moral_scenarios|0": {
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "community|arabic_mmlu:nutrition|0": {
      "acc_norm": 0.22549019607843138,
      "acc_norm_stderr": 0.023929155517351284
    },
    "community|arabic_mmlu:philosophy|0": {
      "acc_norm": 0.1864951768488746,
      "acc_norm_stderr": 0.02212243977248077
    },
    "community|arabic_mmlu:prehistory|0": {
      "acc_norm": 0.21604938271604937,
      "acc_norm_stderr": 0.022899162918445806
    },
    "community|arabic_mmlu:professional_accounting|0": {
      "acc_norm": 0.23404255319148937,
      "acc_norm_stderr": 0.025257861359432417
    },
    "community|arabic_mmlu:professional_law|0": {
      "acc_norm": 0.2457627118644068,
      "acc_norm_stderr": 0.010996156635142692
    },
    "community|arabic_mmlu:professional_medicine|0": {
      "acc_norm": 0.18382352941176472,
      "acc_norm_stderr": 0.023529242185193106
    },
    "community|arabic_mmlu:professional_psychology|0": {
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.01751781884501444
    },
    "community|arabic_mmlu:public_relations|0": {
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03955932861795833
    },
    "community|arabic_mmlu:security_studies|0": {
      "acc_norm": 0.18775510204081633,
      "acc_norm_stderr": 0.02500025603954621
    },
    "community|arabic_mmlu:sociology|0": {
      "acc_norm": 0.24378109452736318,
      "acc_norm_stderr": 0.03036049015401465
    },
    "community|arabic_mmlu:us_foreign_policy|0": {
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "community|arabic_mmlu:virology|0": {
      "acc_norm": 0.28313253012048195,
      "acc_norm_stderr": 0.03507295431370518
    },
    "community|arabic_mmlu:world_religions|0": {
      "acc_norm": 0.3216374269005848,
      "acc_norm_stderr": 0.03582529442573122
    },
    "community|arc_challenge_okapi_ar|0": {
      "acc_norm": 0.328448275862069,
      "acc_norm_stderr": 0.013795311440005615
    },
    "community|arc_easy_ar|0": {
      "acc_norm": 0.3011844331641286,
      "acc_norm_stderr": 0.00943768950722336
    },
    "community|boolq_ar|0": {
      "acc_norm": 0.747239263803681,
      "acc_norm_stderr": 0.0076127657687087135
    },
    "community|copa_ext_ar|0": {
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.05298680599073449
    },
    "community|hellaswag_okapi_ar|0": {
      "acc_norm": 0.2590775269872424,
      "acc_norm_stderr": 0.004575268517394528
    },
    "community|mmlu_okapi_ar|0": {
      "acc_norm": 0.2907219685831463,
      "acc_norm_stderr": 0.003994682907466869
    },
    "community|openbook_qa_ext_ar|0": {
      "acc_norm": 0.4101010101010101,
      "acc_norm_stderr": 0.02212945769797249
    },
    "community|piqa_ar|0": {
      "acc_norm": 0.574468085106383,
      "acc_norm_stderr": 0.011551444273029118
    },
    "community|race_ar|0": {
      "acc_norm": 0.3280584297017651,
      "acc_norm_stderr": 0.006688150023078613
    },
    "community|sciq_ar|0": {
      "acc_norm": 0.36683417085427134,
      "acc_norm_stderr": 0.015286231556506097
    },
    "community|toxigen_ar|0": {
      "acc_norm": 0.6374331550802139,
      "acc_norm_stderr": 0.015730333491045415
    },
    "lighteval|xstory_cloze:ar|0": {
      "acc": 0.5433487756452681,
      "acc_stderr": 0.012818676452481956
    },
    "community|acva:_average|0": {
      "acc_norm": 0.4067861894540611,
      "acc_norm_stderr": 0.045859527429225826
    },
    "community|alghafa:_average|0": {
      "acc_norm": 0.43119591895409803,
      "acc_norm_stderr": 0.022255652691603976
    },
    "community|arabic_mmlu:_average|0": {
      "acc_norm": 0.23105174452338867,
      "acc_norm_stderr": 0.03149265412654658
    },
    "all": {
      "acc_norm": 0.33539166157198297,
      "acc_norm_stderr": 0.03556894971453069,
      "acc": 0.5433487756452681,
      "acc_stderr": 0.012818676452481956
    }
  },
  "versions": {
    "community|acva:Algeria|0": 0,
    "community|acva:Ancient_Egypt|0": 0,
    "community|acva:Arab_Empire|0": 0,
    "community|acva:Arabic_Architecture|0": 0,
    "community|acva:Arabic_Art|0": 0,
    "community|acva:Arabic_Astronomy|0": 0,
    "community|acva:Arabic_Calligraphy|0": 0,
    "community|acva:Arabic_Ceremony|0": 0,
    "community|acva:Arabic_Clothing|0": 0,
    "community|acva:Arabic_Culture|0": 0,
    "community|acva:Arabic_Food|0": 0,
    "community|acva:Arabic_Funeral|0": 0,
    "community|acva:Arabic_Geography|0": 0,
    "community|acva:Arabic_History|0": 0,
    "community|acva:Arabic_Language_Origin|0": 0,
    "community|acva:Arabic_Literature|0": 0,
    "community|acva:Arabic_Math|0": 0,
    "community|acva:Arabic_Medicine|0": 0,
    "community|acva:Arabic_Music|0": 0,
    "community|acva:Arabic_Ornament|0": 0,
    "community|acva:Arabic_Philosophy|0": 0,
    "community|acva:Arabic_Physics_and_Chemistry|0": 0,
    "community|acva:Arabic_Wedding|0": 0,
    "community|acva:Bahrain|0": 0,
    "community|acva:Comoros|0": 0,
    "community|acva:Egypt_modern|0": 0,
    "community|acva:InfluenceFromAncientEgypt|0": 0,
    "community|acva:InfluenceFromByzantium|0": 0,
    "community|acva:InfluenceFromChina|0": 0,
    "community|acva:InfluenceFromGreece|0": 0,
    "community|acva:InfluenceFromIslam|0": 0,
    "community|acva:InfluenceFromPersia|0": 0,
    "community|acva:InfluenceFromRome|0": 0,
    "community|acva:Iraq|0": 0,
    "community|acva:Islam_Education|0": 0,
    "community|acva:Islam_branches_and_schools|0": 0,
    "community|acva:Islamic_law_system|0": 0,
    "community|acva:Jordan|0": 0,
    "community|acva:Kuwait|0": 0,
    "community|acva:Lebanon|0": 0,
    "community|acva:Libya|0": 0,
    "community|acva:Mauritania|0": 0,
    "community|acva:Mesopotamia_civilization|0": 0,
    "community|acva:Morocco|0": 0,
    "community|acva:Oman|0": 0,
    "community|acva:Palestine|0": 0,
    "community|acva:Qatar|0": 0,
    "community|acva:Saudi_Arabia|0": 0,
    "community|acva:Somalia|0": 0,
    "community|acva:Sudan|0": 0,
    "community|acva:Syria|0": 0,
    "community|acva:Tunisia|0": 0,
    "community|acva:United_Arab_Emirates|0": 0,
    "community|acva:Yemen|0": 0,
    "community|acva:communication|0": 0,
    "community|acva:computer_and_phone|0": 0,
    "community|acva:daily_life|0": 0,
    "community|acva:entertainment|0": 0,
    "community|alghafa:mcq_exams_test_ar|0": 0,
    "community|alghafa:meta_ar_dialects|0": 0,
    "community|alghafa:meta_ar_msa|0": 0,
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": 0,
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": 0,
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": 0,
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": 0,
    "community|alghafa:multiple_choice_rating_sentiment_task|0": 0,
    "community|alghafa:multiple_choice_sentiment_task|0": 0,
    "community|arabic_exams|0": 0,
    "community|arabic_mmlu:abstract_algebra|0": 0,
    "community|arabic_mmlu:anatomy|0": 0,
    "community|arabic_mmlu:astronomy|0": 0,
    "community|arabic_mmlu:business_ethics|0": 0,
    "community|arabic_mmlu:clinical_knowledge|0": 0,
    "community|arabic_mmlu:college_biology|0": 0,
    "community|arabic_mmlu:college_chemistry|0": 0,
    "community|arabic_mmlu:college_computer_science|0": 0,
    "community|arabic_mmlu:college_mathematics|0": 0,
    "community|arabic_mmlu:college_medicine|0": 0,
    "community|arabic_mmlu:college_physics|0": 0,
    "community|arabic_mmlu:computer_security|0": 0,
    "community|arabic_mmlu:conceptual_physics|0": 0,
    "community|arabic_mmlu:econometrics|0": 0,
    "community|arabic_mmlu:electrical_engineering|0": 0,
    "community|arabic_mmlu:elementary_mathematics|0": 0,
    "community|arabic_mmlu:formal_logic|0": 0,
    "community|arabic_mmlu:global_facts|0": 0,
    "community|arabic_mmlu:high_school_biology|0": 0,
    "community|arabic_mmlu:high_school_chemistry|0": 0,
    "community|arabic_mmlu:high_school_computer_science|0": 0,
    "community|arabic_mmlu:high_school_european_history|0": 0,
    "community|arabic_mmlu:high_school_geography|0": 0,
    "community|arabic_mmlu:high_school_government_and_politics|0": 0,
    "community|arabic_mmlu:high_school_macroeconomics|0": 0,
    "community|arabic_mmlu:high_school_mathematics|0": 0,
    "community|arabic_mmlu:high_school_microeconomics|0": 0,
    "community|arabic_mmlu:high_school_physics|0": 0,
    "community|arabic_mmlu:high_school_psychology|0": 0,
    "community|arabic_mmlu:high_school_statistics|0": 0,
    "community|arabic_mmlu:high_school_us_history|0": 0,
    "community|arabic_mmlu:high_school_world_history|0": 0,
    "community|arabic_mmlu:human_aging|0": 0,
    "community|arabic_mmlu:human_sexuality|0": 0,
    "community|arabic_mmlu:international_law|0": 0,
    "community|arabic_mmlu:jurisprudence|0": 0,
    "community|arabic_mmlu:logical_fallacies|0": 0,
    "community|arabic_mmlu:machine_learning|0": 0,
    "community|arabic_mmlu:management|0": 0,
    "community|arabic_mmlu:marketing|0": 0,
    "community|arabic_mmlu:medical_genetics|0": 0,
    "community|arabic_mmlu:miscellaneous|0": 0,
    "community|arabic_mmlu:moral_disputes|0": 0,
    "community|arabic_mmlu:moral_scenarios|0": 0,
    "community|arabic_mmlu:nutrition|0": 0,
    "community|arabic_mmlu:philosophy|0": 0,
    "community|arabic_mmlu:prehistory|0": 0,
    "community|arabic_mmlu:professional_accounting|0": 0,
    "community|arabic_mmlu:professional_law|0": 0,
    "community|arabic_mmlu:professional_medicine|0": 0,
    "community|arabic_mmlu:professional_psychology|0": 0,
    "community|arabic_mmlu:public_relations|0": 0,
    "community|arabic_mmlu:security_studies|0": 0,
    "community|arabic_mmlu:sociology|0": 0,
    "community|arabic_mmlu:us_foreign_policy|0": 0,
    "community|arabic_mmlu:virology|0": 0,
    "community|arabic_mmlu:world_religions|0": 0,
    "community|arc_challenge_okapi_ar|0": 0,
    "community|arc_easy_ar|0": 0,
    "community|boolq_ar|0": 0,
    "community|copa_ext_ar|0": 0,
    "community|hellaswag_okapi_ar|0": 0,
    "community|mmlu_okapi_ar|0": 0,
    "community|openbook_qa_ext_ar|0": 0,
    "community|piqa_ar|0": 0,
    "community|race_ar|0": 0,
    "community|sciq_ar|0": 0,
    "community|toxigen_ar|0": 0,
    "lighteval|xstory_cloze:ar|0": 0
  },
  "config_tasks": {
    "community|acva:Algeria": {
      "name": "acva:Algeria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Algeria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Ancient_Egypt": {
      "name": "acva:Ancient_Egypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Ancient_Egypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 315,
      "effective_num_docs": 315,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arab_Empire": {
      "name": "acva:Arab_Empire",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arab_Empire",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Architecture": {
      "name": "acva:Arabic_Architecture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Architecture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Art": {
      "name": "acva:Arabic_Art",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Art",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Astronomy": {
      "name": "acva:Arabic_Astronomy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Calligraphy": {
      "name": "acva:Arabic_Calligraphy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Calligraphy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 255,
      "effective_num_docs": 255,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ceremony": {
      "name": "acva:Arabic_Ceremony",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ceremony",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 185,
      "effective_num_docs": 185,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Clothing": {
      "name": "acva:Arabic_Clothing",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Clothing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Culture": {
      "name": "acva:Arabic_Culture",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Culture",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Food": {
      "name": "acva:Arabic_Food",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Food",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Funeral": {
      "name": "acva:Arabic_Funeral",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Funeral",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Geography": {
      "name": "acva:Arabic_Geography",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_History": {
      "name": "acva:Arabic_History",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_History",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Language_Origin": {
      "name": "acva:Arabic_Language_Origin",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Language_Origin",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Literature": {
      "name": "acva:Arabic_Literature",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Literature",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Math": {
      "name": "acva:Arabic_Math",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Math",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Medicine": {
      "name": "acva:Arabic_Medicine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Music": {
      "name": "acva:Arabic_Music",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Music",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 139,
      "effective_num_docs": 139,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Ornament": {
      "name": "acva:Arabic_Ornament",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Ornament",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Philosophy": {
      "name": "acva:Arabic_Philosophy",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry": {
      "name": "acva:Arabic_Physics_and_Chemistry",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Physics_and_Chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Arabic_Wedding": {
      "name": "acva:Arabic_Wedding",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Arabic_Wedding",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Bahrain": {
      "name": "acva:Bahrain",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Bahrain",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Comoros": {
      "name": "acva:Comoros",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Comoros",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Egypt_modern": {
      "name": "acva:Egypt_modern",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Egypt_modern",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 95,
      "effective_num_docs": 95,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromAncientEgypt": {
      "name": "acva:InfluenceFromAncientEgypt",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromAncientEgypt",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromByzantium": {
      "name": "acva:InfluenceFromByzantium",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromByzantium",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromChina": {
      "name": "acva:InfluenceFromChina",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromChina",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromGreece": {
      "name": "acva:InfluenceFromGreece",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromGreece",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromIslam": {
      "name": "acva:InfluenceFromIslam",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromIslam",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromPersia": {
      "name": "acva:InfluenceFromPersia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromPersia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:InfluenceFromRome": {
      "name": "acva:InfluenceFromRome",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "InfluenceFromRome",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Iraq": {
      "name": "acva:Iraq",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Iraq",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_Education": {
      "name": "acva:Islam_Education",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_Education",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islam_branches_and_schools": {
      "name": "acva:Islam_branches_and_schools",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islam_branches_and_schools",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 175,
      "effective_num_docs": 175,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Islamic_law_system": {
      "name": "acva:Islamic_law_system",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Islamic_law_system",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Jordan": {
      "name": "acva:Jordan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Jordan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Kuwait": {
      "name": "acva:Kuwait",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Kuwait",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Lebanon": {
      "name": "acva:Lebanon",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Lebanon",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Libya": {
      "name": "acva:Libya",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Libya",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mauritania": {
      "name": "acva:Mauritania",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mauritania",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Mesopotamia_civilization": {
      "name": "acva:Mesopotamia_civilization",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Mesopotamia_civilization",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 155,
      "effective_num_docs": 155,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Morocco": {
      "name": "acva:Morocco",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Morocco",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Oman": {
      "name": "acva:Oman",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Oman",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Palestine": {
      "name": "acva:Palestine",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Palestine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Qatar": {
      "name": "acva:Qatar",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Qatar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Saudi_Arabia": {
      "name": "acva:Saudi_Arabia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Saudi_Arabia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 195,
      "effective_num_docs": 195,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Somalia": {
      "name": "acva:Somalia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Somalia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Sudan": {
      "name": "acva:Sudan",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Sudan",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Syria": {
      "name": "acva:Syria",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Syria",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Tunisia": {
      "name": "acva:Tunisia",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Tunisia",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 45,
      "effective_num_docs": 45,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:United_Arab_Emirates": {
      "name": "acva:United_Arab_Emirates",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "United_Arab_Emirates",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 85,
      "effective_num_docs": 85,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:Yemen": {
      "name": "acva:Yemen",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "Yemen",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 10,
      "effective_num_docs": 10,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:communication": {
      "name": "acva:communication",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "communication",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 364,
      "effective_num_docs": 364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:computer_and_phone": {
      "name": "acva:computer_and_phone",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "computer_and_phone",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:daily_life": {
      "name": "acva:daily_life",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "daily_life",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 337,
      "effective_num_docs": 337,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|acva:entertainment": {
      "name": "acva:entertainment",
      "prompt_function": "acva",
      "hf_repo": "OALL/ACVA",
      "hf_subset": "entertainment",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 295,
      "effective_num_docs": 295,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:mcq_exams_test_ar": {
      "name": "alghafa:mcq_exams_test_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "mcq_exams_test_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 557,
      "effective_num_docs": 557,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_dialects": {
      "name": "alghafa:meta_ar_dialects",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_dialects",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5395,
      "effective_num_docs": 5395,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:meta_ar_msa": {
      "name": "alghafa:meta_ar_msa",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "meta_ar_msa",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task": {
      "name": "alghafa:multiple_choice_facts_truefalse_balanced_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_facts_truefalse_balanced_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 75,
      "effective_num_docs": 75,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task": {
      "name": "alghafa:multiple_choice_grounded_statement_soqal_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_soqal_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task": {
      "name": "alghafa:multiple_choice_grounded_statement_xglue_mlqa_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_grounded_statement_xglue_mlqa_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 150,
      "effective_num_docs": 150,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_no_neutral_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_no_neutral_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 7995,
      "effective_num_docs": 7995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task": {
      "name": "alghafa:multiple_choice_rating_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_rating_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 5995,
      "effective_num_docs": 5995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|alghafa:multiple_choice_sentiment_task": {
      "name": "alghafa:multiple_choice_sentiment_task",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Native",
      "hf_subset": "multiple_choice_sentiment_task",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1720,
      "effective_num_docs": 1720,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_exams": {
      "name": "arabic_exams",
      "prompt_function": "arabic_exams",
      "hf_repo": "OALL/Arabic_EXAMS",
      "hf_subset": "default",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 537,
      "effective_num_docs": 537,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:abstract_algebra": {
      "name": "arabic_mmlu:abstract_algebra",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "abstract_algebra",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:anatomy": {
      "name": "arabic_mmlu:anatomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "anatomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 135,
      "effective_num_docs": 135,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:astronomy": {
      "name": "arabic_mmlu:astronomy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "astronomy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 152,
      "effective_num_docs": 152,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:business_ethics": {
      "name": "arabic_mmlu:business_ethics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "business_ethics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:clinical_knowledge": {
      "name": "arabic_mmlu:clinical_knowledge",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "clinical_knowledge",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 265,
      "effective_num_docs": 265,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_biology": {
      "name": "arabic_mmlu:college_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 144,
      "effective_num_docs": 144,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_chemistry": {
      "name": "arabic_mmlu:college_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_computer_science": {
      "name": "arabic_mmlu:college_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_mathematics": {
      "name": "arabic_mmlu:college_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_medicine": {
      "name": "arabic_mmlu:college_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 173,
      "effective_num_docs": 173,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:college_physics": {
      "name": "arabic_mmlu:college_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "college_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 102,
      "effective_num_docs": 102,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:computer_security": {
      "name": "arabic_mmlu:computer_security",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "computer_security",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:conceptual_physics": {
      "name": "arabic_mmlu:conceptual_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "conceptual_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 235,
      "effective_num_docs": 235,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:econometrics": {
      "name": "arabic_mmlu:econometrics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "econometrics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 114,
      "effective_num_docs": 114,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:electrical_engineering": {
      "name": "arabic_mmlu:electrical_engineering",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "electrical_engineering",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 145,
      "effective_num_docs": 145,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:elementary_mathematics": {
      "name": "arabic_mmlu:elementary_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "elementary_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 378,
      "effective_num_docs": 378,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:formal_logic": {
      "name": "arabic_mmlu:formal_logic",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "formal_logic",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 126,
      "effective_num_docs": 126,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:global_facts": {
      "name": "arabic_mmlu:global_facts",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "global_facts",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_biology": {
      "name": "arabic_mmlu:high_school_biology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_biology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 310,
      "effective_num_docs": 310,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_chemistry": {
      "name": "arabic_mmlu:high_school_chemistry",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_chemistry",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 203,
      "effective_num_docs": 203,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_computer_science": {
      "name": "arabic_mmlu:high_school_computer_science",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_computer_science",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_european_history": {
      "name": "arabic_mmlu:high_school_european_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_european_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 165,
      "effective_num_docs": 165,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_geography": {
      "name": "arabic_mmlu:high_school_geography",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_geography",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 198,
      "effective_num_docs": 198,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics": {
      "name": "arabic_mmlu:high_school_government_and_politics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_government_and_politics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 193,
      "effective_num_docs": 193,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics": {
      "name": "arabic_mmlu:high_school_macroeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_macroeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 390,
      "effective_num_docs": 390,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_mathematics": {
      "name": "arabic_mmlu:high_school_mathematics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_mathematics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 270,
      "effective_num_docs": 270,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_microeconomics": {
      "name": "arabic_mmlu:high_school_microeconomics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_microeconomics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 238,
      "effective_num_docs": 238,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_physics": {
      "name": "arabic_mmlu:high_school_physics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_physics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 151,
      "effective_num_docs": 151,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_psychology": {
      "name": "arabic_mmlu:high_school_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 545,
      "effective_num_docs": 545,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_statistics": {
      "name": "arabic_mmlu:high_school_statistics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_statistics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 216,
      "effective_num_docs": 216,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_us_history": {
      "name": "arabic_mmlu:high_school_us_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_us_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 204,
      "effective_num_docs": 204,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:high_school_world_history": {
      "name": "arabic_mmlu:high_school_world_history",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "high_school_world_history",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 237,
      "effective_num_docs": 237,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_aging": {
      "name": "arabic_mmlu:human_aging",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_aging",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 223,
      "effective_num_docs": 223,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:human_sexuality": {
      "name": "arabic_mmlu:human_sexuality",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "human_sexuality",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 131,
      "effective_num_docs": 131,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:international_law": {
      "name": "arabic_mmlu:international_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "international_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 121,
      "effective_num_docs": 121,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:jurisprudence": {
      "name": "arabic_mmlu:jurisprudence",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "jurisprudence",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 108,
      "effective_num_docs": 108,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:logical_fallacies": {
      "name": "arabic_mmlu:logical_fallacies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "logical_fallacies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 163,
      "effective_num_docs": 163,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:machine_learning": {
      "name": "arabic_mmlu:machine_learning",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "machine_learning",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 112,
      "effective_num_docs": 112,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:management": {
      "name": "arabic_mmlu:management",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "management",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 103,
      "effective_num_docs": 103,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:marketing": {
      "name": "arabic_mmlu:marketing",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "marketing",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 234,
      "effective_num_docs": 234,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:medical_genetics": {
      "name": "arabic_mmlu:medical_genetics",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "medical_genetics",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:miscellaneous": {
      "name": "arabic_mmlu:miscellaneous",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "miscellaneous",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 783,
      "effective_num_docs": 783,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_disputes": {
      "name": "arabic_mmlu:moral_disputes",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_disputes",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 346,
      "effective_num_docs": 346,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:moral_scenarios": {
      "name": "arabic_mmlu:moral_scenarios",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "moral_scenarios",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 895,
      "effective_num_docs": 895,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:nutrition": {
      "name": "arabic_mmlu:nutrition",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "nutrition",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 306,
      "effective_num_docs": 306,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:philosophy": {
      "name": "arabic_mmlu:philosophy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "philosophy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 311,
      "effective_num_docs": 311,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:prehistory": {
      "name": "arabic_mmlu:prehistory",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "prehistory",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 324,
      "effective_num_docs": 324,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_accounting": {
      "name": "arabic_mmlu:professional_accounting",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_accounting",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 282,
      "effective_num_docs": 282,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_law": {
      "name": "arabic_mmlu:professional_law",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_law",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1534,
      "effective_num_docs": 1534,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_medicine": {
      "name": "arabic_mmlu:professional_medicine",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_medicine",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 272,
      "effective_num_docs": 272,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:professional_psychology": {
      "name": "arabic_mmlu:professional_psychology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "professional_psychology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 612,
      "effective_num_docs": 612,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:public_relations": {
      "name": "arabic_mmlu:public_relations",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "public_relations",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 110,
      "effective_num_docs": 110,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:security_studies": {
      "name": "arabic_mmlu:security_studies",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "security_studies",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 245,
      "effective_num_docs": 245,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:sociology": {
      "name": "arabic_mmlu:sociology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "sociology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 201,
      "effective_num_docs": 201,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:us_foreign_policy": {
      "name": "arabic_mmlu:us_foreign_policy",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "us_foreign_policy",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 100,
      "effective_num_docs": 100,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:virology": {
      "name": "arabic_mmlu:virology",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "virology",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 166,
      "effective_num_docs": 166,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arabic_mmlu:world_religions": {
      "name": "arabic_mmlu:world_religions",
      "prompt_function": "mmlu_arabic",
      "hf_repo": "OALL/Arabic_MMLU",
      "hf_subset": "world_religions",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "dev"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "dev",
      "few_shots_select": "sequential",
      "generation_size": -1,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 171,
      "effective_num_docs": 171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_challenge_okapi_ar": {
      "name": "arc_challenge_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_challenge_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1160,
      "effective_num_docs": 1160,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|arc_easy_ar": {
      "name": "arc_easy_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "arc_easy_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 2364,
      "effective_num_docs": 2364,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|copa_ext_ar": {
      "name": "copa_ext_ar",
      "prompt_function": "copa_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "copa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 90,
      "effective_num_docs": 90,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|mmlu_okapi_ar": {
      "name": "mmlu_okapi_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "mmlu_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 12923,
      "effective_num_docs": 12923,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|openbook_qa_ext_ar": {
      "name": "openbook_qa_ext_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "openbook_qa_ext_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 495,
      "effective_num_docs": 495,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|piqa_ar": {
      "name": "piqa_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "piqa_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 1833,
      "effective_num_docs": 1833,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|sciq_ar": {
      "name": "sciq_ar",
      "prompt_function": "sciq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "sciq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 995,
      "effective_num_docs": 995,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|toxigen_ar": {
      "name": "toxigen_ar",
      "prompt_function": "toxigen_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "toxigen_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 935,
      "effective_num_docs": 935,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "lighteval|xstory_cloze:ar": {
      "name": "xstory_cloze:ar",
      "prompt_function": "storycloze",
      "hf_repo": "juletxara/xstory_cloze",
      "hf_subset": "ar",
      "metric": [
        "loglikelihood_acc"
      ],
      "hf_avail_splits": [
        "training",
        "eval"
      ],
      "evaluation_splits": [
        "eval"
      ],
      "few_shots_split": null,
      "few_shots_select": null,
      "generation_size": -1,
      "stop_sequence": [
        "\n"
      ],
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "lighteval"
      ],
      "original_num_docs": 1511,
      "effective_num_docs": 1511,
      "trust_dataset": true,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|acva:Algeria|0": {
      "hashes": {
        "hash_examples": "da5a3003cd46f6f9",
        "hash_full_prompts": "77de0392549e4b09",
        "hash_input_tokens": "d710a27aac1b6ad6",
        "hash_cont_tokens": "55711835d6667013"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Ancient_Egypt|0": {
      "hashes": {
        "hash_examples": "52d6f767fede195b",
        "hash_full_prompts": "28866eeb75c203b6",
        "hash_input_tokens": "6a3c66c9ded397bf",
        "hash_cont_tokens": "4b46c5ef3b299324"
      },
      "truncated": 0,
      "non_truncated": 315,
      "padded": 630,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arab_Empire|0": {
      "hashes": {
        "hash_examples": "8dacff6a79804a75",
        "hash_full_prompts": "5c11371da55d72aa",
        "hash_input_tokens": "d98b58a1e6d76ca8",
        "hash_cont_tokens": "f50cdb8b425f4c4a"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 530,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Architecture|0": {
      "hashes": {
        "hash_examples": "df286cd862d9f6bb",
        "hash_full_prompts": "8fa9c452208bff94",
        "hash_input_tokens": "72d2ba1d66335184",
        "hash_cont_tokens": "985a2171e9202ee2"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Art|0": {
      "hashes": {
        "hash_examples": "112883d764118a49",
        "hash_full_prompts": "2f29cd622c15ac86",
        "hash_input_tokens": "fc85f6360fa8e850",
        "hash_cont_tokens": "162a838394f03f6a"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Astronomy|0": {
      "hashes": {
        "hash_examples": "20dcdf2454bf8671",
        "hash_full_prompts": "53c9f5002a498566",
        "hash_input_tokens": "b2506f0e676b8a04",
        "hash_cont_tokens": "d2663016f19002df"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Calligraphy|0": {
      "hashes": {
        "hash_examples": "3a9f9d1ebe868a15",
        "hash_full_prompts": "0ccb38517022966f",
        "hash_input_tokens": "037a26d20b3adbc9",
        "hash_cont_tokens": "ea2cfb08c189704f"
      },
      "truncated": 0,
      "non_truncated": 255,
      "padded": 510,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ceremony|0": {
      "hashes": {
        "hash_examples": "c927630f8d2f44da",
        "hash_full_prompts": "fc9f7b5be4150f47",
        "hash_input_tokens": "256e2d3db9e23dba",
        "hash_cont_tokens": "faaf1fbd8d237ce6"
      },
      "truncated": 0,
      "non_truncated": 185,
      "padded": 370,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Clothing|0": {
      "hashes": {
        "hash_examples": "6ad0740c2ac6ac92",
        "hash_full_prompts": "fb75f56cd28214fc",
        "hash_input_tokens": "cad9253018780924",
        "hash_cont_tokens": "90eb258dda9d8a18"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Culture|0": {
      "hashes": {
        "hash_examples": "2177bd857ad872ae",
        "hash_full_prompts": "e8796a9fda699081",
        "hash_input_tokens": "643cc39e5ae39a79",
        "hash_cont_tokens": "ce241cc04ec83270"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Food|0": {
      "hashes": {
        "hash_examples": "a6ada65b71d7c9c5",
        "hash_full_prompts": "e9329a05858f7078",
        "hash_input_tokens": "db56de326bc34602",
        "hash_cont_tokens": "a6ab3d23030b1763"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Funeral|0": {
      "hashes": {
        "hash_examples": "fcee39dc29eaae91",
        "hash_full_prompts": "1140b7ed088aee04",
        "hash_input_tokens": "2a67e209c223b3a1",
        "hash_cont_tokens": "2af28601ab9b8a9a"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Geography|0": {
      "hashes": {
        "hash_examples": "d36eda7c89231c02",
        "hash_full_prompts": "e9e137c5145af70c",
        "hash_input_tokens": "7e1be36bbd8a9dfd",
        "hash_cont_tokens": "13450f4d187ef4a7"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_History|0": {
      "hashes": {
        "hash_examples": "6354ac0d6db6a5fc",
        "hash_full_prompts": "ae3216cb3dbdb580",
        "hash_input_tokens": "248bf6786b9296ba",
        "hash_cont_tokens": "7ee49769902ac165"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Language_Origin|0": {
      "hashes": {
        "hash_examples": "ddc967c8aca34402",
        "hash_full_prompts": "0e1bbf5a7ad9933f",
        "hash_input_tokens": "b27874e7355fd2ac",
        "hash_cont_tokens": "248313868a695209"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Literature|0": {
      "hashes": {
        "hash_examples": "4305379fd46be5d8",
        "hash_full_prompts": "54f42b4252782a8d",
        "hash_input_tokens": "6dc0b8eaad5c4c2c",
        "hash_cont_tokens": "772e59379446e268"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Math|0": {
      "hashes": {
        "hash_examples": "dec621144f4d28be",
        "hash_full_prompts": "7470dcef68e73466",
        "hash_input_tokens": "b56cf79c865d7975",
        "hash_cont_tokens": "6b0ae8fb5e9a14ee"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Medicine|0": {
      "hashes": {
        "hash_examples": "2b344cdae9495ff2",
        "hash_full_prompts": "246d9b22dc989ddc",
        "hash_input_tokens": "01f3a03b06685b85",
        "hash_cont_tokens": "2ead9c5ac21f3809"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Music|0": {
      "hashes": {
        "hash_examples": "0c54624d881944ce",
        "hash_full_prompts": "04c30eb0a116bc1e",
        "hash_input_tokens": "eafcf145ba3a769c",
        "hash_cont_tokens": "ff8505ab5e7a2ec2"
      },
      "truncated": 0,
      "non_truncated": 139,
      "padded": 278,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Ornament|0": {
      "hashes": {
        "hash_examples": "251a4a84289d8bc1",
        "hash_full_prompts": "a70cd2f5b2a54b85",
        "hash_input_tokens": "daba32789a4665a0",
        "hash_cont_tokens": "36f542f2cbad430b"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Philosophy|0": {
      "hashes": {
        "hash_examples": "3f86fb9c94c13d22",
        "hash_full_prompts": "bbe034266d18bf23",
        "hash_input_tokens": "d35d79e8723e9738",
        "hash_cont_tokens": "79c38559bda4bbee"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Physics_and_Chemistry|0": {
      "hashes": {
        "hash_examples": "8fec65af3695b62a",
        "hash_full_prompts": "f797afca0168ed4e",
        "hash_input_tokens": "42ccf79a27d65324",
        "hash_cont_tokens": "5b91abf23db5ec4e"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Arabic_Wedding|0": {
      "hashes": {
        "hash_examples": "9cc3477184d7a4b8",
        "hash_full_prompts": "071defc53e50e572",
        "hash_input_tokens": "44d45c2b96c7c721",
        "hash_cont_tokens": "4ccb5738c336d2f2"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Bahrain|0": {
      "hashes": {
        "hash_examples": "c92e803a0fa8b9e2",
        "hash_full_prompts": "21d9d429ec23d6b9",
        "hash_input_tokens": "5d44e17bb18453f1",
        "hash_cont_tokens": "cf84d40202472662"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Comoros|0": {
      "hashes": {
        "hash_examples": "06e5d4bba8e54cae",
        "hash_full_prompts": "13414e577a952681",
        "hash_input_tokens": "ae86be7944f9090c",
        "hash_cont_tokens": "4b99ff91a4805e48"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Egypt_modern|0": {
      "hashes": {
        "hash_examples": "c6ec369164f93446",
        "hash_full_prompts": "5d080ba6d3b6394d",
        "hash_input_tokens": "c0e01137dafa7d8c",
        "hash_cont_tokens": "75df99cadd84f8ed"
      },
      "truncated": 0,
      "non_truncated": 95,
      "padded": 190,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromAncientEgypt|0": {
      "hashes": {
        "hash_examples": "b9d56d74818b9bd4",
        "hash_full_prompts": "eba1b02d81512c68",
        "hash_input_tokens": "21a97f544193fd35",
        "hash_cont_tokens": "9805574b73f34de9"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromByzantium|0": {
      "hashes": {
        "hash_examples": "5316c9624e7e59b8",
        "hash_full_prompts": "a5b29eaf66b5f86f",
        "hash_input_tokens": "f1bdd6083281adaa",
        "hash_cont_tokens": "686683a27e22ffe1"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromChina|0": {
      "hashes": {
        "hash_examples": "87894bce95a56411",
        "hash_full_prompts": "5526274816d30d3d",
        "hash_input_tokens": "e22402f12be87b68",
        "hash_cont_tokens": "1020615abc2cad49"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromGreece|0": {
      "hashes": {
        "hash_examples": "0baa78a27e469312",
        "hash_full_prompts": "56fe6594b5b5a493",
        "hash_input_tokens": "6514e21a606612f9",
        "hash_cont_tokens": "cdd8a0229ea9f836"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromIslam|0": {
      "hashes": {
        "hash_examples": "0c2532cde6541ff2",
        "hash_full_prompts": "38e1e4b999c03ae9",
        "hash_input_tokens": "d24c00a13ffd4a58",
        "hash_cont_tokens": "b3c840bea39dc88b"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 290,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromPersia|0": {
      "hashes": {
        "hash_examples": "efcd8112dc53c6e5",
        "hash_full_prompts": "08dee01f39e1d076",
        "hash_input_tokens": "e28491878367a76d",
        "hash_cont_tokens": "cfb0102629b11d84"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:InfluenceFromRome|0": {
      "hashes": {
        "hash_examples": "9db61480e2e85fd3",
        "hash_full_prompts": "267b1766f757c2f6",
        "hash_input_tokens": "8978ea437ffc3a29",
        "hash_cont_tokens": "3e06ae1c7eeba4ce"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Iraq|0": {
      "hashes": {
        "hash_examples": "96dac3dfa8d2f41f",
        "hash_full_prompts": "73c2089819652c3a",
        "hash_input_tokens": "10b47ae466948788",
        "hash_cont_tokens": "1ad61f300140e97c"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_Education|0": {
      "hashes": {
        "hash_examples": "0d80355f6a4cb51b",
        "hash_full_prompts": "532b7276c305c9f2",
        "hash_input_tokens": "f79d3de27c65fcf6",
        "hash_cont_tokens": "a6ab3d23030b1763"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islam_branches_and_schools|0": {
      "hashes": {
        "hash_examples": "5cedce1be2c3ad50",
        "hash_full_prompts": "3db7460daa9ab89d",
        "hash_input_tokens": "90fa6b5b4de404cd",
        "hash_cont_tokens": "0d10be400bca4a8c"
      },
      "truncated": 0,
      "non_truncated": 175,
      "padded": 350,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Islamic_law_system|0": {
      "hashes": {
        "hash_examples": "c0e6db8bc84e105e",
        "hash_full_prompts": "f50b2ffb1af4483f",
        "hash_input_tokens": "2b01a3f9a3b6290b",
        "hash_cont_tokens": "1807108b46b474f6"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Jordan|0": {
      "hashes": {
        "hash_examples": "33deb5b4e5ddd6a1",
        "hash_full_prompts": "61745499fe85ae47",
        "hash_input_tokens": "3d0e0a690284b835",
        "hash_cont_tokens": "cf84d40202472662"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Kuwait|0": {
      "hashes": {
        "hash_examples": "eb41773346d7c46c",
        "hash_full_prompts": "5b1e7dbd26bc4c50",
        "hash_input_tokens": "a83605d1d7d9e02b",
        "hash_cont_tokens": "cf84d40202472662"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Lebanon|0": {
      "hashes": {
        "hash_examples": "25932dbf4c13d34f",
        "hash_full_prompts": "f5681ba9901cb073",
        "hash_input_tokens": "3a7e43a30b2f9f4a",
        "hash_cont_tokens": "152eb3f4ee6cda3f"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Libya|0": {
      "hashes": {
        "hash_examples": "f2c4db63cd402926",
        "hash_full_prompts": "6cd22aba574e424f",
        "hash_input_tokens": "275ee743f94ec03c",
        "hash_cont_tokens": "35762568c3dc1391"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mauritania|0": {
      "hashes": {
        "hash_examples": "8723ab5fdf286b54",
        "hash_full_prompts": "1f1bf71819c79c72",
        "hash_input_tokens": "8d242db5364ad1b7",
        "hash_cont_tokens": "735fdcecd6aeb5fc"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Mesopotamia_civilization|0": {
      "hashes": {
        "hash_examples": "c33f5502a6130ca9",
        "hash_full_prompts": "be0fb5377d5202dd",
        "hash_input_tokens": "4ac23b3cf045bade",
        "hash_cont_tokens": "59a412812903e4ac"
      },
      "truncated": 0,
      "non_truncated": 155,
      "padded": 310,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Morocco|0": {
      "hashes": {
        "hash_examples": "588a5ed27904b1ae",
        "hash_full_prompts": "086d9b09da3a55ed",
        "hash_input_tokens": "65f179be7b8d0be1",
        "hash_cont_tokens": "ddbe93aee0e52907"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Oman|0": {
      "hashes": {
        "hash_examples": "d447c52b94248b69",
        "hash_full_prompts": "fd6c0769a31755f3",
        "hash_input_tokens": "a94da6405175b7cc",
        "hash_cont_tokens": "a36a318234191304"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Palestine|0": {
      "hashes": {
        "hash_examples": "19197e076ad14ff5",
        "hash_full_prompts": "88f9babec64a778b",
        "hash_input_tokens": "9e56c86f658b7019",
        "hash_cont_tokens": "ae61f26c1f7ebc5c"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Qatar|0": {
      "hashes": {
        "hash_examples": "cf0736fa185b28f6",
        "hash_full_prompts": "ffcd9f1544d039f5",
        "hash_input_tokens": "0f6db3241313c058",
        "hash_cont_tokens": "89b579780923a3f4"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Saudi_Arabia|0": {
      "hashes": {
        "hash_examples": "69beda6e1b85a08d",
        "hash_full_prompts": "8d418f75a8c77d28",
        "hash_input_tokens": "17af33d594a15aaf",
        "hash_cont_tokens": "e331ce9639615292"
      },
      "truncated": 0,
      "non_truncated": 195,
      "padded": 390,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Somalia|0": {
      "hashes": {
        "hash_examples": "b387940c65784fbf",
        "hash_full_prompts": "3446f47f1425e428",
        "hash_input_tokens": "d1c883c741f4b8cc",
        "hash_cont_tokens": "e81314a3066430a8"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Sudan|0": {
      "hashes": {
        "hash_examples": "e02c32b9d2dd0c3f",
        "hash_full_prompts": "8fcd3a296d56aabf",
        "hash_input_tokens": "feb0b1372b911830",
        "hash_cont_tokens": "fcd6308513f0eee6"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Syria|0": {
      "hashes": {
        "hash_examples": "60a6f8fe73bda4bb",
        "hash_full_prompts": "f61d836924fc4bea",
        "hash_input_tokens": "68711bad4ab46948",
        "hash_cont_tokens": "152eb3f4ee6cda3f"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Tunisia|0": {
      "hashes": {
        "hash_examples": "34bb15d3830c5649",
        "hash_full_prompts": "87ed3387e5f4fd7f",
        "hash_input_tokens": "30b984db46f1174f",
        "hash_cont_tokens": "35762568c3dc1391"
      },
      "truncated": 0,
      "non_truncated": 45,
      "padded": 90,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:United_Arab_Emirates|0": {
      "hashes": {
        "hash_examples": "98a0ba78172718ce",
        "hash_full_prompts": "6591948babd1c41a",
        "hash_input_tokens": "e8cfb4ffd6b64258",
        "hash_cont_tokens": "80f3c90839483bd3"
      },
      "truncated": 0,
      "non_truncated": 85,
      "padded": 170,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:Yemen|0": {
      "hashes": {
        "hash_examples": "18e9bcccbb4ced7a",
        "hash_full_prompts": "0d5578573a4a3b05",
        "hash_input_tokens": "2244461185590efc",
        "hash_cont_tokens": "195b1ea3acb54947"
      },
      "truncated": 0,
      "non_truncated": 10,
      "padded": 20,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:communication|0": {
      "hashes": {
        "hash_examples": "9ff28ab5eab5c97b",
        "hash_full_prompts": "81e11757609760ab",
        "hash_input_tokens": "88c366a4ee718b24",
        "hash_cont_tokens": "501163b8eb278629"
      },
      "truncated": 0,
      "non_truncated": 364,
      "padded": 728,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:computer_and_phone|0": {
      "hashes": {
        "hash_examples": "37bac2f086aaf6c2",
        "hash_full_prompts": "3b010466c2cbf51c",
        "hash_input_tokens": "b2326596429e387f",
        "hash_cont_tokens": "2e1448acda745f80"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:daily_life|0": {
      "hashes": {
        "hash_examples": "bf07363c1c252e2f",
        "hash_full_prompts": "0965f8d50c96e53b",
        "hash_input_tokens": "69b08d1f3cf30b49",
        "hash_cont_tokens": "940dd6a709b627a7"
      },
      "truncated": 0,
      "non_truncated": 337,
      "padded": 674,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|acva:entertainment|0": {
      "hashes": {
        "hash_examples": "37077bc00f0ac56a",
        "hash_full_prompts": "713ee60ceda81426",
        "hash_input_tokens": "5fa2d259a649b480",
        "hash_cont_tokens": "f463a75db494a023"
      },
      "truncated": 0,
      "non_truncated": 295,
      "padded": 590,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:mcq_exams_test_ar|0": {
      "hashes": {
        "hash_examples": "c07a5e78c5c0b8fe",
        "hash_full_prompts": "3aa44c0acc675498",
        "hash_input_tokens": "087c0196b38ac1ca",
        "hash_cont_tokens": "85e4f840a36620cc"
      },
      "truncated": 0,
      "non_truncated": 557,
      "padded": 2228,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_dialects|0": {
      "hashes": {
        "hash_examples": "c0b6081f83e14064",
        "hash_full_prompts": "432ee2ee192ab68f",
        "hash_input_tokens": "49b3866175cca93e",
        "hash_cont_tokens": "9d6a90ecd6f45ad2"
      },
      "truncated": 0,
      "non_truncated": 5395,
      "padded": 21580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:meta_ar_msa|0": {
      "hashes": {
        "hash_examples": "64eb78a7c5b7484b",
        "hash_full_prompts": "ee21514e8434ea72",
        "hash_input_tokens": "e84fdb0f527f8181",
        "hash_cont_tokens": "6b4de2670a37b32e"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_facts_truefalse_balanced_task|0": {
      "hashes": {
        "hash_examples": "54fc3502c1c02c06",
        "hash_full_prompts": "f1c6aac23a5fb263",
        "hash_input_tokens": "7c241f3a94ca9877",
        "hash_cont_tokens": "9276275cf5a73dc9"
      },
      "truncated": 0,
      "non_truncated": 75,
      "padded": 150,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_soqal_task|0": {
      "hashes": {
        "hash_examples": "46572d83696552ae",
        "hash_full_prompts": "fa355419836084b6",
        "hash_input_tokens": "a7c949ae442c5dda",
        "hash_cont_tokens": "0e97ada8bb78e922"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_grounded_statement_xglue_mlqa_task|0": {
      "hashes": {
        "hash_examples": "f430d97ff715bc1c",
        "hash_full_prompts": "209f420cfa14f92a",
        "hash_input_tokens": "11b77e0b2b2b8b6a",
        "hash_cont_tokens": "e0ccc899f034b9aa"
      },
      "truncated": 0,
      "non_truncated": 150,
      "padded": 750,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_no_neutral_task|0": {
      "hashes": {
        "hash_examples": "6b70a7416584f98c",
        "hash_full_prompts": "414a7b766b972084",
        "hash_input_tokens": "ff9f8cfd44f50993",
        "hash_cont_tokens": "8546966e156e91a0"
      },
      "truncated": 0,
      "non_truncated": 7995,
      "padded": 15990,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_rating_sentiment_task|0": {
      "hashes": {
        "hash_examples": "bc2005cc9d2f436e",
        "hash_full_prompts": "d8ba218478c2c0df",
        "hash_input_tokens": "2a1e9a5a97b928ad",
        "hash_cont_tokens": "d3330995fba33f48"
      },
      "truncated": 0,
      "non_truncated": 5995,
      "padded": 17985,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|alghafa:multiple_choice_sentiment_task|0": {
      "hashes": {
        "hash_examples": "6fb0e254ea5945d8",
        "hash_full_prompts": "6a97b6ef16772e57",
        "hash_input_tokens": "82d365bd4ab7df07",
        "hash_cont_tokens": "b897a66087441558"
      },
      "truncated": 0,
      "non_truncated": 1720,
      "padded": 5160,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_exams|0": {
      "hashes": {
        "hash_examples": "6d721df351722656",
        "hash_full_prompts": "1cbccc26e316371d",
        "hash_input_tokens": "77232ce961118ae3",
        "hash_cont_tokens": "dc0b8f823c9d5f67"
      },
      "truncated": 0,
      "non_truncated": 537,
      "padded": 2148,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:abstract_algebra|0": {
      "hashes": {
        "hash_examples": "f2ddca8f45c0a511",
        "hash_full_prompts": "8b3606d5ec02d5cb",
        "hash_input_tokens": "527da1ccda3cf54b",
        "hash_cont_tokens": "eee1d5bf8f6ad20e"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:anatomy|0": {
      "hashes": {
        "hash_examples": "dfdbc1b83107668d",
        "hash_full_prompts": "6f726188bff49b4d",
        "hash_input_tokens": "69acdef5066143a1",
        "hash_cont_tokens": "c7bc8050fc430cc8"
      },
      "truncated": 0,
      "non_truncated": 135,
      "padded": 540,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:astronomy|0": {
      "hashes": {
        "hash_examples": "9736a606002a848e",
        "hash_full_prompts": "f046a226101634d6",
        "hash_input_tokens": "5cecb76cc71c1ca6",
        "hash_cont_tokens": "2882f8b8395b26d5"
      },
      "truncated": 0,
      "non_truncated": 152,
      "padded": 608,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:business_ethics|0": {
      "hashes": {
        "hash_examples": "735e452fbb6dc63d",
        "hash_full_prompts": "5926af11bf4fb216",
        "hash_input_tokens": "a4121b5c1e7f46c6",
        "hash_cont_tokens": "e62f1679f2f3bb10"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:clinical_knowledge|0": {
      "hashes": {
        "hash_examples": "6ab0ca4da98aedcf",
        "hash_full_prompts": "0e74baa2392988cb",
        "hash_input_tokens": "f5069a860d886a71",
        "hash_cont_tokens": "7c5578ce4508619c"
      },
      "truncated": 0,
      "non_truncated": 265,
      "padded": 1060,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_biology|0": {
      "hashes": {
        "hash_examples": "17e4e390848018a4",
        "hash_full_prompts": "17efe8277416892c",
        "hash_input_tokens": "4bc2e93c5e306148",
        "hash_cont_tokens": "7d0bce964851983e"
      },
      "truncated": 0,
      "non_truncated": 144,
      "padded": 576,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_chemistry|0": {
      "hashes": {
        "hash_examples": "4abb169f6dfd234b",
        "hash_full_prompts": "2b7ab912b038a61a",
        "hash_input_tokens": "8b13c70bb218624f",
        "hash_cont_tokens": "d079e3a4074f99d2"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_computer_science|0": {
      "hashes": {
        "hash_examples": "a369e2e941358a1e",
        "hash_full_prompts": "787b4d38aec70faf",
        "hash_input_tokens": "b6cc854f1220a434",
        "hash_cont_tokens": "6a08e5ccc268fe98"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_mathematics|0": {
      "hashes": {
        "hash_examples": "d7be03b8b6020bff",
        "hash_full_prompts": "2b162ed391fae012",
        "hash_input_tokens": "85d5e545cbd2314c",
        "hash_cont_tokens": "04eeac77dd26aab5"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_medicine|0": {
      "hashes": {
        "hash_examples": "0518a00f097346bf",
        "hash_full_prompts": "4defb57ef50e06cd",
        "hash_input_tokens": "39d68d55075c3a2b",
        "hash_cont_tokens": "1d08361184394fb7"
      },
      "truncated": 0,
      "non_truncated": 173,
      "padded": 692,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:college_physics|0": {
      "hashes": {
        "hash_examples": "5d842cd49bc70e12",
        "hash_full_prompts": "a5c77aea9a12fd8b",
        "hash_input_tokens": "6a04045a61c9b34b",
        "hash_cont_tokens": "fa0757429719f147"
      },
      "truncated": 0,
      "non_truncated": 102,
      "padded": 408,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:computer_security|0": {
      "hashes": {
        "hash_examples": "8e85d9f85be9b32f",
        "hash_full_prompts": "98b24223610888c4",
        "hash_input_tokens": "b45bba6c0aced418",
        "hash_cont_tokens": "6a1f29634d484afa"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:conceptual_physics|0": {
      "hashes": {
        "hash_examples": "7964b55a0a49502b",
        "hash_full_prompts": "7d28cd39e256b21a",
        "hash_input_tokens": "424a8da7f726f5d1",
        "hash_cont_tokens": "57c67f187e12b58b"
      },
      "truncated": 0,
      "non_truncated": 235,
      "padded": 940,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:econometrics|0": {
      "hashes": {
        "hash_examples": "1e192eae38347257",
        "hash_full_prompts": "6a2be9a2fcf13fd1",
        "hash_input_tokens": "baa013f8a2053208",
        "hash_cont_tokens": "9efd11b84eafbeac"
      },
      "truncated": 0,
      "non_truncated": 114,
      "padded": 456,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:electrical_engineering|0": {
      "hashes": {
        "hash_examples": "cf97671d5c441da1",
        "hash_full_prompts": "ae2c8781d05baccb",
        "hash_input_tokens": "b3fe251f1d25efe3",
        "hash_cont_tokens": "9355d29f1fb881bd"
      },
      "truncated": 0,
      "non_truncated": 145,
      "padded": 580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:elementary_mathematics|0": {
      "hashes": {
        "hash_examples": "6f49107ed43c40c5",
        "hash_full_prompts": "0173c58272013bc9",
        "hash_input_tokens": "edc9609e27023ed4",
        "hash_cont_tokens": "7b7a26d1ad677a76"
      },
      "truncated": 0,
      "non_truncated": 378,
      "padded": 1512,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:formal_logic|0": {
      "hashes": {
        "hash_examples": "7922c376008ba77b",
        "hash_full_prompts": "433e93f07687aac8",
        "hash_input_tokens": "25a50ba411ec2764",
        "hash_cont_tokens": "0317b3650f19ee92"
      },
      "truncated": 0,
      "non_truncated": 126,
      "padded": 504,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:global_facts|0": {
      "hashes": {
        "hash_examples": "11f9813185047d5b",
        "hash_full_prompts": "85dd909d81087255",
        "hash_input_tokens": "3c0d2368fd311e88",
        "hash_cont_tokens": "6a1f29634d484afa"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_biology|0": {
      "hashes": {
        "hash_examples": "2a804b1d90cbe66e",
        "hash_full_prompts": "42378ac2f2775e4d",
        "hash_input_tokens": "66f3343b678fefff",
        "hash_cont_tokens": "aad3fa3098be7f43"
      },
      "truncated": 0,
      "non_truncated": 310,
      "padded": 1240,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_chemistry|0": {
      "hashes": {
        "hash_examples": "0032168adabc53b4",
        "hash_full_prompts": "1c336cb5fe5ddf6f",
        "hash_input_tokens": "a60637c9db90b096",
        "hash_cont_tokens": "f45cea208b507580"
      },
      "truncated": 0,
      "non_truncated": 203,
      "padded": 812,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_computer_science|0": {
      "hashes": {
        "hash_examples": "f2fb8740f9df980f",
        "hash_full_prompts": "7ddea1b422190cf1",
        "hash_input_tokens": "6d8100d806def810",
        "hash_cont_tokens": "f669fe31b5ba6823"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_european_history|0": {
      "hashes": {
        "hash_examples": "73509021e7e66435",
        "hash_full_prompts": "c2a5c6b318336c64",
        "hash_input_tokens": "4bfa883dee7eadbe",
        "hash_cont_tokens": "8c8ade2da33cad27"
      },
      "truncated": 0,
      "non_truncated": 165,
      "padded": 660,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_geography|0": {
      "hashes": {
        "hash_examples": "9e08d1894940ff42",
        "hash_full_prompts": "877ab6bea018723f",
        "hash_input_tokens": "6e889e52169d4f8d",
        "hash_cont_tokens": "2c478b4d5ccaefb8"
      },
      "truncated": 0,
      "non_truncated": 198,
      "padded": 792,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_government_and_politics|0": {
      "hashes": {
        "hash_examples": "64b7e97817ca6c76",
        "hash_full_prompts": "e8a0d4cec8131715",
        "hash_input_tokens": "539f84766a469325",
        "hash_cont_tokens": "40cef2c7115f2fb9"
      },
      "truncated": 0,
      "non_truncated": 193,
      "padded": 772,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_macroeconomics|0": {
      "hashes": {
        "hash_examples": "9f582da8534bd2ef",
        "hash_full_prompts": "d7aeaee5a5804d1e",
        "hash_input_tokens": "a55e9064c747623f",
        "hash_cont_tokens": "dbdd412e69845f79"
      },
      "truncated": 0,
      "non_truncated": 390,
      "padded": 1560,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_mathematics|0": {
      "hashes": {
        "hash_examples": "fd54f1c10d423c51",
        "hash_full_prompts": "8f008689cb29e1ef",
        "hash_input_tokens": "24ee09bd11ef780d",
        "hash_cont_tokens": "f279d2a8890e6914"
      },
      "truncated": 0,
      "non_truncated": 270,
      "padded": 1080,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_microeconomics|0": {
      "hashes": {
        "hash_examples": "7037896925aaf42f",
        "hash_full_prompts": "64536c977bb3c43a",
        "hash_input_tokens": "cf004ea5f478187a",
        "hash_cont_tokens": "ad8890a3051d4135"
      },
      "truncated": 0,
      "non_truncated": 238,
      "padded": 952,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_physics|0": {
      "hashes": {
        "hash_examples": "60c3776215167dae",
        "hash_full_prompts": "94a21727417e2c20",
        "hash_input_tokens": "dfe504ba29358137",
        "hash_cont_tokens": "24933e0de9d15619"
      },
      "truncated": 0,
      "non_truncated": 151,
      "padded": 604,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_psychology|0": {
      "hashes": {
        "hash_examples": "61176bfd5da1298f",
        "hash_full_prompts": "f7c6c9fa7db5bb6b",
        "hash_input_tokens": "78c0244d68fa9e8a",
        "hash_cont_tokens": "c9d98df72c9b4b20"
      },
      "truncated": 0,
      "non_truncated": 545,
      "padded": 2180,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_statistics|0": {
      "hashes": {
        "hash_examples": "40dfeebd1ea10f76",
        "hash_full_prompts": "e89ea1b3b229d45e",
        "hash_input_tokens": "3e3e62bacd123508",
        "hash_cont_tokens": "1173c348f11f3766"
      },
      "truncated": 0,
      "non_truncated": 216,
      "padded": 864,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_us_history|0": {
      "hashes": {
        "hash_examples": "03daa510ba917f4d",
        "hash_full_prompts": "38ce59a2cf8caf49",
        "hash_input_tokens": "6e3115a673913e1c",
        "hash_cont_tokens": "aab37f3b087ccbf8"
      },
      "truncated": 0,
      "non_truncated": 204,
      "padded": 816,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:high_school_world_history|0": {
      "hashes": {
        "hash_examples": "be075ffd579f43c2",
        "hash_full_prompts": "84c1661895dd5fea",
        "hash_input_tokens": "33162a74e64cff3c",
        "hash_cont_tokens": "d003c40d258310aa"
      },
      "truncated": 0,
      "non_truncated": 237,
      "padded": 948,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_aging|0": {
      "hashes": {
        "hash_examples": "caa5b69f640bd1ef",
        "hash_full_prompts": "cd7e8209a9ebc891",
        "hash_input_tokens": "a27a745afd187bce",
        "hash_cont_tokens": "489862d5aef36496"
      },
      "truncated": 0,
      "non_truncated": 223,
      "padded": 892,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:human_sexuality|0": {
      "hashes": {
        "hash_examples": "5ed2e38fb25a3767",
        "hash_full_prompts": "7ee820c9d9101c38",
        "hash_input_tokens": "811670e2d2efdb52",
        "hash_cont_tokens": "39b197af8a631153"
      },
      "truncated": 0,
      "non_truncated": 131,
      "padded": 524,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:international_law|0": {
      "hashes": {
        "hash_examples": "4e3e9e28d1b96484",
        "hash_full_prompts": "7327181159a54cf0",
        "hash_input_tokens": "9eea3c5c31a8f1d6",
        "hash_cont_tokens": "7dd5d2c18f3348da"
      },
      "truncated": 0,
      "non_truncated": 121,
      "padded": 484,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:jurisprudence|0": {
      "hashes": {
        "hash_examples": "e264b755366310b3",
        "hash_full_prompts": "c7d06f385f0ba899",
        "hash_input_tokens": "438eba7a48b2cc74",
        "hash_cont_tokens": "c4913963232df977"
      },
      "truncated": 0,
      "non_truncated": 108,
      "padded": 432,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:logical_fallacies|0": {
      "hashes": {
        "hash_examples": "a4ab6965a3e38071",
        "hash_full_prompts": "d5ce150b427f37d1",
        "hash_input_tokens": "736d0a5d61476089",
        "hash_cont_tokens": "6e2963468534c560"
      },
      "truncated": 0,
      "non_truncated": 163,
      "padded": 652,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:machine_learning|0": {
      "hashes": {
        "hash_examples": "b92320efa6636b40",
        "hash_full_prompts": "ba11aeef06c324df",
        "hash_input_tokens": "8fd3fe1c9b83cd5d",
        "hash_cont_tokens": "1d645449eafbe32d"
      },
      "truncated": 0,
      "non_truncated": 112,
      "padded": 448,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:management|0": {
      "hashes": {
        "hash_examples": "c9ee4872a850fe20",
        "hash_full_prompts": "263745187217d00e",
        "hash_input_tokens": "275c6aeb09eca624",
        "hash_cont_tokens": "b64be441b72bddbe"
      },
      "truncated": 0,
      "non_truncated": 103,
      "padded": 412,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:marketing|0": {
      "hashes": {
        "hash_examples": "0c151b70f6a047e3",
        "hash_full_prompts": "2e07ab23353bc538",
        "hash_input_tokens": "2c5b0154b5c90920",
        "hash_cont_tokens": "41c2291bf511073b"
      },
      "truncated": 0,
      "non_truncated": 234,
      "padded": 936,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:medical_genetics|0": {
      "hashes": {
        "hash_examples": "513f6cb8fca3a24e",
        "hash_full_prompts": "5da339e5a4b3ed9d",
        "hash_input_tokens": "2821b2d9af60d0b7",
        "hash_cont_tokens": "6a1f29634d484afa"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 400,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:miscellaneous|0": {
      "hashes": {
        "hash_examples": "259a190d635331db",
        "hash_full_prompts": "65ec137c74913a48",
        "hash_input_tokens": "b212a3a39c268b2c",
        "hash_cont_tokens": "abe902bca24be128"
      },
      "truncated": 0,
      "non_truncated": 783,
      "padded": 3132,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_disputes|0": {
      "hashes": {
        "hash_examples": "b85052c48a0b7bc3",
        "hash_full_prompts": "039a72a9c568fa4b",
        "hash_input_tokens": "7637c92f27ee0f34",
        "hash_cont_tokens": "669a05d30a6e7357"
      },
      "truncated": 0,
      "non_truncated": 346,
      "padded": 1384,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:moral_scenarios|0": {
      "hashes": {
        "hash_examples": "28d0b069ef00dd00",
        "hash_full_prompts": "829a416afcccc2bb",
        "hash_input_tokens": "6c19db579ec0493b",
        "hash_cont_tokens": "3270430dce5f3321"
      },
      "truncated": 0,
      "non_truncated": 895,
      "padded": 3580,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:nutrition|0": {
      "hashes": {
        "hash_examples": "00c9bc5f1d305b2f",
        "hash_full_prompts": "9015d8299636362d",
        "hash_input_tokens": "1d9167ed96c8881d",
        "hash_cont_tokens": "24046fb43cce8e96"
      },
      "truncated": 0,
      "non_truncated": 306,
      "padded": 1197,
      "non_padded": 27,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:philosophy|0": {
      "hashes": {
        "hash_examples": "a458c08454a3fd5f",
        "hash_full_prompts": "d096e94881b8243e",
        "hash_input_tokens": "c92809ec34442fbe",
        "hash_cont_tokens": "067c86c877536ae3"
      },
      "truncated": 0,
      "non_truncated": 311,
      "padded": 1240,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:prehistory|0": {
      "hashes": {
        "hash_examples": "d6a0ecbdbb670e9c",
        "hash_full_prompts": "10a89d156d6effc6",
        "hash_input_tokens": "8b4e78c94eecdda9",
        "hash_cont_tokens": "538d7a28c7220485"
      },
      "truncated": 0,
      "non_truncated": 324,
      "padded": 1288,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_accounting|0": {
      "hashes": {
        "hash_examples": "b4a95fe480b6540e",
        "hash_full_prompts": "d9ec9ba07c200d94",
        "hash_input_tokens": "e89ca01c1d20510d",
        "hash_cont_tokens": "4f33a6ccd556eeb0"
      },
      "truncated": 0,
      "non_truncated": 282,
      "padded": 1124,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_law|0": {
      "hashes": {
        "hash_examples": "c2be9651cdbdde3b",
        "hash_full_prompts": "f17b175a01dfd4a6",
        "hash_input_tokens": "b8ab30d8763e7495",
        "hash_cont_tokens": "ec743336ea03f80b"
      },
      "truncated": 0,
      "non_truncated": 1534,
      "padded": 6128,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_medicine|0": {
      "hashes": {
        "hash_examples": "26ce92416288f273",
        "hash_full_prompts": "382b0fb7647d7897",
        "hash_input_tokens": "cab6e2b4ef76f0fe",
        "hash_cont_tokens": "8edc159daeba5755"
      },
      "truncated": 0,
      "non_truncated": 272,
      "padded": 1088,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:professional_psychology|0": {
      "hashes": {
        "hash_examples": "71ea5f182ea9a641",
        "hash_full_prompts": "31c93ace06723177",
        "hash_input_tokens": "8d331437c5e1f083",
        "hash_cont_tokens": "ee4be3b682f5d77f"
      },
      "truncated": 0,
      "non_truncated": 612,
      "padded": 2416,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:public_relations|0": {
      "hashes": {
        "hash_examples": "125adc21f91f8d77",
        "hash_full_prompts": "febfbac620b44cd3",
        "hash_input_tokens": "9a032305d1c0d99c",
        "hash_cont_tokens": "38daf3c2f2412ed7"
      },
      "truncated": 0,
      "non_truncated": 110,
      "padded": 436,
      "non_padded": 4,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:security_studies|0": {
      "hashes": {
        "hash_examples": "3c18b216c099fb26",
        "hash_full_prompts": "d938eb6d7a19e376",
        "hash_input_tokens": "3bebeb96fe71bc8d",
        "hash_cont_tokens": "78d0a23c697ea1a9"
      },
      "truncated": 0,
      "non_truncated": 245,
      "padded": 980,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:sociology|0": {
      "hashes": {
        "hash_examples": "3f2a9634cef7417d",
        "hash_full_prompts": "07e947584429ad74",
        "hash_input_tokens": "a049f567de0d52d0",
        "hash_cont_tokens": "4f103594803d5263"
      },
      "truncated": 0,
      "non_truncated": 201,
      "padded": 784,
      "non_padded": 20,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:us_foreign_policy|0": {
      "hashes": {
        "hash_examples": "22249da54056475e",
        "hash_full_prompts": "52cbb477b3a2cc6c",
        "hash_input_tokens": "f447bb2afc176b5e",
        "hash_cont_tokens": "8c4b661123edc1ce"
      },
      "truncated": 0,
      "non_truncated": 100,
      "padded": 392,
      "non_padded": 8,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:virology|0": {
      "hashes": {
        "hash_examples": "9d194b9471dc624e",
        "hash_full_prompts": "ba54e1c625f1a1f8",
        "hash_input_tokens": "cf70fb6cf720a90a",
        "hash_cont_tokens": "f10723658baabfbd"
      },
      "truncated": 0,
      "non_truncated": 166,
      "padded": 664,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arabic_mmlu:world_religions|0": {
      "hashes": {
        "hash_examples": "229e5fe50082b064",
        "hash_full_prompts": "fbfba3c6c08575b6",
        "hash_input_tokens": "9ed17525472ec37f",
        "hash_cont_tokens": "4b50d48948588fbb"
      },
      "truncated": 0,
      "non_truncated": 171,
      "padded": 684,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_challenge_okapi_ar|0": {
      "hashes": {
        "hash_examples": "ab893807673bc355",
        "hash_full_prompts": "6ccac171e65c3f2e",
        "hash_input_tokens": "ed60df24ee75fb76",
        "hash_cont_tokens": "159131f992b13166"
      },
      "truncated": 0,
      "non_truncated": 1160,
      "padded": 4596,
      "non_padded": 44,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|arc_easy_ar|0": {
      "hashes": {
        "hash_examples": "acb688624acc3d04",
        "hash_full_prompts": "f5f56da456d83576",
        "hash_input_tokens": "4c7a6d4358feb5b7",
        "hash_cont_tokens": "f5136a10698c4879"
      },
      "truncated": 0,
      "non_truncated": 2364,
      "padded": 9407,
      "non_padded": 49,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|boolq_ar|0": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "b322d73abb7af8f2",
        "hash_input_tokens": "0d9a1e320d2eb8a3",
        "hash_cont_tokens": "6aa9bffe80466104"
      },
      "truncated": 0,
      "non_truncated": 3260,
      "padded": 6482,
      "non_padded": 38,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|copa_ext_ar|0": {
      "hashes": {
        "hash_examples": "9bb83301bb72eecf",
        "hash_full_prompts": "2f1361a6b48346aa",
        "hash_input_tokens": "9e889c47af965fa4",
        "hash_cont_tokens": "0ae0472db840e598"
      },
      "truncated": 0,
      "non_truncated": 90,
      "padded": 180,
      "non_padded": 0,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|0": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "db299df883248d59",
        "hash_input_tokens": "af67ded96ed5dbf0",
        "hash_cont_tokens": "4129917521bb3789"
      },
      "truncated": 0,
      "non_truncated": 9171,
      "padded": 36615,
      "non_padded": 69,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|mmlu_okapi_ar|0": {
      "hashes": {
        "hash_examples": "723b5371ad6fdc31",
        "hash_full_prompts": "6a3af3840e72e7e2",
        "hash_input_tokens": "d84cb3b08ae21a20",
        "hash_cont_tokens": "85954449f5eb0799"
      },
      "truncated": 0,
      "non_truncated": 12923,
      "padded": 51137,
      "non_padded": 555,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|openbook_qa_ext_ar|0": {
      "hashes": {
        "hash_examples": "923d41eb0aca93eb",
        "hash_full_prompts": "82f135b514d00643",
        "hash_input_tokens": "e1ea0ce423760d13",
        "hash_cont_tokens": "4897eb86acf11977"
      },
      "truncated": 0,
      "non_truncated": 495,
      "padded": 1948,
      "non_padded": 32,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|piqa_ar|0": {
      "hashes": {
        "hash_examples": "94bc205a520d3ea0",
        "hash_full_prompts": "713f568fb04c0b18",
        "hash_input_tokens": "cf0f77d38eab31da",
        "hash_cont_tokens": "88fe1baa62bc0548"
      },
      "truncated": 0,
      "non_truncated": 1833,
      "padded": 3604,
      "non_padded": 62,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|race_ar|0": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "48f230ba87ef1891",
        "hash_input_tokens": "9e5780da30fbffd8",
        "hash_cont_tokens": "2074037b3e223398"
      },
      "truncated": 0,
      "non_truncated": 4929,
      "padded": 19692,
      "non_padded": 24,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|sciq_ar|0": {
      "hashes": {
        "hash_examples": "eec32f8a3edee774",
        "hash_full_prompts": "4e638f6f6c0190c2",
        "hash_input_tokens": "ec5859a9aa19516e",
        "hash_cont_tokens": "88bdcb6a7d6a8da6"
      },
      "truncated": 0,
      "non_truncated": 995,
      "padded": 3951,
      "non_padded": 29,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|toxigen_ar|0": {
      "hashes": {
        "hash_examples": "1e139513004a9a2e",
        "hash_full_prompts": "2fc6f1ba657d31cf",
        "hash_input_tokens": "de530c84067e130f",
        "hash_cont_tokens": "2d42618764d734a1"
      },
      "truncated": 0,
      "non_truncated": 935,
      "padded": 1842,
      "non_padded": 28,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "lighteval|xstory_cloze:ar|0": {
      "hashes": {
        "hash_examples": "865426a22c787481",
        "hash_full_prompts": "a00d8b44545aaef0",
        "hash_input_tokens": "815daa7b772ba31f",
        "hash_cont_tokens": "36942d3902677f8d"
      },
      "truncated": 0,
      "non_truncated": 1511,
      "padded": 2978,
      "non_padded": 44,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "ef317922d57d5474",
      "hash_full_prompts": "5c8c231cf368c06f",
      "hash_input_tokens": "2f83dcafaa9a7836",
      "hash_cont_tokens": "563b4a8a7475c773"
    },
    "truncated": 0,
    "non_truncated": 85887,
    "padded": 286226,
    "non_padded": 1089,
    "num_truncated_few_shots": 0
  }
}