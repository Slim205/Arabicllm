{
  "config_general": {
    "lighteval_sha": "76aac4b0335f18606fb7f869b1419c1f6ab79aa1",
    "num_fewshot_seeds": 1,
    "override_batch_size": 2,
    "max_samples": null,
    "job_id": "",
    "start_time": 1839.955146833,
    "end_time": 3738.371437668,
    "total_evaluation_time_secondes": "1898.416290835",
    "model_name": "_gpfs_workdir_barkallasl_outputs_big_gemma_pad",
    "model_sha": "",
    "model_dtype": "torch.bfloat16",
    "model_size": "18.02 GB",
    "config": null
  },
  "results": {
    "community|boolq_ar|0": {
      "acc_norm": 0.8638036809815951,
      "acc_norm_stderr": 0.006008250482321191
    },
    "community|hellaswag_okapi_ar|0": {
      "acc_norm": 0.3800021807872642,
      "acc_norm_stderr": 0.00506878018256556
    },
    "community|race_ar|0": {
      "acc_norm": 0.4873199431933455,
      "acc_norm_stderr": 0.007120245282957779
    },
    "all": {
      "acc_norm": 0.5770419349874016,
      "acc_norm_stderr": 0.006065758649281511
    }
  },
  "versions": {
    "community|boolq_ar|0": 0,
    "community|hellaswag_okapi_ar|0": 0,
    "community|race_ar|0": 0
  },
  "config_tasks": {
    "community|boolq_ar": {
      "name": "boolq_ar",
      "prompt_function": "boolq_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "boolq_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 3260,
      "effective_num_docs": 3260,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|hellaswag_okapi_ar": {
      "name": "hellaswag_okapi_ar",
      "prompt_function": "hellaswag_prompt_arabic",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "hellaswag_okapi_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 9171,
      "effective_num_docs": 9171,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    },
    "community|race_ar": {
      "name": "race_ar",
      "prompt_function": "alghafa_prompt",
      "hf_repo": "OALL/AlGhafa-Arabic-LLM-Benchmark-Translated",
      "hf_subset": "race_ar",
      "metric": [
        "loglikelihood_acc_norm"
      ],
      "hf_avail_splits": [
        "test",
        "validation"
      ],
      "evaluation_splits": [
        "test"
      ],
      "few_shots_split": "validation",
      "few_shots_select": "sequential",
      "generation_size": null,
      "stop_sequence": null,
      "output_regex": null,
      "num_samples": null,
      "frozen": false,
      "suite": [
        "community"
      ],
      "original_num_docs": 4929,
      "effective_num_docs": 4929,
      "trust_dataset": null,
      "must_remove_duplicate_docs": null,
      "version": 0
    }
  },
  "summary_tasks": {
    "community|boolq_ar|0": {
      "hashes": {
        "hash_examples": "48355a67867e0c32",
        "hash_full_prompts": "48355a67867e0c32",
        "hash_input_tokens": "bab0527a2890be78",
        "hash_cont_tokens": "333bbdc15342c728"
      },
      "truncated": 0,
      "non_truncated": 3260,
      "padded": 6499,
      "non_padded": 21,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|hellaswag_okapi_ar|0": {
      "hashes": {
        "hash_examples": "6e8cf57a322dfadd",
        "hash_full_prompts": "6e8cf57a322dfadd",
        "hash_input_tokens": "37bc387443d76ba3",
        "hash_cont_tokens": "b0317896b905bef4"
      },
      "truncated": 0,
      "non_truncated": 9171,
      "padded": 36548,
      "non_padded": 136,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    },
    "community|race_ar|0": {
      "hashes": {
        "hash_examples": "de65130bae647516",
        "hash_full_prompts": "de65130bae647516",
        "hash_input_tokens": "e09c68ab1cdfe8cd",
        "hash_cont_tokens": "db8f87b3a7aa7c32"
      },
      "truncated": 0,
      "non_truncated": 4929,
      "padded": 19643,
      "non_padded": 73,
      "effective_few_shots": 0.0,
      "num_truncated_few_shots": 0
    }
  },
  "summary_general": {
    "hashes": {
      "hash_examples": "f551627c8cf8a506",
      "hash_full_prompts": "f551627c8cf8a506",
      "hash_input_tokens": "7d2ecdcbd5c6c1ab",
      "hash_cont_tokens": "4e3a356df4c4c945"
    },
    "truncated": 0,
    "non_truncated": 17360,
    "padded": 62690,
    "non_padded": 230,
    "num_truncated_few_shots": 0
  }
}