print("hello list")

TASKS_TABLE = [{'name': 'arabic_mmlu:abstract_algebra', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'abstract_algebra', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:anatomy', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'anatomy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:astronomy', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'astronomy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:business_ethics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'business_ethics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:clinical_knowledge', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'clinical_knowledge', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:college_biology', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'college_biology', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:college_chemistry', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'college_chemistry', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:college_computer_science', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'college_computer_science', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:college_mathematics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'college_mathematics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:college_medicine', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'college_medicine', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:college_physics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'college_physics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:computer_security', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'computer_security', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:conceptual_physics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'conceptual_physics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:econometrics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'econometrics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:electrical_engineering', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'electrical_engineering', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:elementary_mathematics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'elementary_mathematics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:formal_logic', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'formal_logic', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:global_facts', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'global_facts', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_biology', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_biology', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_chemistry', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_chemistry', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_computer_science', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_computer_science', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_european_history', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_european_history', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_geography', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_geography', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_government_and_politics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_government_and_politics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_macroeconomics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_macroeconomics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_mathematics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_mathematics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_microeconomics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_microeconomics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_physics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_physics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_psychology', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_psychology', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_statistics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_statistics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_us_history', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_us_history', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:high_school_world_history', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'high_school_world_history', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:human_aging', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'human_aging', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:human_sexuality', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'human_sexuality', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:international_law', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'international_law', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:jurisprudence', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'jurisprudence', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:logical_fallacies', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'logical_fallacies', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:machine_learning', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'machine_learning', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:management', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'management', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:marketing', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'marketing', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:medical_genetics', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'medical_genetics', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:miscellaneous', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'miscellaneous', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:moral_disputes', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'moral_disputes', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:moral_scenarios', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'moral_scenarios', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:nutrition', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'nutrition', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:philosophy', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'philosophy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:prehistory', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'prehistory', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:professional_accounting', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'professional_accounting', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:professional_law', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'professional_law', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:professional_medicine', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'professional_medicine', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:professional_psychology', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'professional_psychology', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:public_relations', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'public_relations', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:security_studies', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'security_studies', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:sociology', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'sociology', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:us_foreign_policy', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'us_foreign_policy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:virology', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'virology', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_mmlu:world_religions', 'prompt_function': 'mmlu_arabic', 'hf_repo': 'OALL/Arabic_MMLU', 'hf_subset': 'world_religions', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'dev'), 'evaluation_splits': ('test',), 'few_shots_split': 'dev', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Algeria', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Algeria', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Ancient_Egypt', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Ancient_Egypt', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arab_Empire', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arab_Empire', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Architecture', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Architecture', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Art', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Art', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Astronomy', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Astronomy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Calligraphy', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Calligraphy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Ceremony', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Ceremony', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Clothing', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Clothing', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Culture', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Culture', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Food', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Food', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Funeral', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Funeral', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Geography', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Geography', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_History', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_History', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Language_Origin', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Language_Origin', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Literature', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Literature', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Math', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Math', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Medicine', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Medicine', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Music', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Music', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Ornament', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Ornament', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Philosophy', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Philosophy', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Physics_and_Chemistry', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Physics_and_Chemistry', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Arabic_Wedding', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Arabic_Wedding', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Bahrain', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Bahrain', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Comoros', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Comoros', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Egypt_modern', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Egypt_modern', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromAncientEgypt', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromAncientEgypt', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromByzantium', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromByzantium', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromChina', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromChina', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromGreece', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromGreece', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromIslam', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromIslam', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromPersia', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromPersia', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:InfluenceFromRome', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'InfluenceFromRome', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Iraq', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Iraq', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Islam_Education', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Islam_Education', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Islam_branches_and_schools', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Islam_branches_and_schools', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Islamic_law_system', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Islamic_law_system', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Jordan', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Jordan', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Kuwait', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Kuwait', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Lebanon', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Lebanon', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Libya', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Libya', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Mauritania', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Mauritania', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Mesopotamia_civilization', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Mesopotamia_civilization', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Morocco', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Morocco', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Oman', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Oman', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Palestine', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Palestine', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Qatar', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Qatar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Saudi_Arabia', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Saudi_Arabia', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Somalia', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Somalia', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Sudan', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Sudan', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Syria', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Syria', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Tunisia', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Tunisia', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:United_Arab_Emirates', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'United_Arab_Emirates', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:Yemen', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'Yemen', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:communication', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'communication', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:computer_and_phone', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'computer_and_phone', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:daily_life', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'daily_life', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'acva:entertainment', 'prompt_function': 'acva', 'hf_repo': 'OALL/ACVA', 'hf_subset': 'entertainment', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:mcq_exams_test_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'mcq_exams_test_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:meta_ar_dialects', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'meta_ar_dialects', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:meta_ar_msa', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'meta_ar_msa', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:multiple_choice_facts_truefalse_balanced_task', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'multiple_choice_facts_truefalse_balanced_task', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:multiple_choice_grounded_statement_soqal_task', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'multiple_choice_grounded_statement_soqal_task', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:multiple_choice_grounded_statement_xglue_mlqa_task', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'multiple_choice_grounded_statement_xglue_mlqa_task', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:multiple_choice_rating_sentiment_no_neutral_task', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'multiple_choice_rating_sentiment_no_neutral_task', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:multiple_choice_rating_sentiment_task', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'multiple_choice_rating_sentiment_task', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'alghafa:multiple_choice_sentiment_task', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Native', 'hf_subset': 'multiple_choice_sentiment_task', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': -1, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arabic_exams', 'prompt_function': 'arabic_exams', 'hf_repo': 'OALL/Arabic_EXAMS', 'hf_subset': 'default', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'race_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'race_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'piqa_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'piqa_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arc_easy_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'arc_easy_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'arc_challenge_okapi_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'arc_challenge_okapi_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'mmlu_okapi_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'mmlu_okapi_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'openbook_qa_ext_ar', 'prompt_function': 'alghafa_prompt', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'openbook_qa_ext_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'boolq_ar', 'prompt_function': 'boolq_prompt_arabic', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'boolq_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'copa_ext_ar', 'prompt_function': 'copa_prompt_arabic', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'copa_ext_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'hellaswag_okapi_ar', 'prompt_function': 'hellaswag_prompt_arabic', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'hellaswag_okapi_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'toxigen_ar', 'prompt_function': 'toxigen_prompt_arabic', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'toxigen_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}, {'name': 'sciq_ar', 'prompt_function': 'sciq_prompt_arabic', 'hf_repo': 'OALL/AlGhafa-Arabic-LLM-Benchmark-Translated', 'hf_subset': 'sciq_ar', 'metric': ('loglikelihood_acc_norm',), 'hf_avail_splits': ('test', 'validation'), 'evaluation_splits': ('test',), 'few_shots_split': 'validation', 'few_shots_select': 'sequential', 'generation_size': None, 'stop_sequence': None, 'output_regex': None, 'frozen': False, 'suite': ('community',), 'version': 0}]